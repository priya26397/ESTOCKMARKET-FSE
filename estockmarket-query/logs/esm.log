2022-06-24 14:04:58,231 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [kafka-coordinator-heartbeat-thread | group_id] [Consumer clientId=consumer-group_id-31, groupId=group_id] Group coordinator host.docker.internal:9092 (id: 2147483647 rack: null) is unavailable or invalid due to cause: session timed out without receiving a heartbeat response.isDisconnected: false. Rediscovery will be attempted.
2022-06-24 14:04:58,304 INFO com.mongodb.diagnostics.logging.SLF4JLogger [cluster-ClusterId{value='62b33f0a52d23d5bec795f97', description='null'}-localhost:27017] Opened connection [connectionId{localValue:17, serverValue:55}] to localhost:27017
2022-06-24 14:04:58,307 INFO com.mongodb.diagnostics.logging.SLF4JLogger [cluster-ClusterId{value='62b33f0a52d23d5bec795f97', description='null'}-localhost:27017] Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=2168936800}
2022-06-24 14:04:58,490 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-31, groupId=group_id] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2022-06-24 14:04:58,490 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-26, groupId=group_id] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2022-06-24 14:04:58,494 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-31, groupId=group_id] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2022-06-24 14:04:58,498 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-29, groupId=group_id] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2022-06-24 14:04:58,500 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-32, groupId=group_id] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2022-06-24 14:04:58,489 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-25, groupId=group_id] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2022-06-24 14:04:58,489 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-27, groupId=group_id] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2022-06-24 14:04:59,008 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2022-06-24 14:04:59,083 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-28, groupId=group_id] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2022-06-24 14:04:59,137 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-31, groupId=group_id] Attempt to heartbeat with Generation{generationId=72, memberId='consumer-group_id-31-16e90bdf-f106-4e34-9a0a-430b2f924369', protocol='range'} and group instance id Optional.empty failed due to UNKNOWN_MEMBER_ID, resetting generation
2022-06-24 14:04:59,140 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-31, groupId=group_id] Giving away all assigned partitions as lost since generation has been reset,indicating that consumer is no longer part of the group
2022-06-24 14:04:59,141 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-31, groupId=group_id] Lost previously assigned partitions removeCompany-0
2022-06-24 14:04:59,143 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-29, groupId=group_id] Attempt to heartbeat with Generation{generationId=72, memberId='consumer-group_id-29-bb3edadb-9469-41ab-901b-ded31a73cd48', protocol='range'} and group instance id Optional.empty failed due to UNKNOWN_MEMBER_ID, resetting generation
2022-06-24 14:04:59,144 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-29, groupId=group_id] Giving away all assigned partitions as lost since generation has been reset,indicating that consumer is no longer part of the group
2022-06-24 14:04:59,144 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-29, groupId=group_id] Lost previously assigned partitions removeStock-0
2022-06-24 14:04:59,145 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-28, groupId=group_id] Attempt to heartbeat with Generation{generationId=72, memberId='consumer-group_id-28-271752dc-8347-486c-828f-3995611373b9', protocol='range'} and group instance id Optional.empty failed due to UNKNOWN_MEMBER_ID, resetting generation
2022-06-24 14:04:59,146 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-28, groupId=group_id] Giving away all assigned partitions as lost since generation has been reset,indicating that consumer is no longer part of the group
2022-06-24 14:04:59,146 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-28, groupId=group_id] Lost previously assigned partitions createSector-0
2022-06-24 14:04:59,146 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] group_id: partitions lost: [removeCompany-0]
2022-06-24 14:04:59,146 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] group_id: partitions lost: [removeStock-0]
2022-06-24 14:04:59,146 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] group_id: partitions lost: [createSector-0]
2022-06-24 14:04:59,146 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] group_id: partitions revoked: [removeCompany-0]
2022-06-24 14:04:59,147 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] group_id: partitions revoked: [createSector-0]
2022-06-24 14:04:59,146 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] group_id: partitions revoked: [removeStock-0]
2022-06-24 14:04:59,147 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-32, groupId=group_id] Attempt to heartbeat with Generation{generationId=72, memberId='consumer-group_id-32-13a9432a-ea42-4e6d-9b20-d5d2bd034a6f', protocol='range'} and group instance id Optional.empty failed due to UNKNOWN_MEMBER_ID, resetting generation
2022-06-24 14:04:59,148 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-26, groupId=group_id] Attempt to heartbeat with Generation{generationId=72, memberId='consumer-group_id-26-99f3951d-c4ff-4cd0-ada6-e8d3928bc842', protocol='range'} and group instance id Optional.empty failed due to UNKNOWN_MEMBER_ID, resetting generation
2022-06-24 14:04:59,148 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-27, groupId=group_id] Attempt to heartbeat with Generation{generationId=72, memberId='consumer-group_id-27-ec2d4dc8-7736-441c-8dc7-15893f4cceaf', protocol='range'} and group instance id Optional.empty failed due to UNKNOWN_MEMBER_ID, resetting generation
2022-06-24 14:04:59,149 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-25, groupId=group_id] Attempt to heartbeat with Generation{generationId=72, memberId='consumer-group_id-25-93ffa8e1-de40-49cf-8f03-d340f1acd539', protocol='range'} and group instance id Optional.empty failed due to UNKNOWN_MEMBER_ID, resetting generation
2022-06-24 14:04:59,149 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Attempt to heartbeat with Generation{generationId=72, memberId='consumer-group_id-30-9b3382db-9efc-4470-ae1d-f81e4b8eb2ae', protocol='range'} and group instance id Optional.empty failed due to UNKNOWN_MEMBER_ID, resetting generation
2022-06-24 14:04:59,184 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-26, groupId=group_id] Giving away all assigned partitions as lost since generation has been reset,indicating that consumer is no longer part of the group
2022-06-24 14:04:59,184 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-32, groupId=group_id] Giving away all assigned partitions as lost since generation has been reset,indicating that consumer is no longer part of the group
2022-06-24 14:04:59,184 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-26, groupId=group_id] Lost previously assigned partitions createUser-0
2022-06-24 14:04:59,185 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] group_id: partitions lost: [createUser-0]
2022-06-24 14:04:59,185 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-27, groupId=group_id] Giving away all assigned partitions as lost since generation has been reset,indicating that consumer is no longer part of the group
2022-06-24 14:04:59,185 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-31, groupId=group_id] (Re-)joining group
2022-06-24 14:04:59,185 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-29, groupId=group_id] (Re-)joining group
2022-06-24 14:04:59,186 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-32, groupId=group_id] Lost previously assigned partitions createCompany-0
2022-06-24 14:04:59,185 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] group_id: partitions revoked: [createUser-0]
2022-06-24 14:04:59,185 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-27, groupId=group_id] Lost previously assigned partitions removeSector-0
2022-06-24 14:04:59,186 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-25, groupId=group_id] Giving away all assigned partitions as lost since generation has been reset,indicating that consumer is no longer part of the group
2022-06-24 14:04:59,186 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Giving away all assigned partitions as lost since generation has been reset,indicating that consumer is no longer part of the group
2022-06-24 14:04:59,187 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-25, groupId=group_id] Lost previously assigned partitions createStock-0
2022-06-24 14:04:59,187 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Lost previously assigned partitions updateStock-0
2022-06-24 14:04:59,187 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] group_id: partitions lost: [createStock-0]
2022-06-24 14:04:59,187 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] group_id: partitions lost: [createCompany-0]
2022-06-24 14:04:59,187 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] group_id: partitions revoked: [createStock-0]
2022-06-24 14:04:59,188 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-25, groupId=group_id] (Re-)joining group
2022-06-24 14:04:59,188 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] group_id: partitions lost: [removeSector-0]
2022-06-24 14:04:59,188 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-26, groupId=group_id] (Re-)joining group
2022-06-24 14:04:59,189 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] group_id: partitions lost: [updateStock-0]
2022-06-24 14:04:59,189 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] group_id: partitions revoked: [createCompany-0]
2022-06-24 14:04:59,189 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] group_id: partitions revoked: [removeSector-0]
2022-06-24 14:04:59,189 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-27, groupId=group_id] (Re-)joining group
2022-06-24 14:04:59,189 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-32, groupId=group_id] (Re-)joining group
2022-06-24 14:04:59,190 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] group_id: partitions revoked: [updateStock-0]
2022-06-24 14:04:59,190 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] (Re-)joining group
2022-06-24 14:04:59,210 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-27, groupId=group_id] (Re-)joining group
2022-06-24 14:04:59,212 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-29, groupId=group_id] (Re-)joining group
2022-06-24 14:04:59,213 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-31, groupId=group_id] (Re-)joining group
2022-06-24 14:04:59,280 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-28, groupId=group_id] (Re-)joining group
2022-06-24 14:04:59,412 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] (Re-)joining group
2022-06-24 14:04:59,413 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-26, groupId=group_id] (Re-)joining group
2022-06-24 14:04:59,414 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-28, groupId=group_id] (Re-)joining group
2022-06-24 14:04:59,415 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-25, groupId=group_id] (Re-)joining group
2022-06-24 14:04:59,416 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-32, groupId=group_id] (Re-)joining group
2022-06-24 14:04:59,833 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-25, groupId=group_id] Successfully joined group with generation Generation{generationId=74, memberId='consumer-group_id-25-5e005694-2858-42fd-86a4-ac53305779e8', protocol='range'}
2022-06-24 14:04:59,835 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-29, groupId=group_id] Successfully joined group with generation Generation{generationId=74, memberId='consumer-group_id-29-01bbfa1a-d699-4c4e-b7ac-e478e6c32ee8', protocol='range'}
2022-06-24 14:04:59,837 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-31, groupId=group_id] Successfully joined group with generation Generation{generationId=74, memberId='consumer-group_id-31-90215034-e069-45c5-9a13-2ef057d6d7f1', protocol='range'}
2022-06-24 14:04:59,839 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-29, groupId=group_id] Finished assignment for group at generation 74: {consumer-group_id-30-6a91f445-8c59-4875-9537-7d8a701dfad0=Assignment(partitions=[updateStock-0]), consumer-group_id-27-ea30e505-29b8-4e6e-a792-7a468bc9cbd3=Assignment(partitions=[removeSector-0]), consumer-group_id-26-0904ec6a-7069-4ec8-b3ec-caad88f61f23=Assignment(partitions=[createUser-0]), consumer-group_id-25-5e005694-2858-42fd-86a4-ac53305779e8=Assignment(partitions=[createStock-0]), consumer-group_id-31-90215034-e069-45c5-9a13-2ef057d6d7f1=Assignment(partitions=[removeCompany-0]), consumer-group_id-32-ce77434d-a973-4cec-bd3c-f6f33a8765f9=Assignment(partitions=[createCompany-0]), consumer-group_id-28-7b67cce9-b390-4abc-ac08-c81adc5085fd=Assignment(partitions=[createSector-0]), consumer-group_id-29-01bbfa1a-d699-4c4e-b7ac-e478e6c32ee8=Assignment(partitions=[removeStock-0])}
2022-06-24 14:04:59,840 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-26, groupId=group_id] Successfully joined group with generation Generation{generationId=74, memberId='consumer-group_id-26-0904ec6a-7069-4ec8-b3ec-caad88f61f23', protocol='range'}
2022-06-24 14:04:59,842 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-27, groupId=group_id] Successfully joined group with generation Generation{generationId=74, memberId='consumer-group_id-27-ea30e505-29b8-4e6e-a792-7a468bc9cbd3', protocol='range'}
2022-06-24 14:04:59,843 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-32, groupId=group_id] Successfully joined group with generation Generation{generationId=74, memberId='consumer-group_id-32-ce77434d-a973-4cec-bd3c-f6f33a8765f9', protocol='range'}
2022-06-24 14:04:59,843 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-28, groupId=group_id] Successfully joined group with generation Generation{generationId=74, memberId='consumer-group_id-28-7b67cce9-b390-4abc-ac08-c81adc5085fd', protocol='range'}
2022-06-24 14:04:59,844 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Successfully joined group with generation Generation{generationId=74, memberId='consumer-group_id-30-6a91f445-8c59-4875-9537-7d8a701dfad0', protocol='range'}
2022-06-24 14:05:00,136 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-25, groupId=group_id] Successfully synced group in generation Generation{generationId=74, memberId='consumer-group_id-25-5e005694-2858-42fd-86a4-ac53305779e8', protocol='range'}
2022-06-24 14:05:00,137 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-29, groupId=group_id] Successfully synced group in generation Generation{generationId=74, memberId='consumer-group_id-29-01bbfa1a-d699-4c4e-b7ac-e478e6c32ee8', protocol='range'}
2022-06-24 14:05:00,137 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-31, groupId=group_id] Successfully synced group in generation Generation{generationId=74, memberId='consumer-group_id-31-90215034-e069-45c5-9a13-2ef057d6d7f1', protocol='range'}
2022-06-24 14:05:00,137 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-29, groupId=group_id] Notifying assignor about the new Assignment(partitions=[removeStock-0])
2022-06-24 14:05:00,138 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-31, groupId=group_id] Notifying assignor about the new Assignment(partitions=[removeCompany-0])
2022-06-24 14:05:00,138 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-31, groupId=group_id] Adding newly assigned partitions: removeCompany-0
2022-06-24 14:05:00,138 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-26, groupId=group_id] Successfully synced group in generation Generation{generationId=74, memberId='consumer-group_id-26-0904ec6a-7069-4ec8-b3ec-caad88f61f23', protocol='range'}
2022-06-24 14:05:00,139 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-26, groupId=group_id] Notifying assignor about the new Assignment(partitions=[createUser-0])
2022-06-24 14:05:00,137 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-25, groupId=group_id] Notifying assignor about the new Assignment(partitions=[createStock-0])
2022-06-24 14:05:00,139 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-26, groupId=group_id] Adding newly assigned partitions: createUser-0
2022-06-24 14:05:00,139 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-25, groupId=group_id] Adding newly assigned partitions: createStock-0
2022-06-24 14:05:00,138 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-29, groupId=group_id] Adding newly assigned partitions: removeStock-0
2022-06-24 14:05:00,139 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-27, groupId=group_id] Successfully synced group in generation Generation{generationId=74, memberId='consumer-group_id-27-ea30e505-29b8-4e6e-a792-7a468bc9cbd3', protocol='range'}
2022-06-24 14:05:00,140 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-32, groupId=group_id] Successfully synced group in generation Generation{generationId=74, memberId='consumer-group_id-32-ce77434d-a973-4cec-bd3c-f6f33a8765f9', protocol='range'}
2022-06-24 14:05:00,140 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-28, groupId=group_id] Successfully synced group in generation Generation{generationId=74, memberId='consumer-group_id-28-7b67cce9-b390-4abc-ac08-c81adc5085fd', protocol='range'}
2022-06-24 14:05:00,141 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Successfully synced group in generation Generation{generationId=74, memberId='consumer-group_id-30-6a91f445-8c59-4875-9537-7d8a701dfad0', protocol='range'}
2022-06-24 14:05:00,146 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-25, groupId=group_id] Setting offset for partition createStock-0 to the committed offset FetchPosition{offset=3, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:05:00,146 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-31, groupId=group_id] Setting offset for partition removeCompany-0 to the committed offset FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:05:00,166 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Notifying assignor about the new Assignment(partitions=[updateStock-0])
2022-06-24 14:05:00,166 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Adding newly assigned partitions: updateStock-0
2022-06-24 14:05:00,167 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-32, groupId=group_id] Notifying assignor about the new Assignment(partitions=[createCompany-0])
2022-06-24 14:05:00,167 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] group_id: partitions assigned: [createStock-0]
2022-06-24 14:05:00,167 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-32, groupId=group_id] Adding newly assigned partitions: createCompany-0
2022-06-24 14:05:00,166 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-27, groupId=group_id] Notifying assignor about the new Assignment(partitions=[removeSector-0])
2022-06-24 14:05:00,166 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-28, groupId=group_id] Notifying assignor about the new Assignment(partitions=[createSector-0])
2022-06-24 14:05:00,167 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-27, groupId=group_id] Adding newly assigned partitions: removeSector-0
2022-06-24 14:05:00,167 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-28, groupId=group_id] Adding newly assigned partitions: createSector-0
2022-06-24 14:05:00,169 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-32, groupId=group_id] Setting offset for partition createCompany-0 to the committed offset FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:05:00,169 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Setting offset for partition updateStock-0 to the committed offset FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:05:00,169 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] group_id: partitions assigned: [updateStock-0]
2022-06-24 14:05:00,170 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] group_id: partitions assigned: [createCompany-0]
2022-06-24 14:05:00,170 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] group_id: partitions assigned: [removeCompany-0]
2022-06-24 14:05:00,172 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-27, groupId=group_id] Setting offset for partition removeSector-0 to the committed offset FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:05:00,172 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] group_id: partitions assigned: [removeSector-0]
2022-06-24 14:05:00,173 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-28, groupId=group_id] Setting offset for partition createSector-0 to the committed offset FetchPosition{offset=1, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:05:00,173 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] group_id: partitions assigned: [createSector-0]
2022-06-24 14:05:00,177 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-26, groupId=group_id] Setting offset for partition createUser-0 to the committed offset FetchPosition{offset=1, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:05:00,177 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] group_id: partitions assigned: [createUser-0]
2022-06-24 14:05:00,181 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-29, groupId=group_id] Setting offset for partition removeStock-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:05:00,181 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] group_id: partitions assigned: [removeStock-0]
2022-06-24 12:33:05,363 INFO com.mongodb.diagnostics.logging.SLF4JLogger [cluster-ClusterId{value='62b33f0a52d23d5bec795f97', description='null'}-localhost:27017] Exception in monitor thread while connecting to server localhost:27017
com.mongodb.MongoSocketReadTimeoutException: Timeout while receiving message
	at com.mongodb.internal.connection.InternalStreamConnection.translateReadException(InternalStreamConnection.java:630)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:515)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:355)
	at com.mongodb.internal.connection.InternalStreamConnection.receive(InternalStreamConnection.java:315)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:215)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:144)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.SocketTimeoutException: Read timed out
	at java.base/java.net.SocketInputStream.socketRead0(Native Method)
	at java.base/java.net.SocketInputStream.socketRead(SocketInputStream.java:115)
	at java.base/java.net.SocketInputStream.read(SocketInputStream.java:168)
	at java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:109)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:131)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:647)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:512)
	... 5 common frames omitted
2022-06-24 12:33:05,367 INFO com.mongodb.diagnostics.logging.SLF4JLogger [cluster-ClusterId{value='62b33f0a52d23d5bec795f97', description='null'}-localhost:27017] Opened connection [connectionId{localValue:18, serverValue:56}] to localhost:27017
2022-06-24 12:33:05,367 INFO com.mongodb.diagnostics.logging.SLF4JLogger [cluster-ClusterId{value='62b33f0a52d23d5bec795f97', description='null'}-localhost:27017] Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=3171100}
2022-06-24 12:35:27,505 INFO com.netflix.discovery.shared.resolver.aws.ConfigClusterResolver [AsyncResolver-bootstrap-executor-0] Resolving eureka endpoints via configuration
2022-06-24 12:40:27,521 INFO com.netflix.discovery.shared.resolver.aws.ConfigClusterResolver [AsyncResolver-bootstrap-executor-0] Resolving eureka endpoints via configuration
2022-06-24 12:41:51,363 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [kafka-coordinator-heartbeat-thread | group_id] [Consumer clientId=consumer-group_id-28, groupId=group_id] Group coordinator host.docker.internal:9092 (id: 2147483647 rack: null) is unavailable or invalid due to cause: coordinator unavailable.isDisconnected: true. Rediscovery will be attempted.
2022-06-24 12:41:51,363 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [kafka-coordinator-heartbeat-thread | group_id] [Consumer clientId=consumer-group_id-30, groupId=group_id] Group coordinator host.docker.internal:9092 (id: 2147483647 rack: null) is unavailable or invalid due to cause: coordinator unavailable.isDisconnected: true. Rediscovery will be attempted.
2022-06-24 12:41:51,365 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-28, groupId=group_id] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2022-06-24 12:41:51,365 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2022-06-24 12:41:51,366 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-28, groupId=group_id] Group coordinator host.docker.internal:9092 (id: 2147483647 rack: null) is unavailable or invalid due to cause: coordinator unavailable.isDisconnected: true. Rediscovery will be attempted.
2022-06-24 12:41:51,366 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Group coordinator host.docker.internal:9092 (id: 2147483647 rack: null) is unavailable or invalid due to cause: coordinator unavailable.isDisconnected: true. Rediscovery will be attempted.
2022-06-24 12:41:51,394 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [kafka-coordinator-heartbeat-thread | group_id] [Consumer clientId=consumer-group_id-25, groupId=group_id] Group coordinator host.docker.internal:9092 (id: 2147483647 rack: null) is unavailable or invalid due to cause: coordinator unavailable.isDisconnected: true. Rediscovery will be attempted.
2022-06-24 12:41:51,395 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-25, groupId=group_id] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2022-06-24 12:41:51,396 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-25, groupId=group_id] Group coordinator host.docker.internal:9092 (id: 2147483647 rack: null) is unavailable or invalid due to cause: coordinator unavailable.isDisconnected: true. Rediscovery will be attempted.
2022-06-24 12:41:51,475 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [kafka-coordinator-heartbeat-thread | group_id] [Consumer clientId=consumer-group_id-26, groupId=group_id] Group coordinator host.docker.internal:9092 (id: 2147483647 rack: null) is unavailable or invalid due to cause: coordinator unavailable.isDisconnected: true. Rediscovery will be attempted.
2022-06-24 12:41:51,475 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [kafka-coordinator-heartbeat-thread | group_id] [Consumer clientId=consumer-group_id-29, groupId=group_id] Group coordinator host.docker.internal:9092 (id: 2147483647 rack: null) is unavailable or invalid due to cause: coordinator unavailable.isDisconnected: true. Rediscovery will be attempted.
2022-06-24 12:41:51,475 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [kafka-coordinator-heartbeat-thread | group_id] [Consumer clientId=consumer-group_id-32, groupId=group_id] Group coordinator host.docker.internal:9092 (id: 2147483647 rack: null) is unavailable or invalid due to cause: coordinator unavailable.isDisconnected: true. Rediscovery will be attempted.
2022-06-24 12:41:51,476 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-29, groupId=group_id] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2022-06-24 12:41:51,476 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-26, groupId=group_id] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2022-06-24 12:41:51,476 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-29, groupId=group_id] Group coordinator host.docker.internal:9092 (id: 2147483647 rack: null) is unavailable or invalid due to cause: coordinator unavailable.isDisconnected: true. Rediscovery will be attempted.
2022-06-24 12:41:51,476 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-26, groupId=group_id] Group coordinator host.docker.internal:9092 (id: 2147483647 rack: null) is unavailable or invalid due to cause: coordinator unavailable.isDisconnected: true. Rediscovery will be attempted.
2022-06-24 12:41:51,543 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [kafka-coordinator-heartbeat-thread | group_id] [Consumer clientId=consumer-group_id-27, groupId=group_id] Group coordinator host.docker.internal:9092 (id: 2147483647 rack: null) is unavailable or invalid due to cause: coordinator unavailable.isDisconnected: true. Rediscovery will be attempted.
2022-06-24 12:41:51,543 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [kafka-coordinator-heartbeat-thread | group_id] [Consumer clientId=consumer-group_id-31, groupId=group_id] Group coordinator host.docker.internal:9092 (id: 2147483647 rack: null) is unavailable or invalid due to cause: coordinator unavailable.isDisconnected: true. Rediscovery will be attempted.
2022-06-24 12:41:51,544 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-27, groupId=group_id] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2022-06-24 12:41:51,544 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-27, groupId=group_id] Group coordinator host.docker.internal:9092 (id: 2147483647 rack: null) is unavailable or invalid due to cause: coordinator unavailable.isDisconnected: true. Rediscovery will be attempted.
2022-06-24 12:41:51,544 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-31, groupId=group_id] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2022-06-24 12:41:51,545 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-31, groupId=group_id] Group coordinator host.docker.internal:9092 (id: 2147483647 rack: null) is unavailable or invalid due to cause: coordinator unavailable.isDisconnected: true. Rediscovery will be attempted.
2022-06-24 12:45:27,533 INFO com.netflix.discovery.shared.resolver.aws.ConfigClusterResolver [AsyncResolver-bootstrap-executor-0] Resolving eureka endpoints via configuration
2022-06-24 12:50:31,022 INFO com.netflix.discovery.shared.resolver.aws.ConfigClusterResolver [AsyncResolver-bootstrap-executor-0] Resolving eureka endpoints via configuration
2022-06-24 12:55:31,075 INFO com.netflix.discovery.shared.resolver.aws.ConfigClusterResolver [AsyncResolver-bootstrap-executor-0] Resolving eureka endpoints via configuration
2022-06-24 13:00:31,082 INFO com.netflix.discovery.shared.resolver.aws.ConfigClusterResolver [AsyncResolver-bootstrap-executor-0] Resolving eureka endpoints via configuration
2022-06-24 13:05:31,095 INFO com.netflix.discovery.shared.resolver.aws.ConfigClusterResolver [AsyncResolver-bootstrap-executor-0] Resolving eureka endpoints via configuration
2022-06-24 13:10:09,914 INFO com.estockmarket.query.application.controller.CompanyController [http-nio-6093-exec-5] get all company details 
2022-06-24 13:10:09,915 INFO com.estockmarket.query.domain.service.CompanyService [http-nio-6093-exec-5] fetch all company {}
2022-06-24 13:10:09,945 INFO com.mongodb.diagnostics.logging.SLF4JLogger [http-nio-6093-exec-5] Closed connection [connectionId{localValue:16, serverValue:54}] to localhost:27017 because there was a socket exception raised on another connection from this pool.
2022-06-24 13:10:09,947 INFO com.mongodb.diagnostics.logging.SLF4JLogger [http-nio-6093-exec-5] Closed connection [connectionId{localValue:15, serverValue:53}] to localhost:27017 because there was a socket exception raised on another connection from this pool.
2022-06-24 13:10:10,022 INFO com.mongodb.diagnostics.logging.SLF4JLogger [http-nio-6093-exec-5] Opened connection [connectionId{localValue:19, serverValue:57}] to localhost:27017
2022-06-24 13:10:10,038 INFO com.estockmarket.query.domain.service.CompanyService [http-nio-6093-exec-5] convert to company dto com.estockmarket.query.domain.model.Company@4a24bdac
2022-06-24 13:10:11,494 INFO com.estockmarket.query.application.controller.CompanyController [http-nio-6093-exec-1] get all company details 
2022-06-24 13:10:11,495 INFO com.estockmarket.query.domain.service.CompanyService [http-nio-6093-exec-1] fetch all company {}
2022-06-24 13:10:11,497 INFO com.estockmarket.query.domain.service.CompanyService [http-nio-6093-exec-1] convert to company dto com.estockmarket.query.domain.model.Company@5285a7d9
2022-06-24 13:10:13,728 INFO com.estockmarket.query.application.controller.CompanyController [http-nio-6093-exec-8] get all company details 
2022-06-24 13:10:13,729 INFO com.estockmarket.query.domain.service.CompanyService [http-nio-6093-exec-8] fetch all company {}
2022-06-24 13:10:13,731 INFO com.estockmarket.query.domain.service.CompanyService [http-nio-6093-exec-8] convert to company dto com.estockmarket.query.domain.model.Company@51fd911b
2022-06-24 13:10:31,112 INFO com.netflix.discovery.shared.resolver.aws.ConfigClusterResolver [AsyncResolver-bootstrap-executor-0] Resolving eureka endpoints via configuration
2022-06-24 13:10:37,374 INFO com.estockmarket.query.application.controller.CompanyController [http-nio-6093-exec-4] get all company details 
2022-06-24 13:10:37,374 INFO com.estockmarket.query.domain.service.CompanyService [http-nio-6093-exec-4] fetch all company {}
2022-06-24 13:10:37,376 INFO com.estockmarket.query.domain.service.CompanyService [http-nio-6093-exec-4] convert to company dto com.estockmarket.query.domain.model.Company@7dcdc90b
2022-06-24 13:10:39,886 INFO com.estockmarket.query.application.controller.StockController [http-nio-6093-exec-2] fetch min,max and average stock price based on company code,date C-008
2022-06-24 13:10:39,886 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-2] fetch stockaggregate based on code,date C-008
2022-06-24 13:10:39,887 INFO com.estockmarket.query.application.controller.StockController [http-nio-6093-exec-10] fetch stocks based on code,date C-008
2022-06-24 13:10:39,887 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-10] fetch company stocks based on code,date C-008
2022-06-24 13:10:39,888 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-10] fetch stock query based on code C-008
2022-06-24 13:10:39,887 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-2] fetch stock query based on code C-008
2022-06-24 13:10:39,987 INFO com.mongodb.diagnostics.logging.SLF4JLogger [http-nio-6093-exec-2] Opened connection [connectionId{localValue:20, serverValue:58}] to localhost:27017
2022-06-24 13:15:31,125 INFO com.netflix.discovery.shared.resolver.aws.ConfigClusterResolver [AsyncResolver-bootstrap-executor-0] Resolving eureka endpoints via configuration
2022-06-24 13:20:31,194 INFO com.netflix.discovery.shared.resolver.aws.ConfigClusterResolver [AsyncResolver-bootstrap-executor-0] Resolving eureka endpoints via configuration
2022-06-24 13:20:33,326 INFO com.estockmarket.query.application.controller.CompanyController [http-nio-6093-exec-3] get all company details 
2022-06-24 13:20:33,326 INFO com.estockmarket.query.domain.service.CompanyService [http-nio-6093-exec-3] fetch all company {}
2022-06-24 13:20:33,330 INFO com.estockmarket.query.domain.service.CompanyService [http-nio-6093-exec-3] convert to company dto com.estockmarket.query.domain.model.Company@6337d236
2022-06-24 13:20:49,471 INFO com.estockmarket.query.application.controller.CompanyController [http-nio-6093-exec-9] get all company details 
2022-06-24 13:20:49,472 INFO com.estockmarket.query.domain.service.CompanyService [http-nio-6093-exec-9] fetch all company {}
2022-06-24 13:20:49,474 INFO com.estockmarket.query.domain.service.CompanyService [http-nio-6093-exec-9] convert to company dto com.estockmarket.query.domain.model.Company@14a734b6
2022-06-24 13:25:31,206 INFO com.netflix.discovery.shared.resolver.aws.ConfigClusterResolver [AsyncResolver-bootstrap-executor-0] Resolving eureka endpoints via configuration
2022-06-24 13:27:26,421 INFO com.estockmarket.query.application.controller.CompanyController [http-nio-6093-exec-1] get all company details 
2022-06-24 13:27:26,422 INFO com.estockmarket.query.domain.service.CompanyService [http-nio-6093-exec-1] fetch all company {}
2022-06-24 13:27:26,425 INFO com.estockmarket.query.domain.service.CompanyService [http-nio-6093-exec-1] convert to company dto com.estockmarket.query.domain.model.Company@7b34f3c1
2022-06-24 13:27:32,874 INFO com.estockmarket.query.application.controller.StockController [http-nio-6093-exec-8] fetch stocks based on code,date C-008
2022-06-24 13:27:32,874 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-8] fetch company stocks based on code,date C-008
2022-06-24 13:27:32,874 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-8] fetch stock query based on code C-008
2022-06-24 13:27:32,880 INFO com.estockmarket.query.application.controller.StockController [http-nio-6093-exec-4] fetch min,max and average stock price based on company code,date C-008
2022-06-24 13:27:32,881 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-4] fetch stockaggregate based on code,date C-008
2022-06-24 13:27:32,881 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-4] fetch stock query based on code C-008
2022-06-24 13:28:51,905 INFO com.estockmarket.query.application.controller.CompanyController [http-nio-6093-exec-7] get all company details 
2022-06-24 13:28:51,905 INFO com.estockmarket.query.domain.service.CompanyService [http-nio-6093-exec-7] fetch all company {}
2022-06-24 13:28:51,908 INFO com.estockmarket.query.domain.service.CompanyService [http-nio-6093-exec-7] convert to company dto com.estockmarket.query.domain.model.Company@34ba3f8d
2022-06-24 13:28:55,045 INFO com.estockmarket.query.application.controller.StockController [http-nio-6093-exec-6] fetch min,max and average stock price based on company code,date C-008
2022-06-24 13:28:55,045 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-6] fetch stockaggregate based on code,date C-008
2022-06-24 13:28:55,046 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-6] fetch stock query based on code C-008
2022-06-24 13:28:55,098 INFO com.estockmarket.query.application.controller.StockController [http-nio-6093-exec-3] fetch stocks based on code,date C-008
2022-06-24 13:28:55,098 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-3] fetch company stocks based on code,date C-008
2022-06-24 13:28:55,099 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-3] fetch stock query based on code C-008
2022-06-24 13:28:59,642 INFO com.estockmarket.query.application.controller.StockController [http-nio-6093-exec-5] fetch min,max and average stock price based on company code,date C-008
2022-06-24 13:28:59,643 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-5] fetch stockaggregate based on code,date C-008
2022-06-24 13:28:59,643 INFO com.estockmarket.query.application.controller.StockController [http-nio-6093-exec-9] fetch stocks based on code,date C-008
2022-06-24 13:28:59,643 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-5] fetch stock query based on code C-008
2022-06-24 13:28:59,643 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-9] fetch company stocks based on code,date C-008
2022-06-24 13:28:59,643 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-9] fetch stock query based on code C-008
2022-06-24 13:30:31,226 INFO com.netflix.discovery.shared.resolver.aws.ConfigClusterResolver [AsyncResolver-bootstrap-executor-0] Resolving eureka endpoints via configuration
2022-06-24 13:30:42,946 INFO com.estockmarket.query.application.controller.UserController [http-nio-6093-exec-8] authenticate user com.estockmarket.query.domain.util.JwtRequest@1c571958
2022-06-24 13:30:42,946 INFO com.estockmarket.query.domain.service.UserService [http-nio-6093-exec-8] user authenticatecom.estockmarket.query.domain.util.JwtRequest@1c571958
2022-06-24 13:32:42,332 INFO com.estockmarket.query.application.controller.StockController [http-nio-6093-exec-2] fetch stocks based on code,date C-008
2022-06-24 13:32:42,332 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-2] fetch company stocks based on code,date C-008
2022-06-24 13:32:42,332 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-2] fetch stock query based on code C-008
2022-06-24 13:33:37,845 INFO com.estockmarket.query.application.controller.CompanyController [http-nio-6093-exec-7] get all company details 
2022-06-24 13:33:37,846 INFO com.estockmarket.query.domain.service.CompanyService [http-nio-6093-exec-7] fetch all company {}
2022-06-24 13:33:37,848 INFO com.estockmarket.query.domain.service.CompanyService [http-nio-6093-exec-7] convert to company dto com.estockmarket.query.domain.model.Company@2a0a110c
2022-06-24 13:33:45,896 INFO com.estockmarket.query.application.controller.StockController [http-nio-6093-exec-6] fetch min,max and average stock price based on company code,date C-008
2022-06-24 13:33:45,896 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-6] fetch stockaggregate based on code,date C-008
2022-06-24 13:33:45,896 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-6] fetch stock query based on code C-008
2022-06-24 13:33:45,906 INFO com.estockmarket.query.application.controller.StockController [http-nio-6093-exec-3] fetch stocks based on code,date C-008
2022-06-24 13:33:45,906 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-3] fetch company stocks based on code,date C-008
2022-06-24 13:33:45,907 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-3] fetch stock query based on code C-008
2022-06-24 13:35:31,238 INFO com.netflix.discovery.shared.resolver.aws.ConfigClusterResolver [AsyncResolver-bootstrap-executor-0] Resolving eureka endpoints via configuration
2022-06-24 13:40:31,249 INFO com.netflix.discovery.shared.resolver.aws.ConfigClusterResolver [AsyncResolver-bootstrap-executor-0] Resolving eureka endpoints via configuration
2022-06-24 13:45:31,256 INFO com.netflix.discovery.shared.resolver.aws.ConfigClusterResolver [AsyncResolver-bootstrap-executor-0] Resolving eureka endpoints via configuration
2022-06-24 13:50:31,262 INFO com.netflix.discovery.shared.resolver.aws.ConfigClusterResolver [AsyncResolver-bootstrap-executor-0] Resolving eureka endpoints via configuration
2022-06-24 13:55:31,275 INFO com.netflix.discovery.shared.resolver.aws.ConfigClusterResolver [AsyncResolver-bootstrap-executor-0] Resolving eureka endpoints via configuration
2022-06-24 13:55:42,865 INFO com.estockmarket.query.application.controller.StockController [http-nio-6093-exec-4] fetch stocks based on code,date C-008
2022-06-24 13:55:42,867 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-4] fetch company stocks based on code,date C-008
2022-06-24 13:55:42,868 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-4] fetch stock query based on code C-008
2022-06-24 13:55:42,865 INFO com.estockmarket.query.application.controller.StockController [http-nio-6093-exec-1] fetch min,max and average stock price based on company code,date C-008
2022-06-24 13:55:42,869 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-1] fetch stockaggregate based on code,date C-008
2022-06-24 13:55:42,869 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-1] fetch stock query based on code C-008
2022-06-24 13:55:49,500 INFO com.estockmarket.query.application.controller.StockController [http-nio-6093-exec-8] fetch min,max and average stock price based on company code,date C-008
2022-06-24 13:55:49,500 INFO com.estockmarket.query.application.controller.StockController [http-nio-6093-exec-10] fetch stocks based on code,date C-008
2022-06-24 13:55:49,501 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-10] fetch company stocks based on code,date C-008
2022-06-24 13:55:49,501 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-10] fetch stock query based on code C-008
2022-06-24 13:55:49,500 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-8] fetch stockaggregate based on code,date C-008
2022-06-24 13:55:49,501 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-8] fetch stock query based on code C-008
2022-06-24 13:57:03,163 INFO com.estockmarket.query.application.controller.CompanyController [http-nio-6093-exec-6] get all company details 
2022-06-24 13:57:03,164 INFO com.estockmarket.query.domain.service.CompanyService [http-nio-6093-exec-6] fetch all company {}
2022-06-24 13:57:03,166 INFO com.estockmarket.query.domain.service.CompanyService [http-nio-6093-exec-6] convert to company dto com.estockmarket.query.domain.model.Company@1fd13fb4
2022-06-24 13:57:29,982 INFO com.estockmarket.query.application.controller.StockController [http-nio-6093-exec-3] fetch stocks based on code,date C-008
2022-06-24 13:57:29,982 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-3] fetch company stocks based on code,date C-008
2022-06-24 13:57:29,983 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-3] fetch stock query based on code C-008
2022-06-24 13:57:30,063 INFO com.estockmarket.query.application.controller.StockController [http-nio-6093-exec-5] fetch min,max and average stock price based on company code,date C-008
2022-06-24 13:57:30,064 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-5] fetch stockaggregate based on code,date C-008
2022-06-24 13:57:30,064 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-5] fetch stock query based on code C-008
2022-06-24 13:58:05,677 INFO com.estockmarket.query.application.controller.CompanyController [http-nio-6093-exec-9] get all company details 
2022-06-24 13:58:05,678 INFO com.estockmarket.query.domain.service.CompanyService [http-nio-6093-exec-9] fetch all company {}
2022-06-24 13:58:05,681 INFO com.estockmarket.query.domain.service.CompanyService [http-nio-6093-exec-9] convert to company dto com.estockmarket.query.domain.model.Company@40b1c4d7
2022-06-24 13:58:13,153 INFO com.estockmarket.query.application.controller.StockController [http-nio-6093-exec-4] fetch stocks based on code,date C-008
2022-06-24 13:58:13,155 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-4] fetch company stocks based on code,date C-008
2022-06-24 13:58:13,155 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-4] fetch stock query based on code C-008
2022-06-24 13:58:13,159 INFO com.estockmarket.query.application.controller.StockController [http-nio-6093-exec-1] fetch min,max and average stock price based on company code,date C-008
2022-06-24 13:58:13,159 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-1] fetch stockaggregate based on code,date C-008
2022-06-24 13:58:13,159 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-1] fetch stock query based on code C-008
2022-06-24 13:59:23,495 INFO com.estockmarket.query.application.controller.CompanyController [http-nio-6093-exec-2] get all company details 
2022-06-24 13:59:23,495 INFO com.estockmarket.query.domain.service.CompanyService [http-nio-6093-exec-2] fetch all company {}
2022-06-24 13:59:23,500 INFO com.estockmarket.query.domain.service.CompanyService [http-nio-6093-exec-2] convert to company dto com.estockmarket.query.domain.model.Company@1ce5f6be
2022-06-24 13:59:30,772 INFO com.estockmarket.query.application.controller.StockController [http-nio-6093-exec-7] fetch min,max and average stock price based on company code,date C-008
2022-06-24 13:59:30,773 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-7] fetch stockaggregate based on code,date C-008
2022-06-24 13:59:30,774 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-7] fetch stock query based on code C-008
2022-06-24 13:59:30,786 INFO com.estockmarket.query.application.controller.StockController [http-nio-6093-exec-6] fetch stocks based on code,date C-008
2022-06-24 13:59:30,786 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-6] fetch company stocks based on code,date C-008
2022-06-24 13:59:30,787 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-6] fetch stock query based on code C-008
2022-06-24 13:59:55,413 INFO com.estockmarket.query.application.controller.UserController [http-nio-6093-exec-3] generateotp priya26397@gmail.com
2022-06-24 13:59:55,413 INFO com.estockmarket.query.domain.service.OTPService [http-nio-6093-exec-3] generate otp priya26397@gmail.com
2022-06-24 13:59:55,415 INFO com.estockmarket.query.domain.service.EmailTemplate [http-nio-6093-exec-3] Email template SendOtp.html
2022-06-24 13:59:55,416 INFO com.estockmarket.query.domain.service.EmailTemplate [http-nio-6093-exec-3] load template SendOtp.html
2022-06-24 13:59:55,435 INFO com.estockmarket.query.domain.service.EmailService [http-nio-6093-exec-3] send otp message to ,subject,messagepriya26397@gmail.com
2022-06-24 14:00:25,919 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-26, groupId=group_id] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2022-06-24 14:00:25,919 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-27, groupId=group_id] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2022-06-24 14:00:25,919 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-25, groupId=group_id] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2022-06-24 14:00:25,919 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-32, groupId=group_id] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2022-06-24 14:00:25,919 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-29, groupId=group_id] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2022-06-24 14:00:25,919 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2022-06-24 14:00:25,919 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-31, groupId=group_id] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2022-06-24 14:00:25,919 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-28, groupId=group_id] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2022-06-24 14:00:25,985 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks com.estockmarket.query.domain.model.Stocks@18a60556
2022-06-24 14:00:26,030 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks com.estockmarket.query.domain.model.Stocks@56ab17a5
2022-06-24 14:00:26,032 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks com.estockmarket.query.domain.model.Stocks@4e354862
2022-06-24 14:00:26,050 ERROR org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Offset commit failed on partition updateStock-0 at offset 2: The coordinator is not aware of this member.
2022-06-24 14:00:26,066 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] OffsetCommit failed with Generation{generationId=74, memberId='consumer-group_id-30-6a91f445-8c59-4875-9537-7d8a701dfad0', protocol='range'}: The coordinator is not aware of this member.
2022-06-24 14:00:26,067 ERROR org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Error handler threw an exception
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing max.poll.interval.ms or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:1256)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:1163)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:1173)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:1148)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:206)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:169)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:129)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:602)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:412)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:297)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:236)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:215)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.commitOffsetsSync(ConsumerCoordinator.java:1005)
	at org.apache.kafka.clients.consumer.KafkaConsumer.commitSync(KafkaConsumer.java:1495)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doCommitSync(KafkaMessageListenerContainer.java:2656)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.commitSync(KafkaMessageListenerContainer.java:2651)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.commitIfNecessary(KafkaMessageListenerContainer.java:2637)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.processCommits(KafkaMessageListenerContainer.java:2451)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2352)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2228)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2142)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2024)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1706)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
2022-06-24 14:00:26,068 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Giving away all assigned partitions as lost since generation has been reset,indicating that consumer is no longer part of the group
2022-06-24 14:00:26,068 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Lost previously assigned partitions updateStock-0
2022-06-24 14:00:26,069 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] group_id: partitions lost: [updateStock-0]
2022-06-24 14:00:26,069 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] group_id: partitions revoked: [updateStock-0]
2022-06-24 14:00:26,069 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] (Re-)joining group
2022-06-24 14:00:26,077 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] (Re-)joining group
2022-06-24 14:00:26,142 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Successfully joined group with generation Generation{generationId=76, memberId='consumer-group_id-30-da3b3d16-05bb-41f6-add4-53ae02e3c114', protocol='range'}
2022-06-24 14:00:26,143 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Finished assignment for group at generation 76: {consumer-group_id-30-da3b3d16-05bb-41f6-add4-53ae02e3c114=Assignment(partitions=[updateStock-0])}
2022-06-24 14:00:26,156 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Successfully synced group in generation Generation{generationId=76, memberId='consumer-group_id-30-da3b3d16-05bb-41f6-add4-53ae02e3c114', protocol='range'}
2022-06-24 14:00:26,157 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Notifying assignor about the new Assignment(partitions=[updateStock-0])
2022-06-24 14:00:26,157 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Adding newly assigned partitions: updateStock-0
2022-06-24 14:00:26,160 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Setting offset for partition updateStock-0 to the committed offset FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:00:26,161 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] group_id: partitions assigned: [updateStock-0]
2022-06-24 14:00:26,540 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks com.estockmarket.query.domain.model.Stocks@59da280d
2022-06-24 14:00:26,542 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks com.estockmarket.query.domain.model.Stocks@dc837e0
2022-06-24 14:00:26,544 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks com.estockmarket.query.domain.model.Stocks@72015535
2022-06-24 14:00:26,568 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Seeking to offset 2 for partition updateStock-0
2022-06-24 14:00:26,590 ERROR org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception; nested exception is org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:206)
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:112)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2359)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2228)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2142)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2024)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1706)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2371)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2342)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2303)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2217)
	... 9 common frames omitted
Caused by: org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.data.mongodb.core.MongoExceptionTranslator.translateExceptionIfPossible(MongoExceptionTranslator.java:106)
	at org.springframework.data.mongodb.core.MongoTemplate.potentiallyConvertRuntimeException(MongoTemplate.java:2899)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:555)
	at org.springframework.data.mongodb.core.MongoTemplate.insertDocument(MongoTemplate.java:1458)
	at org.springframework.data.mongodb.core.MongoTemplate.doInsert(MongoTemplate.java:1257)
	at org.springframework.data.mongodb.core.MongoTemplate.insert(MongoTemplate.java:1172)
	at org.springframework.data.mongodb.repository.support.SimpleMongoRepository.save(SimpleMongoRepository.java:91)
	at jdk.internal.reflect.GeneratedMethodAccessor267.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:289)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:137)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:121)
	at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:529)
	at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:285)
	at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:599)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:163)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:138)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:80)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at jdk.internal.reflect.GeneratedMethodAccessor266.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:344)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:198)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:137)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at com.estockmarket.query.domain.service.StockService.updateStock(StockService.java:105)
	at com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(KafkaStocksEventListener.java:36)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:56)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:347)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:92)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:53)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2322)
	... 11 common frames omitted
Caused by: com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at com.mongodb.client.internal.MongoCollectionImpl.executeSingleWriteRequest(MongoCollectionImpl.java:1017)
	at com.mongodb.client.internal.MongoCollectionImpl.executeInsertOne(MongoCollectionImpl.java:470)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:453)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:447)
	at org.springframework.data.mongodb.core.MongoTemplate.lambda$insertDocument$16(MongoTemplate.java:1464)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:553)
	... 57 common frames omitted
2022-06-24 14:00:27,057 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks com.estockmarket.query.domain.model.Stocks@150a099a
2022-06-24 14:00:27,085 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Seeking to offset 2 for partition updateStock-0
2022-06-24 14:00:27,085 ERROR org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception; nested exception is org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:206)
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:112)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2359)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2228)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2142)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2024)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1706)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2371)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2342)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2303)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2217)
	... 9 common frames omitted
Caused by: org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.data.mongodb.core.MongoExceptionTranslator.translateExceptionIfPossible(MongoExceptionTranslator.java:106)
	at org.springframework.data.mongodb.core.MongoTemplate.potentiallyConvertRuntimeException(MongoTemplate.java:2899)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:555)
	at org.springframework.data.mongodb.core.MongoTemplate.insertDocument(MongoTemplate.java:1458)
	at org.springframework.data.mongodb.core.MongoTemplate.doInsert(MongoTemplate.java:1257)
	at org.springframework.data.mongodb.core.MongoTemplate.insert(MongoTemplate.java:1172)
	at org.springframework.data.mongodb.repository.support.SimpleMongoRepository.save(SimpleMongoRepository.java:91)
	at jdk.internal.reflect.GeneratedMethodAccessor267.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:289)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:137)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:121)
	at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:529)
	at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:285)
	at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:599)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:163)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:138)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:80)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at jdk.internal.reflect.GeneratedMethodAccessor266.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:344)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:198)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:137)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at com.estockmarket.query.domain.service.StockService.updateStock(StockService.java:105)
	at com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(KafkaStocksEventListener.java:36)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:56)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:347)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:92)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:53)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2322)
	... 11 common frames omitted
Caused by: com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at com.mongodb.client.internal.MongoCollectionImpl.executeSingleWriteRequest(MongoCollectionImpl.java:1017)
	at com.mongodb.client.internal.MongoCollectionImpl.executeInsertOne(MongoCollectionImpl.java:470)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:453)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:447)
	at org.springframework.data.mongodb.core.MongoTemplate.lambda$insertDocument$16(MongoTemplate.java:1464)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:553)
	... 57 common frames omitted
2022-06-24 14:00:27,599 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks com.estockmarket.query.domain.model.Stocks@1d98c176
2022-06-24 14:00:27,627 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Seeking to offset 2 for partition updateStock-0
2022-06-24 14:00:27,628 ERROR org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception; nested exception is org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:206)
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:112)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2359)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2228)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2142)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2024)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1706)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2371)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2342)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2303)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2217)
	... 9 common frames omitted
Caused by: org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.data.mongodb.core.MongoExceptionTranslator.translateExceptionIfPossible(MongoExceptionTranslator.java:106)
	at org.springframework.data.mongodb.core.MongoTemplate.potentiallyConvertRuntimeException(MongoTemplate.java:2899)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:555)
	at org.springframework.data.mongodb.core.MongoTemplate.insertDocument(MongoTemplate.java:1458)
	at org.springframework.data.mongodb.core.MongoTemplate.doInsert(MongoTemplate.java:1257)
	at org.springframework.data.mongodb.core.MongoTemplate.insert(MongoTemplate.java:1172)
	at org.springframework.data.mongodb.repository.support.SimpleMongoRepository.save(SimpleMongoRepository.java:91)
	at jdk.internal.reflect.GeneratedMethodAccessor267.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:289)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:137)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:121)
	at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:529)
	at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:285)
	at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:599)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:163)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:138)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:80)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at jdk.internal.reflect.GeneratedMethodAccessor266.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:344)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:198)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:137)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at com.estockmarket.query.domain.service.StockService.updateStock(StockService.java:105)
	at com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(KafkaStocksEventListener.java:36)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:56)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:347)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:92)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:53)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2322)
	... 11 common frames omitted
Caused by: com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at com.mongodb.client.internal.MongoCollectionImpl.executeSingleWriteRequest(MongoCollectionImpl.java:1017)
	at com.mongodb.client.internal.MongoCollectionImpl.executeInsertOne(MongoCollectionImpl.java:470)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:453)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:447)
	at org.springframework.data.mongodb.core.MongoTemplate.lambda$insertDocument$16(MongoTemplate.java:1464)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:553)
	... 57 common frames omitted
2022-06-24 14:00:28,147 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks com.estockmarket.query.domain.model.Stocks@51239212
2022-06-24 14:00:28,175 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Seeking to offset 2 for partition updateStock-0
2022-06-24 14:00:28,176 ERROR org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception; nested exception is org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:206)
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:112)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2359)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2228)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2142)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2024)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1706)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2371)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2342)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2303)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2217)
	... 9 common frames omitted
Caused by: org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.data.mongodb.core.MongoExceptionTranslator.translateExceptionIfPossible(MongoExceptionTranslator.java:106)
	at org.springframework.data.mongodb.core.MongoTemplate.potentiallyConvertRuntimeException(MongoTemplate.java:2899)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:555)
	at org.springframework.data.mongodb.core.MongoTemplate.insertDocument(MongoTemplate.java:1458)
	at org.springframework.data.mongodb.core.MongoTemplate.doInsert(MongoTemplate.java:1257)
	at org.springframework.data.mongodb.core.MongoTemplate.insert(MongoTemplate.java:1172)
	at org.springframework.data.mongodb.repository.support.SimpleMongoRepository.save(SimpleMongoRepository.java:91)
	at jdk.internal.reflect.GeneratedMethodAccessor267.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:289)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:137)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:121)
	at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:529)
	at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:285)
	at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:599)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:163)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:138)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:80)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at jdk.internal.reflect.GeneratedMethodAccessor266.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:344)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:198)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:137)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at com.estockmarket.query.domain.service.StockService.updateStock(StockService.java:105)
	at com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(KafkaStocksEventListener.java:36)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:56)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:347)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:92)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:53)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2322)
	... 11 common frames omitted
Caused by: com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at com.mongodb.client.internal.MongoCollectionImpl.executeSingleWriteRequest(MongoCollectionImpl.java:1017)
	at com.mongodb.client.internal.MongoCollectionImpl.executeInsertOne(MongoCollectionImpl.java:470)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:453)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:447)
	at org.springframework.data.mongodb.core.MongoTemplate.lambda$insertDocument$16(MongoTemplate.java:1464)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:553)
	... 57 common frames omitted
2022-06-24 14:00:28,690 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks com.estockmarket.query.domain.model.Stocks@2973cdfe
2022-06-24 14:00:28,718 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Seeking to offset 2 for partition updateStock-0
2022-06-24 14:00:28,719 ERROR org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception; nested exception is org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:206)
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:112)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2359)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2228)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2142)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2024)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1706)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2371)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2342)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2303)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2217)
	... 9 common frames omitted
Caused by: org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.data.mongodb.core.MongoExceptionTranslator.translateExceptionIfPossible(MongoExceptionTranslator.java:106)
	at org.springframework.data.mongodb.core.MongoTemplate.potentiallyConvertRuntimeException(MongoTemplate.java:2899)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:555)
	at org.springframework.data.mongodb.core.MongoTemplate.insertDocument(MongoTemplate.java:1458)
	at org.springframework.data.mongodb.core.MongoTemplate.doInsert(MongoTemplate.java:1257)
	at org.springframework.data.mongodb.core.MongoTemplate.insert(MongoTemplate.java:1172)
	at org.springframework.data.mongodb.repository.support.SimpleMongoRepository.save(SimpleMongoRepository.java:91)
	at jdk.internal.reflect.GeneratedMethodAccessor267.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:289)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:137)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:121)
	at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:529)
	at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:285)
	at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:599)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:163)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:138)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:80)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at jdk.internal.reflect.GeneratedMethodAccessor266.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:344)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:198)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:137)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at com.estockmarket.query.domain.service.StockService.updateStock(StockService.java:105)
	at com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(KafkaStocksEventListener.java:36)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:56)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:347)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:92)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:53)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2322)
	... 11 common frames omitted
Caused by: com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at com.mongodb.client.internal.MongoCollectionImpl.executeSingleWriteRequest(MongoCollectionImpl.java:1017)
	at com.mongodb.client.internal.MongoCollectionImpl.executeInsertOne(MongoCollectionImpl.java:470)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:453)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:447)
	at org.springframework.data.mongodb.core.MongoTemplate.lambda$insertDocument$16(MongoTemplate.java:1464)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:553)
	... 57 common frames omitted
2022-06-24 14:00:29,233 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks com.estockmarket.query.domain.model.Stocks@6cf04793
2022-06-24 14:00:29,262 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Seeking to offset 2 for partition updateStock-0
2022-06-24 14:00:29,263 ERROR org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception; nested exception is org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:206)
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:112)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2359)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2228)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2142)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2024)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1706)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2371)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2342)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2303)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2217)
	... 9 common frames omitted
Caused by: org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.data.mongodb.core.MongoExceptionTranslator.translateExceptionIfPossible(MongoExceptionTranslator.java:106)
	at org.springframework.data.mongodb.core.MongoTemplate.potentiallyConvertRuntimeException(MongoTemplate.java:2899)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:555)
	at org.springframework.data.mongodb.core.MongoTemplate.insertDocument(MongoTemplate.java:1458)
	at org.springframework.data.mongodb.core.MongoTemplate.doInsert(MongoTemplate.java:1257)
	at org.springframework.data.mongodb.core.MongoTemplate.insert(MongoTemplate.java:1172)
	at org.springframework.data.mongodb.repository.support.SimpleMongoRepository.save(SimpleMongoRepository.java:91)
	at jdk.internal.reflect.GeneratedMethodAccessor267.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:289)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:137)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:121)
	at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:529)
	at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:285)
	at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:599)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:163)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:138)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:80)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at jdk.internal.reflect.GeneratedMethodAccessor266.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:344)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:198)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:137)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at com.estockmarket.query.domain.service.StockService.updateStock(StockService.java:105)
	at com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(KafkaStocksEventListener.java:36)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:56)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:347)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:92)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:53)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2322)
	... 11 common frames omitted
Caused by: com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at com.mongodb.client.internal.MongoCollectionImpl.executeSingleWriteRequest(MongoCollectionImpl.java:1017)
	at com.mongodb.client.internal.MongoCollectionImpl.executeInsertOne(MongoCollectionImpl.java:470)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:453)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:447)
	at org.springframework.data.mongodb.core.MongoTemplate.lambda$insertDocument$16(MongoTemplate.java:1464)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:553)
	... 57 common frames omitted
2022-06-24 14:00:29,777 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks com.estockmarket.query.domain.model.Stocks@3738a80a
2022-06-24 14:00:29,805 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Seeking to offset 2 for partition updateStock-0
2022-06-24 14:00:29,805 ERROR org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception; nested exception is org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:206)
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:112)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2359)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2228)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2142)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2024)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1706)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2371)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2342)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2303)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2217)
	... 9 common frames omitted
Caused by: org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.data.mongodb.core.MongoExceptionTranslator.translateExceptionIfPossible(MongoExceptionTranslator.java:106)
	at org.springframework.data.mongodb.core.MongoTemplate.potentiallyConvertRuntimeException(MongoTemplate.java:2899)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:555)
	at org.springframework.data.mongodb.core.MongoTemplate.insertDocument(MongoTemplate.java:1458)
	at org.springframework.data.mongodb.core.MongoTemplate.doInsert(MongoTemplate.java:1257)
	at org.springframework.data.mongodb.core.MongoTemplate.insert(MongoTemplate.java:1172)
	at org.springframework.data.mongodb.repository.support.SimpleMongoRepository.save(SimpleMongoRepository.java:91)
	at jdk.internal.reflect.GeneratedMethodAccessor267.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:289)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:137)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:121)
	at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:529)
	at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:285)
	at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:599)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:163)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:138)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:80)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at jdk.internal.reflect.GeneratedMethodAccessor266.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:344)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:198)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:137)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at com.estockmarket.query.domain.service.StockService.updateStock(StockService.java:105)
	at com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(KafkaStocksEventListener.java:36)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:56)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:347)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:92)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:53)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2322)
	... 11 common frames omitted
Caused by: com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at com.mongodb.client.internal.MongoCollectionImpl.executeSingleWriteRequest(MongoCollectionImpl.java:1017)
	at com.mongodb.client.internal.MongoCollectionImpl.executeInsertOne(MongoCollectionImpl.java:470)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:453)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:447)
	at org.springframework.data.mongodb.core.MongoTemplate.lambda$insertDocument$16(MongoTemplate.java:1464)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:553)
	... 57 common frames omitted
2022-06-24 14:00:30,322 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks com.estockmarket.query.domain.model.Stocks@7ac61ffd
2022-06-24 14:00:30,351 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Seeking to offset 2 for partition updateStock-0
2022-06-24 14:00:30,351 ERROR org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception; nested exception is org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:206)
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:112)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2359)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2228)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2142)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2024)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1706)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2371)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2342)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2303)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2217)
	... 9 common frames omitted
Caused by: org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.data.mongodb.core.MongoExceptionTranslator.translateExceptionIfPossible(MongoExceptionTranslator.java:106)
	at org.springframework.data.mongodb.core.MongoTemplate.potentiallyConvertRuntimeException(MongoTemplate.java:2899)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:555)
	at org.springframework.data.mongodb.core.MongoTemplate.insertDocument(MongoTemplate.java:1458)
	at org.springframework.data.mongodb.core.MongoTemplate.doInsert(MongoTemplate.java:1257)
	at org.springframework.data.mongodb.core.MongoTemplate.insert(MongoTemplate.java:1172)
	at org.springframework.data.mongodb.repository.support.SimpleMongoRepository.save(SimpleMongoRepository.java:91)
	at jdk.internal.reflect.GeneratedMethodAccessor267.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:289)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:137)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:121)
	at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:529)
	at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:285)
	at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:599)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:163)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:138)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:80)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at jdk.internal.reflect.GeneratedMethodAccessor266.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:344)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:198)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:137)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at com.estockmarket.query.domain.service.StockService.updateStock(StockService.java:105)
	at com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(KafkaStocksEventListener.java:36)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:56)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:347)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:92)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:53)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2322)
	... 11 common frames omitted
Caused by: com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at com.mongodb.client.internal.MongoCollectionImpl.executeSingleWriteRequest(MongoCollectionImpl.java:1017)
	at com.mongodb.client.internal.MongoCollectionImpl.executeInsertOne(MongoCollectionImpl.java:470)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:453)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:447)
	at org.springframework.data.mongodb.core.MongoTemplate.lambda$insertDocument$16(MongoTemplate.java:1464)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:553)
	... 57 common frames omitted
2022-06-24 14:00:30,867 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks com.estockmarket.query.domain.model.Stocks@4ded3166
2022-06-24 14:00:30,895 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Seeking to offset 2 for partition updateStock-0
2022-06-24 14:00:30,895 ERROR org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception; nested exception is org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:206)
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:112)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2359)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2228)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2142)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2024)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1706)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2371)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2342)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2303)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2217)
	... 9 common frames omitted
Caused by: org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.data.mongodb.core.MongoExceptionTranslator.translateExceptionIfPossible(MongoExceptionTranslator.java:106)
	at org.springframework.data.mongodb.core.MongoTemplate.potentiallyConvertRuntimeException(MongoTemplate.java:2899)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:555)
	at org.springframework.data.mongodb.core.MongoTemplate.insertDocument(MongoTemplate.java:1458)
	at org.springframework.data.mongodb.core.MongoTemplate.doInsert(MongoTemplate.java:1257)
	at org.springframework.data.mongodb.core.MongoTemplate.insert(MongoTemplate.java:1172)
	at org.springframework.data.mongodb.repository.support.SimpleMongoRepository.save(SimpleMongoRepository.java:91)
	at jdk.internal.reflect.GeneratedMethodAccessor267.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:289)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:137)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:121)
	at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:529)
	at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:285)
	at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:599)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:163)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:138)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:80)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at jdk.internal.reflect.GeneratedMethodAccessor266.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:344)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:198)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:137)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at com.estockmarket.query.domain.service.StockService.updateStock(StockService.java:105)
	at com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(KafkaStocksEventListener.java:36)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:56)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:347)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:92)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:53)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2322)
	... 11 common frames omitted
Caused by: com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at com.mongodb.client.internal.MongoCollectionImpl.executeSingleWriteRequest(MongoCollectionImpl.java:1017)
	at com.mongodb.client.internal.MongoCollectionImpl.executeInsertOne(MongoCollectionImpl.java:470)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:453)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:447)
	at org.springframework.data.mongodb.core.MongoTemplate.lambda$insertDocument$16(MongoTemplate.java:1464)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:553)
	... 57 common frames omitted
2022-06-24 14:00:31,307 INFO com.netflix.discovery.shared.resolver.aws.ConfigClusterResolver [AsyncResolver-bootstrap-executor-0] Resolving eureka endpoints via configuration
2022-06-24 14:00:31,415 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks com.estockmarket.query.domain.model.Stocks@23cf2b97
2022-06-24 14:00:31,419 ERROR org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Backoff FixedBackOff{interval=0, currentAttempts=10, maxAttempts=9} exhausted for updateStock-0@2
org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2371)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2342)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2303)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2217)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2142)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2024)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1706)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.data.mongodb.core.MongoExceptionTranslator.translateExceptionIfPossible(MongoExceptionTranslator.java:106)
	at org.springframework.data.mongodb.core.MongoTemplate.potentiallyConvertRuntimeException(MongoTemplate.java:2899)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:555)
	at org.springframework.data.mongodb.core.MongoTemplate.insertDocument(MongoTemplate.java:1458)
	at org.springframework.data.mongodb.core.MongoTemplate.doInsert(MongoTemplate.java:1257)
	at org.springframework.data.mongodb.core.MongoTemplate.insert(MongoTemplate.java:1172)
	at org.springframework.data.mongodb.repository.support.SimpleMongoRepository.save(SimpleMongoRepository.java:91)
	at jdk.internal.reflect.GeneratedMethodAccessor267.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:289)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:137)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:121)
	at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:529)
	at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:285)
	at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:599)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:163)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:138)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:80)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at jdk.internal.reflect.GeneratedMethodAccessor266.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:344)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:198)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:137)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at com.estockmarket.query.domain.service.StockService.updateStock(StockService.java:105)
	at com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(KafkaStocksEventListener.java:36)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:56)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:347)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:92)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:53)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2322)
	... 11 common frames omitted
Caused by: com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at com.mongodb.client.internal.MongoCollectionImpl.executeSingleWriteRequest(MongoCollectionImpl.java:1017)
	at com.mongodb.client.internal.MongoCollectionImpl.executeInsertOne(MongoCollectionImpl.java:470)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:453)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:447)
	at org.springframework.data.mongodb.core.MongoTemplate.lambda$insertDocument$16(MongoTemplate.java:1464)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:553)
	... 57 common frames omitted
2022-06-24 14:00:41,086 INFO com.estockmarket.query.application.controller.UserController [http-nio-6093-exec-9] validate otp email,otppriya26397@gmail.com
2022-06-24 14:00:41,087 INFO com.estockmarket.query.domain.service.OTPService [http-nio-6093-exec-9] get otp priya26397@gmail.com
2022-06-24 14:00:41,087 INFO com.estockmarket.query.domain.service.OTPService [http-nio-6093-exec-9] clear otp priya26397@gmail.com
2022-06-24 14:00:42,082 INFO com.estockmarket.query.application.controller.UserController [http-nio-6093-exec-4] authenticate user com.estockmarket.query.domain.util.JwtRequest@2a58747b
2022-06-24 14:00:42,083 INFO com.estockmarket.query.domain.service.UserService [http-nio-6093-exec-4] user authenticatecom.estockmarket.query.domain.util.JwtRequest@2a58747b
2022-06-24 14:00:42,124 INFO com.estockmarket.query.application.controller.CompanyController [http-nio-6093-exec-1] get all company details 
2022-06-24 14:00:42,124 INFO com.estockmarket.query.domain.service.CompanyService [http-nio-6093-exec-1] fetch all company {}
2022-06-24 14:00:42,127 INFO com.estockmarket.query.domain.service.CompanyService [http-nio-6093-exec-1] convert to company dto com.estockmarket.query.domain.model.Company@79e7d315
2022-06-24 14:00:44,164 INFO com.estockmarket.query.application.controller.CompanyController [http-nio-6093-exec-8] get all company details 
2022-06-24 14:00:44,164 INFO com.estockmarket.query.domain.service.CompanyService [http-nio-6093-exec-8] fetch all company {}
2022-06-24 14:00:44,166 INFO com.estockmarket.query.domain.service.CompanyService [http-nio-6093-exec-8] convert to company dto com.estockmarket.query.domain.model.Company@2bdb3c37
2022-06-24 14:00:48,960 INFO com.estockmarket.query.application.controller.StockController [http-nio-6093-exec-10] fetch min,max and average stock price based on company code,date C-008
2022-06-24 14:00:48,961 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-10] fetch stockaggregate based on code,date C-008
2022-06-24 14:00:48,961 INFO com.estockmarket.query.application.controller.StockController [http-nio-6093-exec-2] fetch stocks based on code,date C-008
2022-06-24 14:00:48,961 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-2] fetch company stocks based on code,date C-008
2022-06-24 14:00:48,961 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-10] fetch stock query based on code C-008
2022-06-24 14:00:48,962 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-2] fetch stock query based on code C-008
2022-06-24 14:00:54,337 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks com.estockmarket.query.domain.model.Stocks@19fce631
2022-06-24 14:00:54,366 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Seeking to offset 3 for partition updateStock-0
2022-06-24 14:00:54,390 ERROR org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception; nested exception is org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:206)
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:112)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2359)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2228)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2142)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2024)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1706)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2371)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2342)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2303)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2217)
	... 9 common frames omitted
Caused by: org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.data.mongodb.core.MongoExceptionTranslator.translateExceptionIfPossible(MongoExceptionTranslator.java:106)
	at org.springframework.data.mongodb.core.MongoTemplate.potentiallyConvertRuntimeException(MongoTemplate.java:2899)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:555)
	at org.springframework.data.mongodb.core.MongoTemplate.insertDocument(MongoTemplate.java:1458)
	at org.springframework.data.mongodb.core.MongoTemplate.doInsert(MongoTemplate.java:1257)
	at org.springframework.data.mongodb.core.MongoTemplate.insert(MongoTemplate.java:1172)
	at org.springframework.data.mongodb.repository.support.SimpleMongoRepository.save(SimpleMongoRepository.java:91)
	at jdk.internal.reflect.GeneratedMethodAccessor267.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:289)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:137)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:121)
	at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:529)
	at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:285)
	at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:599)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:163)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:138)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:80)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at jdk.internal.reflect.GeneratedMethodAccessor266.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:344)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:198)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:137)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at com.estockmarket.query.domain.service.StockService.updateStock(StockService.java:105)
	at com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(KafkaStocksEventListener.java:36)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:56)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:347)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:92)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:53)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2322)
	... 11 common frames omitted
Caused by: com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at com.mongodb.client.internal.MongoCollectionImpl.executeSingleWriteRequest(MongoCollectionImpl.java:1017)
	at com.mongodb.client.internal.MongoCollectionImpl.executeInsertOne(MongoCollectionImpl.java:470)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:453)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:447)
	at org.springframework.data.mongodb.core.MongoTemplate.lambda$insertDocument$16(MongoTemplate.java:1464)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:553)
	... 57 common frames omitted
2022-06-24 14:00:54,390 INFO com.estockmarket.query.application.controller.StockController [http-nio-6093-exec-6] fetch stocks based on code,date C-008
2022-06-24 14:00:54,390 INFO com.estockmarket.query.application.controller.StockController [http-nio-6093-exec-7] fetch min,max and average stock price based on company code,date C-008
2022-06-24 14:00:54,391 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-7] fetch stockaggregate based on code,date C-008
2022-06-24 14:00:54,391 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-6] fetch company stocks based on code,date C-008
2022-06-24 14:00:54,391 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-7] fetch stock query based on code C-008
2022-06-24 14:00:54,391 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-6] fetch stock query based on code C-008
2022-06-24 14:00:54,901 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks com.estockmarket.query.domain.model.Stocks@3037709f
2022-06-24 14:00:54,930 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Seeking to offset 3 for partition updateStock-0
2022-06-24 14:00:54,930 ERROR org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception; nested exception is org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:206)
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:112)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2359)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2228)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2142)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2024)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1706)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2371)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2342)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2303)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2217)
	... 9 common frames omitted
Caused by: org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.data.mongodb.core.MongoExceptionTranslator.translateExceptionIfPossible(MongoExceptionTranslator.java:106)
	at org.springframework.data.mongodb.core.MongoTemplate.potentiallyConvertRuntimeException(MongoTemplate.java:2899)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:555)
	at org.springframework.data.mongodb.core.MongoTemplate.insertDocument(MongoTemplate.java:1458)
	at org.springframework.data.mongodb.core.MongoTemplate.doInsert(MongoTemplate.java:1257)
	at org.springframework.data.mongodb.core.MongoTemplate.insert(MongoTemplate.java:1172)
	at org.springframework.data.mongodb.repository.support.SimpleMongoRepository.save(SimpleMongoRepository.java:91)
	at jdk.internal.reflect.GeneratedMethodAccessor267.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:289)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:137)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:121)
	at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:529)
	at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:285)
	at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:599)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:163)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:138)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:80)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at jdk.internal.reflect.GeneratedMethodAccessor266.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:344)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:198)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:137)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at com.estockmarket.query.domain.service.StockService.updateStock(StockService.java:105)
	at com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(KafkaStocksEventListener.java:36)
	at jdk.internal.reflect.GeneratedMethodAccessor301.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:56)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:347)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:92)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:53)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2322)
	... 11 common frames omitted
Caused by: com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at com.mongodb.client.internal.MongoCollectionImpl.executeSingleWriteRequest(MongoCollectionImpl.java:1017)
	at com.mongodb.client.internal.MongoCollectionImpl.executeInsertOne(MongoCollectionImpl.java:470)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:453)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:447)
	at org.springframework.data.mongodb.core.MongoTemplate.lambda$insertDocument$16(MongoTemplate.java:1464)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:553)
	... 56 common frames omitted
2022-06-24 14:00:55,436 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks com.estockmarket.query.domain.model.Stocks@51956890
2022-06-24 14:00:55,449 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Seeking to offset 3 for partition updateStock-0
2022-06-24 14:00:55,449 ERROR org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception; nested exception is org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:206)
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:112)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2359)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2228)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2142)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2024)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1706)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2371)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2342)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2303)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2217)
	... 9 common frames omitted
Caused by: org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.data.mongodb.core.MongoExceptionTranslator.translateExceptionIfPossible(MongoExceptionTranslator.java:106)
	at org.springframework.data.mongodb.core.MongoTemplate.potentiallyConvertRuntimeException(MongoTemplate.java:2899)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:555)
	at org.springframework.data.mongodb.core.MongoTemplate.insertDocument(MongoTemplate.java:1458)
	at org.springframework.data.mongodb.core.MongoTemplate.doInsert(MongoTemplate.java:1257)
	at org.springframework.data.mongodb.core.MongoTemplate.insert(MongoTemplate.java:1172)
	at org.springframework.data.mongodb.repository.support.SimpleMongoRepository.save(SimpleMongoRepository.java:91)
	at jdk.internal.reflect.GeneratedMethodAccessor267.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:289)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:137)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:121)
	at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:529)
	at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:285)
	at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:599)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:163)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:138)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:80)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at jdk.internal.reflect.GeneratedMethodAccessor266.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:344)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:198)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:137)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at com.estockmarket.query.domain.service.StockService.updateStock(StockService.java:105)
	at com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(KafkaStocksEventListener.java:36)
	at jdk.internal.reflect.GeneratedMethodAccessor301.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:56)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:347)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:92)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:53)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2322)
	... 11 common frames omitted
Caused by: com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at com.mongodb.client.internal.MongoCollectionImpl.executeSingleWriteRequest(MongoCollectionImpl.java:1017)
	at com.mongodb.client.internal.MongoCollectionImpl.executeInsertOne(MongoCollectionImpl.java:470)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:453)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:447)
	at org.springframework.data.mongodb.core.MongoTemplate.lambda$insertDocument$16(MongoTemplate.java:1464)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:553)
	... 56 common frames omitted
2022-06-24 14:00:55,964 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks com.estockmarket.query.domain.model.Stocks@f8bb1c5
2022-06-24 14:00:55,991 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Seeking to offset 3 for partition updateStock-0
2022-06-24 14:00:55,991 ERROR org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception; nested exception is org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:206)
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:112)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2359)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2228)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2142)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2024)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1706)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2371)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2342)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2303)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2217)
	... 9 common frames omitted
Caused by: org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.data.mongodb.core.MongoExceptionTranslator.translateExceptionIfPossible(MongoExceptionTranslator.java:106)
	at org.springframework.data.mongodb.core.MongoTemplate.potentiallyConvertRuntimeException(MongoTemplate.java:2899)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:555)
	at org.springframework.data.mongodb.core.MongoTemplate.insertDocument(MongoTemplate.java:1458)
	at org.springframework.data.mongodb.core.MongoTemplate.doInsert(MongoTemplate.java:1257)
	at org.springframework.data.mongodb.core.MongoTemplate.insert(MongoTemplate.java:1172)
	at org.springframework.data.mongodb.repository.support.SimpleMongoRepository.save(SimpleMongoRepository.java:91)
	at jdk.internal.reflect.GeneratedMethodAccessor267.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:289)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:137)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:121)
	at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:529)
	at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:285)
	at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:599)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:163)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:138)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:80)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at jdk.internal.reflect.GeneratedMethodAccessor266.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:344)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:198)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:137)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at com.estockmarket.query.domain.service.StockService.updateStock(StockService.java:105)
	at com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(KafkaStocksEventListener.java:36)
	at jdk.internal.reflect.GeneratedMethodAccessor301.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:56)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:347)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:92)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:53)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2322)
	... 11 common frames omitted
Caused by: com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at com.mongodb.client.internal.MongoCollectionImpl.executeSingleWriteRequest(MongoCollectionImpl.java:1017)
	at com.mongodb.client.internal.MongoCollectionImpl.executeInsertOne(MongoCollectionImpl.java:470)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:453)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:447)
	at org.springframework.data.mongodb.core.MongoTemplate.lambda$insertDocument$16(MongoTemplate.java:1464)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:553)
	... 56 common frames omitted
2022-06-24 14:00:56,501 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks com.estockmarket.query.domain.model.Stocks@327c7e70
2022-06-24 14:00:56,530 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Seeking to offset 3 for partition updateStock-0
2022-06-24 14:00:56,530 ERROR org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception; nested exception is org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:206)
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:112)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2359)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2228)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2142)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2024)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1706)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2371)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2342)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2303)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2217)
	... 9 common frames omitted
Caused by: org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.data.mongodb.core.MongoExceptionTranslator.translateExceptionIfPossible(MongoExceptionTranslator.java:106)
	at org.springframework.data.mongodb.core.MongoTemplate.potentiallyConvertRuntimeException(MongoTemplate.java:2899)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:555)
	at org.springframework.data.mongodb.core.MongoTemplate.insertDocument(MongoTemplate.java:1458)
	at org.springframework.data.mongodb.core.MongoTemplate.doInsert(MongoTemplate.java:1257)
	at org.springframework.data.mongodb.core.MongoTemplate.insert(MongoTemplate.java:1172)
	at org.springframework.data.mongodb.repository.support.SimpleMongoRepository.save(SimpleMongoRepository.java:91)
	at jdk.internal.reflect.GeneratedMethodAccessor267.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:289)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:137)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:121)
	at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:529)
	at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:285)
	at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:599)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:163)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:138)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:80)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at jdk.internal.reflect.GeneratedMethodAccessor266.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:344)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:198)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:137)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at com.estockmarket.query.domain.service.StockService.updateStock(StockService.java:105)
	at com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(KafkaStocksEventListener.java:36)
	at jdk.internal.reflect.GeneratedMethodAccessor301.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:56)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:347)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:92)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:53)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2322)
	... 11 common frames omitted
Caused by: com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at com.mongodb.client.internal.MongoCollectionImpl.executeSingleWriteRequest(MongoCollectionImpl.java:1017)
	at com.mongodb.client.internal.MongoCollectionImpl.executeInsertOne(MongoCollectionImpl.java:470)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:453)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:447)
	at org.springframework.data.mongodb.core.MongoTemplate.lambda$insertDocument$16(MongoTemplate.java:1464)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:553)
	... 56 common frames omitted
2022-06-24 14:00:57,047 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks com.estockmarket.query.domain.model.Stocks@39444328
2022-06-24 14:00:57,076 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Seeking to offset 3 for partition updateStock-0
2022-06-24 14:00:57,076 ERROR org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception; nested exception is org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:206)
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:112)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2359)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2228)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2142)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2024)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1706)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2371)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2342)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2303)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2217)
	... 9 common frames omitted
Caused by: org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.data.mongodb.core.MongoExceptionTranslator.translateExceptionIfPossible(MongoExceptionTranslator.java:106)
	at org.springframework.data.mongodb.core.MongoTemplate.potentiallyConvertRuntimeException(MongoTemplate.java:2899)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:555)
	at org.springframework.data.mongodb.core.MongoTemplate.insertDocument(MongoTemplate.java:1458)
	at org.springframework.data.mongodb.core.MongoTemplate.doInsert(MongoTemplate.java:1257)
	at org.springframework.data.mongodb.core.MongoTemplate.insert(MongoTemplate.java:1172)
	at org.springframework.data.mongodb.repository.support.SimpleMongoRepository.save(SimpleMongoRepository.java:91)
	at jdk.internal.reflect.GeneratedMethodAccessor267.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:289)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:137)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:121)
	at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:529)
	at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:285)
	at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:599)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:163)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:138)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:80)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at jdk.internal.reflect.GeneratedMethodAccessor266.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:344)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:198)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:137)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at com.estockmarket.query.domain.service.StockService.updateStock(StockService.java:105)
	at com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(KafkaStocksEventListener.java:36)
	at jdk.internal.reflect.GeneratedMethodAccessor301.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:56)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:347)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:92)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:53)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2322)
	... 11 common frames omitted
Caused by: com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at com.mongodb.client.internal.MongoCollectionImpl.executeSingleWriteRequest(MongoCollectionImpl.java:1017)
	at com.mongodb.client.internal.MongoCollectionImpl.executeInsertOne(MongoCollectionImpl.java:470)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:453)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:447)
	at org.springframework.data.mongodb.core.MongoTemplate.lambda$insertDocument$16(MongoTemplate.java:1464)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:553)
	... 56 common frames omitted
2022-06-24 14:00:57,589 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks com.estockmarket.query.domain.model.Stocks@43ef6d78
2022-06-24 14:00:57,616 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Seeking to offset 3 for partition updateStock-0
2022-06-24 14:00:57,616 ERROR org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception; nested exception is org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:206)
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:112)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2359)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2228)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2142)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2024)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1706)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2371)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2342)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2303)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2217)
	... 9 common frames omitted
Caused by: org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.data.mongodb.core.MongoExceptionTranslator.translateExceptionIfPossible(MongoExceptionTranslator.java:106)
	at org.springframework.data.mongodb.core.MongoTemplate.potentiallyConvertRuntimeException(MongoTemplate.java:2899)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:555)
	at org.springframework.data.mongodb.core.MongoTemplate.insertDocument(MongoTemplate.java:1458)
	at org.springframework.data.mongodb.core.MongoTemplate.doInsert(MongoTemplate.java:1257)
	at org.springframework.data.mongodb.core.MongoTemplate.insert(MongoTemplate.java:1172)
	at org.springframework.data.mongodb.repository.support.SimpleMongoRepository.save(SimpleMongoRepository.java:91)
	at jdk.internal.reflect.GeneratedMethodAccessor267.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:289)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:137)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:121)
	at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:529)
	at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:285)
	at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:599)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:163)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:138)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:80)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at jdk.internal.reflect.GeneratedMethodAccessor266.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:344)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:198)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:137)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at com.estockmarket.query.domain.service.StockService.updateStock(StockService.java:105)
	at com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(KafkaStocksEventListener.java:36)
	at jdk.internal.reflect.GeneratedMethodAccessor301.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:56)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:347)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:92)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:53)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2322)
	... 11 common frames omitted
Caused by: com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at com.mongodb.client.internal.MongoCollectionImpl.executeSingleWriteRequest(MongoCollectionImpl.java:1017)
	at com.mongodb.client.internal.MongoCollectionImpl.executeInsertOne(MongoCollectionImpl.java:470)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:453)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:447)
	at org.springframework.data.mongodb.core.MongoTemplate.lambda$insertDocument$16(MongoTemplate.java:1464)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:553)
	... 56 common frames omitted
2022-06-24 14:00:58,129 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks com.estockmarket.query.domain.model.Stocks@1138e959
2022-06-24 14:00:58,143 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Seeking to offset 3 for partition updateStock-0
2022-06-24 14:00:58,143 ERROR org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception; nested exception is org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:206)
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:112)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2359)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2228)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2142)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2024)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1706)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2371)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2342)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2303)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2217)
	... 9 common frames omitted
Caused by: org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.data.mongodb.core.MongoExceptionTranslator.translateExceptionIfPossible(MongoExceptionTranslator.java:106)
	at org.springframework.data.mongodb.core.MongoTemplate.potentiallyConvertRuntimeException(MongoTemplate.java:2899)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:555)
	at org.springframework.data.mongodb.core.MongoTemplate.insertDocument(MongoTemplate.java:1458)
	at org.springframework.data.mongodb.core.MongoTemplate.doInsert(MongoTemplate.java:1257)
	at org.springframework.data.mongodb.core.MongoTemplate.insert(MongoTemplate.java:1172)
	at org.springframework.data.mongodb.repository.support.SimpleMongoRepository.save(SimpleMongoRepository.java:91)
	at jdk.internal.reflect.GeneratedMethodAccessor267.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:289)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:137)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:121)
	at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:529)
	at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:285)
	at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:599)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:163)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:138)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:80)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at jdk.internal.reflect.GeneratedMethodAccessor266.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:344)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:198)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:137)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at com.estockmarket.query.domain.service.StockService.updateStock(StockService.java:105)
	at com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(KafkaStocksEventListener.java:36)
	at jdk.internal.reflect.GeneratedMethodAccessor301.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:56)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:347)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:92)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:53)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2322)
	... 11 common frames omitted
Caused by: com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at com.mongodb.client.internal.MongoCollectionImpl.executeSingleWriteRequest(MongoCollectionImpl.java:1017)
	at com.mongodb.client.internal.MongoCollectionImpl.executeInsertOne(MongoCollectionImpl.java:470)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:453)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:447)
	at org.springframework.data.mongodb.core.MongoTemplate.lambda$insertDocument$16(MongoTemplate.java:1464)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:553)
	... 56 common frames omitted
2022-06-24 14:00:58,662 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks com.estockmarket.query.domain.model.Stocks@40669af0
2022-06-24 14:00:58,689 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Seeking to offset 3 for partition updateStock-0
2022-06-24 14:00:58,689 ERROR org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception; nested exception is org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:206)
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:112)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2359)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2228)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2142)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2024)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1706)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2371)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2342)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2303)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2217)
	... 9 common frames omitted
Caused by: org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.data.mongodb.core.MongoExceptionTranslator.translateExceptionIfPossible(MongoExceptionTranslator.java:106)
	at org.springframework.data.mongodb.core.MongoTemplate.potentiallyConvertRuntimeException(MongoTemplate.java:2899)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:555)
	at org.springframework.data.mongodb.core.MongoTemplate.insertDocument(MongoTemplate.java:1458)
	at org.springframework.data.mongodb.core.MongoTemplate.doInsert(MongoTemplate.java:1257)
	at org.springframework.data.mongodb.core.MongoTemplate.insert(MongoTemplate.java:1172)
	at org.springframework.data.mongodb.repository.support.SimpleMongoRepository.save(SimpleMongoRepository.java:91)
	at jdk.internal.reflect.GeneratedMethodAccessor267.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:289)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:137)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:121)
	at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:529)
	at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:285)
	at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:599)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:163)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:138)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:80)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at jdk.internal.reflect.GeneratedMethodAccessor266.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:344)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:198)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:137)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at com.estockmarket.query.domain.service.StockService.updateStock(StockService.java:105)
	at com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(KafkaStocksEventListener.java:36)
	at jdk.internal.reflect.GeneratedMethodAccessor301.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:56)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:347)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:92)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:53)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2322)
	... 11 common frames omitted
Caused by: com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at com.mongodb.client.internal.MongoCollectionImpl.executeSingleWriteRequest(MongoCollectionImpl.java:1017)
	at com.mongodb.client.internal.MongoCollectionImpl.executeInsertOne(MongoCollectionImpl.java:470)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:453)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:447)
	at org.springframework.data.mongodb.core.MongoTemplate.lambda$insertDocument$16(MongoTemplate.java:1464)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:553)
	... 56 common frames omitted
2022-06-24 14:00:59,203 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks com.estockmarket.query.domain.model.Stocks@5ba0dcd7
2022-06-24 14:00:59,208 ERROR org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Backoff FixedBackOff{interval=0, currentAttempts=10, maxAttempts=9} exhausted for updateStock-0@3
org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2371)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2342)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2303)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2217)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2142)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2024)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1706)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.data.mongodb.core.MongoExceptionTranslator.translateExceptionIfPossible(MongoExceptionTranslator.java:106)
	at org.springframework.data.mongodb.core.MongoTemplate.potentiallyConvertRuntimeException(MongoTemplate.java:2899)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:555)
	at org.springframework.data.mongodb.core.MongoTemplate.insertDocument(MongoTemplate.java:1458)
	at org.springframework.data.mongodb.core.MongoTemplate.doInsert(MongoTemplate.java:1257)
	at org.springframework.data.mongodb.core.MongoTemplate.insert(MongoTemplate.java:1172)
	at org.springframework.data.mongodb.repository.support.SimpleMongoRepository.save(SimpleMongoRepository.java:91)
	at jdk.internal.reflect.GeneratedMethodAccessor267.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:289)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:137)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:121)
	at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:529)
	at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:285)
	at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:599)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:163)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:138)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:80)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at jdk.internal.reflect.GeneratedMethodAccessor266.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:344)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:198)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:137)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at com.estockmarket.query.domain.service.StockService.updateStock(StockService.java:105)
	at com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(KafkaStocksEventListener.java:36)
	at jdk.internal.reflect.GeneratedMethodAccessor301.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:56)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:347)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:92)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:53)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2322)
	... 11 common frames omitted
Caused by: com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at com.mongodb.client.internal.MongoCollectionImpl.executeSingleWriteRequest(MongoCollectionImpl.java:1017)
	at com.mongodb.client.internal.MongoCollectionImpl.executeInsertOne(MongoCollectionImpl.java:470)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:453)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:447)
	at org.springframework.data.mongodb.core.MongoTemplate.lambda$insertDocument$16(MongoTemplate.java:1464)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:553)
	... 56 common frames omitted
2022-06-24 14:00:59,210 ERROR org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Offset commit failed on partition updateStock-0 at offset 4: The coordinator is not aware of this member.
2022-06-24 14:00:59,210 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] OffsetCommit failed with Generation{generationId=76, memberId='consumer-group_id-30-da3b3d16-05bb-41f6-add4-53ae02e3c114', protocol='range'}: The coordinator is not aware of this member.
2022-06-24 14:00:59,210 ERROR org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Consumer exception
java.lang.IllegalStateException: This error handler cannot process 'org.apache.kafka.clients.consumer.CommitFailedException's; no record information is available
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:200)
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:112)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.handleConsumerException(KafkaMessageListenerContainer.java:1602)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1210)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing max.poll.interval.ms or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:1256)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:1163)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:1173)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:1148)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:206)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:169)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:129)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:602)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:412)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:297)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:236)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:215)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.commitOffsetsSync(ConsumerCoordinator.java:1005)
	at org.apache.kafka.clients.consumer.KafkaConsumer.commitSync(KafkaConsumer.java:1495)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doCommitSync(KafkaMessageListenerContainer.java:2656)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.commitSync(KafkaMessageListenerContainer.java:2651)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.commitIfNecessary(KafkaMessageListenerContainer.java:2637)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.processCommits(KafkaMessageListenerContainer.java:2451)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1235)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	... 3 common frames omitted
2022-06-24 14:00:59,211 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Giving away all assigned partitions as lost since generation has been reset,indicating that consumer is no longer part of the group
2022-06-24 14:00:59,211 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Lost previously assigned partitions updateStock-0
2022-06-24 14:00:59,211 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] group_id: partitions lost: [updateStock-0]
2022-06-24 14:00:59,211 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] group_id: partitions revoked: [updateStock-0]
2022-06-24 14:00:59,212 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] (Re-)joining group
2022-06-24 14:00:59,213 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] (Re-)joining group
2022-06-24 14:00:59,217 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Successfully joined group with generation Generation{generationId=78, memberId='consumer-group_id-30-6d0093ad-7a73-4141-b5da-b2fb576e5032', protocol='range'}
2022-06-24 14:00:59,217 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Finished assignment for group at generation 78: {consumer-group_id-30-6d0093ad-7a73-4141-b5da-b2fb576e5032=Assignment(partitions=[updateStock-0])}
2022-06-24 14:00:59,220 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Successfully synced group in generation Generation{generationId=78, memberId='consumer-group_id-30-6d0093ad-7a73-4141-b5da-b2fb576e5032', protocol='range'}
2022-06-24 14:00:59,220 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Notifying assignor about the new Assignment(partitions=[updateStock-0])
2022-06-24 14:00:59,220 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Adding newly assigned partitions: updateStock-0
2022-06-24 14:00:59,222 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Setting offset for partition updateStock-0 to the committed offset FetchPosition{offset=3, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:00:59,222 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] group_id: partitions assigned: [updateStock-0]
2022-06-24 14:00:59,713 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks com.estockmarket.query.domain.model.Stocks@1c6626ac
2022-06-24 14:00:59,757 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Seeking to offset 3 for partition updateStock-0
2022-06-24 14:00:59,757 ERROR org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception; nested exception is org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:206)
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:112)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2359)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2228)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2142)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2024)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1706)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2371)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2342)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2303)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2217)
	... 9 common frames omitted
Caused by: org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.data.mongodb.core.MongoExceptionTranslator.translateExceptionIfPossible(MongoExceptionTranslator.java:106)
	at org.springframework.data.mongodb.core.MongoTemplate.potentiallyConvertRuntimeException(MongoTemplate.java:2899)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:555)
	at org.springframework.data.mongodb.core.MongoTemplate.insertDocument(MongoTemplate.java:1458)
	at org.springframework.data.mongodb.core.MongoTemplate.doInsert(MongoTemplate.java:1257)
	at org.springframework.data.mongodb.core.MongoTemplate.insert(MongoTemplate.java:1172)
	at org.springframework.data.mongodb.repository.support.SimpleMongoRepository.save(SimpleMongoRepository.java:91)
	at jdk.internal.reflect.GeneratedMethodAccessor267.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:289)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:137)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:121)
	at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:529)
	at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:285)
	at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:599)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:163)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:138)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:80)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at jdk.internal.reflect.GeneratedMethodAccessor266.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:344)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:198)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:137)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at com.estockmarket.query.domain.service.StockService.updateStock(StockService.java:105)
	at com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(KafkaStocksEventListener.java:36)
	at jdk.internal.reflect.GeneratedMethodAccessor301.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:56)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:347)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:92)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:53)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2322)
	... 11 common frames omitted
Caused by: com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at com.mongodb.client.internal.MongoCollectionImpl.executeSingleWriteRequest(MongoCollectionImpl.java:1017)
	at com.mongodb.client.internal.MongoCollectionImpl.executeInsertOne(MongoCollectionImpl.java:470)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:453)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:447)
	at org.springframework.data.mongodb.core.MongoTemplate.lambda$insertDocument$16(MongoTemplate.java:1464)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:553)
	... 56 common frames omitted
2022-06-24 14:01:00,269 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks com.estockmarket.query.domain.model.Stocks@684eed3d
2022-06-24 14:01:00,298 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Seeking to offset 3 for partition updateStock-0
2022-06-24 14:01:00,298 ERROR org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception; nested exception is org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:206)
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:112)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2359)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2228)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2142)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2024)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1706)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2371)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2342)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2303)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2217)
	... 9 common frames omitted
Caused by: org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.data.mongodb.core.MongoExceptionTranslator.translateExceptionIfPossible(MongoExceptionTranslator.java:106)
	at org.springframework.data.mongodb.core.MongoTemplate.potentiallyConvertRuntimeException(MongoTemplate.java:2899)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:555)
	at org.springframework.data.mongodb.core.MongoTemplate.insertDocument(MongoTemplate.java:1458)
	at org.springframework.data.mongodb.core.MongoTemplate.doInsert(MongoTemplate.java:1257)
	at org.springframework.data.mongodb.core.MongoTemplate.insert(MongoTemplate.java:1172)
	at org.springframework.data.mongodb.repository.support.SimpleMongoRepository.save(SimpleMongoRepository.java:91)
	at jdk.internal.reflect.GeneratedMethodAccessor267.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:289)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:137)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:121)
	at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:529)
	at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:285)
	at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:599)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:163)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:138)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:80)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at jdk.internal.reflect.GeneratedMethodAccessor266.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:344)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:198)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:137)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at com.estockmarket.query.domain.service.StockService.updateStock(StockService.java:105)
	at com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(KafkaStocksEventListener.java:36)
	at jdk.internal.reflect.GeneratedMethodAccessor301.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:56)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:347)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:92)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:53)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2322)
	... 11 common frames omitted
Caused by: com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at com.mongodb.client.internal.MongoCollectionImpl.executeSingleWriteRequest(MongoCollectionImpl.java:1017)
	at com.mongodb.client.internal.MongoCollectionImpl.executeInsertOne(MongoCollectionImpl.java:470)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:453)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:447)
	at org.springframework.data.mongodb.core.MongoTemplate.lambda$insertDocument$16(MongoTemplate.java:1464)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:553)
	... 56 common frames omitted
2022-06-24 14:01:00,818 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks com.estockmarket.query.domain.model.Stocks@41fbe098
2022-06-24 14:01:00,846 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Seeking to offset 3 for partition updateStock-0
2022-06-24 14:01:00,846 ERROR org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception; nested exception is org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:206)
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:112)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2359)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2228)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2142)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2024)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1706)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2371)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2342)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2303)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2217)
	... 9 common frames omitted
Caused by: org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.data.mongodb.core.MongoExceptionTranslator.translateExceptionIfPossible(MongoExceptionTranslator.java:106)
	at org.springframework.data.mongodb.core.MongoTemplate.potentiallyConvertRuntimeException(MongoTemplate.java:2899)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:555)
	at org.springframework.data.mongodb.core.MongoTemplate.insertDocument(MongoTemplate.java:1458)
	at org.springframework.data.mongodb.core.MongoTemplate.doInsert(MongoTemplate.java:1257)
	at org.springframework.data.mongodb.core.MongoTemplate.insert(MongoTemplate.java:1172)
	at org.springframework.data.mongodb.repository.support.SimpleMongoRepository.save(SimpleMongoRepository.java:91)
	at jdk.internal.reflect.GeneratedMethodAccessor267.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:289)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:137)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:121)
	at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:529)
	at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:285)
	at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:599)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:163)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:138)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:80)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at jdk.internal.reflect.GeneratedMethodAccessor266.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:344)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:198)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:137)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at com.estockmarket.query.domain.service.StockService.updateStock(StockService.java:105)
	at com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(KafkaStocksEventListener.java:36)
	at jdk.internal.reflect.GeneratedMethodAccessor301.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:56)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:347)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:92)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:53)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2322)
	... 11 common frames omitted
Caused by: com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at com.mongodb.client.internal.MongoCollectionImpl.executeSingleWriteRequest(MongoCollectionImpl.java:1017)
	at com.mongodb.client.internal.MongoCollectionImpl.executeInsertOne(MongoCollectionImpl.java:470)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:453)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:447)
	at org.springframework.data.mongodb.core.MongoTemplate.lambda$insertDocument$16(MongoTemplate.java:1464)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:553)
	... 56 common frames omitted
2022-06-24 14:01:01,362 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks com.estockmarket.query.domain.model.Stocks@6c05fd4d
2022-06-24 14:01:01,390 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Seeking to offset 3 for partition updateStock-0
2022-06-24 14:01:01,390 ERROR org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception; nested exception is org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:206)
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:112)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2359)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2228)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2142)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2024)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1706)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2371)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2342)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2303)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2217)
	... 9 common frames omitted
Caused by: org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.data.mongodb.core.MongoExceptionTranslator.translateExceptionIfPossible(MongoExceptionTranslator.java:106)
	at org.springframework.data.mongodb.core.MongoTemplate.potentiallyConvertRuntimeException(MongoTemplate.java:2899)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:555)
	at org.springframework.data.mongodb.core.MongoTemplate.insertDocument(MongoTemplate.java:1458)
	at org.springframework.data.mongodb.core.MongoTemplate.doInsert(MongoTemplate.java:1257)
	at org.springframework.data.mongodb.core.MongoTemplate.insert(MongoTemplate.java:1172)
	at org.springframework.data.mongodb.repository.support.SimpleMongoRepository.save(SimpleMongoRepository.java:91)
	at jdk.internal.reflect.GeneratedMethodAccessor267.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:289)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:137)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:121)
	at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:529)
	at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:285)
	at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:599)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:163)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:138)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:80)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at jdk.internal.reflect.GeneratedMethodAccessor266.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:344)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:198)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:137)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at com.estockmarket.query.domain.service.StockService.updateStock(StockService.java:105)
	at com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(KafkaStocksEventListener.java:36)
	at jdk.internal.reflect.GeneratedMethodAccessor301.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:56)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:347)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:92)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:53)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2322)
	... 11 common frames omitted
Caused by: com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at com.mongodb.client.internal.MongoCollectionImpl.executeSingleWriteRequest(MongoCollectionImpl.java:1017)
	at com.mongodb.client.internal.MongoCollectionImpl.executeInsertOne(MongoCollectionImpl.java:470)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:453)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:447)
	at org.springframework.data.mongodb.core.MongoTemplate.lambda$insertDocument$16(MongoTemplate.java:1464)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:553)
	... 56 common frames omitted
2022-06-24 14:01:01,906 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks com.estockmarket.query.domain.model.Stocks@1c589901
2022-06-24 14:01:01,934 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Seeking to offset 3 for partition updateStock-0
2022-06-24 14:01:01,934 ERROR org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception; nested exception is org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:206)
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:112)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2359)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2228)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2142)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2024)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1706)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2371)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2342)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2303)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2217)
	... 9 common frames omitted
Caused by: org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.data.mongodb.core.MongoExceptionTranslator.translateExceptionIfPossible(MongoExceptionTranslator.java:106)
	at org.springframework.data.mongodb.core.MongoTemplate.potentiallyConvertRuntimeException(MongoTemplate.java:2899)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:555)
	at org.springframework.data.mongodb.core.MongoTemplate.insertDocument(MongoTemplate.java:1458)
	at org.springframework.data.mongodb.core.MongoTemplate.doInsert(MongoTemplate.java:1257)
	at org.springframework.data.mongodb.core.MongoTemplate.insert(MongoTemplate.java:1172)
	at org.springframework.data.mongodb.repository.support.SimpleMongoRepository.save(SimpleMongoRepository.java:91)
	at jdk.internal.reflect.GeneratedMethodAccessor267.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:289)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:137)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:121)
	at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:529)
	at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:285)
	at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:599)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:163)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:138)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:80)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at jdk.internal.reflect.GeneratedMethodAccessor266.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:344)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:198)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:137)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at com.estockmarket.query.domain.service.StockService.updateStock(StockService.java:105)
	at com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(KafkaStocksEventListener.java:36)
	at jdk.internal.reflect.GeneratedMethodAccessor301.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:56)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:347)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:92)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:53)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2322)
	... 11 common frames omitted
Caused by: com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at com.mongodb.client.internal.MongoCollectionImpl.executeSingleWriteRequest(MongoCollectionImpl.java:1017)
	at com.mongodb.client.internal.MongoCollectionImpl.executeInsertOne(MongoCollectionImpl.java:470)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:453)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:447)
	at org.springframework.data.mongodb.core.MongoTemplate.lambda$insertDocument$16(MongoTemplate.java:1464)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:553)
	... 56 common frames omitted
2022-06-24 14:01:02,449 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks com.estockmarket.query.domain.model.Stocks@360b8b2
2022-06-24 14:01:02,477 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Seeking to offset 3 for partition updateStock-0
2022-06-24 14:01:02,477 ERROR org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception; nested exception is org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:206)
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:112)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2359)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2228)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2142)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2024)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1706)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2371)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2342)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2303)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2217)
	... 9 common frames omitted
Caused by: org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.data.mongodb.core.MongoExceptionTranslator.translateExceptionIfPossible(MongoExceptionTranslator.java:106)
	at org.springframework.data.mongodb.core.MongoTemplate.potentiallyConvertRuntimeException(MongoTemplate.java:2899)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:555)
	at org.springframework.data.mongodb.core.MongoTemplate.insertDocument(MongoTemplate.java:1458)
	at org.springframework.data.mongodb.core.MongoTemplate.doInsert(MongoTemplate.java:1257)
	at org.springframework.data.mongodb.core.MongoTemplate.insert(MongoTemplate.java:1172)
	at org.springframework.data.mongodb.repository.support.SimpleMongoRepository.save(SimpleMongoRepository.java:91)
	at jdk.internal.reflect.GeneratedMethodAccessor267.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:289)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:137)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:121)
	at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:529)
	at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:285)
	at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:599)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:163)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:138)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:80)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at jdk.internal.reflect.GeneratedMethodAccessor266.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:344)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:198)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:137)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at com.estockmarket.query.domain.service.StockService.updateStock(StockService.java:105)
	at com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(KafkaStocksEventListener.java:36)
	at jdk.internal.reflect.GeneratedMethodAccessor301.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:56)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:347)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:92)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:53)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2322)
	... 11 common frames omitted
Caused by: com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at com.mongodb.client.internal.MongoCollectionImpl.executeSingleWriteRequest(MongoCollectionImpl.java:1017)
	at com.mongodb.client.internal.MongoCollectionImpl.executeInsertOne(MongoCollectionImpl.java:470)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:453)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:447)
	at org.springframework.data.mongodb.core.MongoTemplate.lambda$insertDocument$16(MongoTemplate.java:1464)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:553)
	... 56 common frames omitted
2022-06-24 14:01:02,991 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks com.estockmarket.query.domain.model.Stocks@1d016766
2022-06-24 14:01:03,020 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Seeking to offset 3 for partition updateStock-0
2022-06-24 14:01:03,020 ERROR org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception; nested exception is org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:206)
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:112)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2359)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2228)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2142)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2024)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1706)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2371)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2342)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2303)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2217)
	... 9 common frames omitted
Caused by: org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.data.mongodb.core.MongoExceptionTranslator.translateExceptionIfPossible(MongoExceptionTranslator.java:106)
	at org.springframework.data.mongodb.core.MongoTemplate.potentiallyConvertRuntimeException(MongoTemplate.java:2899)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:555)
	at org.springframework.data.mongodb.core.MongoTemplate.insertDocument(MongoTemplate.java:1458)
	at org.springframework.data.mongodb.core.MongoTemplate.doInsert(MongoTemplate.java:1257)
	at org.springframework.data.mongodb.core.MongoTemplate.insert(MongoTemplate.java:1172)
	at org.springframework.data.mongodb.repository.support.SimpleMongoRepository.save(SimpleMongoRepository.java:91)
	at jdk.internal.reflect.GeneratedMethodAccessor267.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:289)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:137)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:121)
	at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:529)
	at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:285)
	at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:599)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:163)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:138)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:80)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at jdk.internal.reflect.GeneratedMethodAccessor266.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:344)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:198)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:137)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at com.estockmarket.query.domain.service.StockService.updateStock(StockService.java:105)
	at com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(KafkaStocksEventListener.java:36)
	at jdk.internal.reflect.GeneratedMethodAccessor301.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:56)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:347)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:92)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:53)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2322)
	... 11 common frames omitted
Caused by: com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at com.mongodb.client.internal.MongoCollectionImpl.executeSingleWriteRequest(MongoCollectionImpl.java:1017)
	at com.mongodb.client.internal.MongoCollectionImpl.executeInsertOne(MongoCollectionImpl.java:470)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:453)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:447)
	at org.springframework.data.mongodb.core.MongoTemplate.lambda$insertDocument$16(MongoTemplate.java:1464)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:553)
	... 56 common frames omitted
2022-06-24 14:01:03,533 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks com.estockmarket.query.domain.model.Stocks@5e2d0e1e
2022-06-24 14:01:03,561 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Seeking to offset 3 for partition updateStock-0
2022-06-24 14:01:03,562 ERROR org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception; nested exception is org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:206)
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:112)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2359)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2228)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2142)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2024)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1706)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2371)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2342)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2303)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2217)
	... 9 common frames omitted
Caused by: org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.data.mongodb.core.MongoExceptionTranslator.translateExceptionIfPossible(MongoExceptionTranslator.java:106)
	at org.springframework.data.mongodb.core.MongoTemplate.potentiallyConvertRuntimeException(MongoTemplate.java:2899)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:555)
	at org.springframework.data.mongodb.core.MongoTemplate.insertDocument(MongoTemplate.java:1458)
	at org.springframework.data.mongodb.core.MongoTemplate.doInsert(MongoTemplate.java:1257)
	at org.springframework.data.mongodb.core.MongoTemplate.insert(MongoTemplate.java:1172)
	at org.springframework.data.mongodb.repository.support.SimpleMongoRepository.save(SimpleMongoRepository.java:91)
	at jdk.internal.reflect.GeneratedMethodAccessor267.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:289)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:137)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:121)
	at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:529)
	at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:285)
	at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:599)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:163)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:138)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:80)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at jdk.internal.reflect.GeneratedMethodAccessor266.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:344)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:198)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:137)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at com.estockmarket.query.domain.service.StockService.updateStock(StockService.java:105)
	at com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(KafkaStocksEventListener.java:36)
	at jdk.internal.reflect.GeneratedMethodAccessor301.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:56)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:347)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:92)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:53)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2322)
	... 11 common frames omitted
Caused by: com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at com.mongodb.client.internal.MongoCollectionImpl.executeSingleWriteRequest(MongoCollectionImpl.java:1017)
	at com.mongodb.client.internal.MongoCollectionImpl.executeInsertOne(MongoCollectionImpl.java:470)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:453)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:447)
	at org.springframework.data.mongodb.core.MongoTemplate.lambda$insertDocument$16(MongoTemplate.java:1464)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:553)
	... 56 common frames omitted
2022-06-24 14:01:03,655 INFO com.estockmarket.query.application.controller.StockController [http-nio-6093-exec-5] fetch min,max and average stock price based on company code,date C-008
2022-06-24 14:01:03,655 INFO com.estockmarket.query.application.controller.StockController [http-nio-6093-exec-3] fetch stocks based on code,date C-008
2022-06-24 14:01:03,655 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-5] fetch stockaggregate based on code,date C-008
2022-06-24 14:01:03,655 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-3] fetch company stocks based on code,date C-008
2022-06-24 14:01:03,656 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-3] fetch stock query based on code C-008
2022-06-24 14:01:03,656 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-5] fetch stock query based on code C-008
2022-06-24 14:01:04,080 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks com.estockmarket.query.domain.model.Stocks@665044b
2022-06-24 14:01:04,108 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Seeking to offset 3 for partition updateStock-0
2022-06-24 14:01:04,108 ERROR org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception; nested exception is org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:206)
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:112)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2359)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2228)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2142)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2024)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1706)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2371)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2342)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2303)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2217)
	... 9 common frames omitted
Caused by: org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.data.mongodb.core.MongoExceptionTranslator.translateExceptionIfPossible(MongoExceptionTranslator.java:106)
	at org.springframework.data.mongodb.core.MongoTemplate.potentiallyConvertRuntimeException(MongoTemplate.java:2899)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:555)
	at org.springframework.data.mongodb.core.MongoTemplate.insertDocument(MongoTemplate.java:1458)
	at org.springframework.data.mongodb.core.MongoTemplate.doInsert(MongoTemplate.java:1257)
	at org.springframework.data.mongodb.core.MongoTemplate.insert(MongoTemplate.java:1172)
	at org.springframework.data.mongodb.repository.support.SimpleMongoRepository.save(SimpleMongoRepository.java:91)
	at jdk.internal.reflect.GeneratedMethodAccessor267.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:289)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:137)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:121)
	at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:529)
	at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:285)
	at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:599)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:163)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:138)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:80)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at jdk.internal.reflect.GeneratedMethodAccessor266.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:344)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:198)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:137)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at com.estockmarket.query.domain.service.StockService.updateStock(StockService.java:105)
	at com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(KafkaStocksEventListener.java:36)
	at jdk.internal.reflect.GeneratedMethodAccessor301.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:56)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:347)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:92)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:53)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2322)
	... 11 common frames omitted
Caused by: com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at com.mongodb.client.internal.MongoCollectionImpl.executeSingleWriteRequest(MongoCollectionImpl.java:1017)
	at com.mongodb.client.internal.MongoCollectionImpl.executeInsertOne(MongoCollectionImpl.java:470)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:453)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:447)
	at org.springframework.data.mongodb.core.MongoTemplate.lambda$insertDocument$16(MongoTemplate.java:1464)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:553)
	... 56 common frames omitted
2022-06-24 14:01:04,626 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks com.estockmarket.query.domain.model.Stocks@c6991d
2022-06-24 14:01:04,630 ERROR org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Backoff FixedBackOff{interval=0, currentAttempts=10, maxAttempts=9} exhausted for updateStock-0@3
org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2371)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2342)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2303)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2217)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2142)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2024)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1706)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.data.mongodb.core.MongoExceptionTranslator.translateExceptionIfPossible(MongoExceptionTranslator.java:106)
	at org.springframework.data.mongodb.core.MongoTemplate.potentiallyConvertRuntimeException(MongoTemplate.java:2899)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:555)
	at org.springframework.data.mongodb.core.MongoTemplate.insertDocument(MongoTemplate.java:1458)
	at org.springframework.data.mongodb.core.MongoTemplate.doInsert(MongoTemplate.java:1257)
	at org.springframework.data.mongodb.core.MongoTemplate.insert(MongoTemplate.java:1172)
	at org.springframework.data.mongodb.repository.support.SimpleMongoRepository.save(SimpleMongoRepository.java:91)
	at jdk.internal.reflect.GeneratedMethodAccessor267.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:289)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:137)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:121)
	at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:529)
	at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:285)
	at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:599)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:163)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:138)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:80)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at jdk.internal.reflect.GeneratedMethodAccessor266.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:344)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:198)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:137)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at com.estockmarket.query.domain.service.StockService.updateStock(StockService.java:105)
	at com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(KafkaStocksEventListener.java:36)
	at jdk.internal.reflect.GeneratedMethodAccessor301.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:56)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:347)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:92)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:53)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2322)
	... 11 common frames omitted
Caused by: com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at com.mongodb.client.internal.MongoCollectionImpl.executeSingleWriteRequest(MongoCollectionImpl.java:1017)
	at com.mongodb.client.internal.MongoCollectionImpl.executeInsertOne(MongoCollectionImpl.java:470)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:453)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:447)
	at org.springframework.data.mongodb.core.MongoTemplate.lambda$insertDocument$16(MongoTemplate.java:1464)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:553)
	... 56 common frames omitted
2022-06-24 14:01:10,013 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks com.estockmarket.query.domain.model.Stocks@166ae434
2022-06-24 14:01:10,027 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Seeking to offset 4 for partition updateStock-0
2022-06-24 14:01:10,028 ERROR org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception; nested exception is org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:206)
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:112)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2359)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2228)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2142)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2024)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1706)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2371)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2342)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2303)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2217)
	... 9 common frames omitted
Caused by: org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.data.mongodb.core.MongoExceptionTranslator.translateExceptionIfPossible(MongoExceptionTranslator.java:106)
	at org.springframework.data.mongodb.core.MongoTemplate.potentiallyConvertRuntimeException(MongoTemplate.java:2899)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:555)
	at org.springframework.data.mongodb.core.MongoTemplate.insertDocument(MongoTemplate.java:1458)
	at org.springframework.data.mongodb.core.MongoTemplate.doInsert(MongoTemplate.java:1257)
	at org.springframework.data.mongodb.core.MongoTemplate.insert(MongoTemplate.java:1172)
	at org.springframework.data.mongodb.repository.support.SimpleMongoRepository.save(SimpleMongoRepository.java:91)
	at jdk.internal.reflect.GeneratedMethodAccessor267.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:289)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:137)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:121)
	at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:529)
	at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:285)
	at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:599)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:163)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:138)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:80)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at jdk.internal.reflect.GeneratedMethodAccessor266.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:344)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:198)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:137)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at com.estockmarket.query.domain.service.StockService.updateStock(StockService.java:105)
	at com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(KafkaStocksEventListener.java:36)
	at jdk.internal.reflect.GeneratedMethodAccessor301.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:56)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:347)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:92)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:53)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2322)
	... 11 common frames omitted
Caused by: com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at com.mongodb.client.internal.MongoCollectionImpl.executeSingleWriteRequest(MongoCollectionImpl.java:1017)
	at com.mongodb.client.internal.MongoCollectionImpl.executeInsertOne(MongoCollectionImpl.java:470)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:453)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:447)
	at org.springframework.data.mongodb.core.MongoTemplate.lambda$insertDocument$16(MongoTemplate.java:1464)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:553)
	... 56 common frames omitted
2022-06-24 14:01:10,029 INFO com.estockmarket.query.application.controller.StockController [http-nio-6093-exec-4] fetch stocks based on code,date C-008
2022-06-24 14:01:10,029 INFO com.estockmarket.query.application.controller.StockController [http-nio-6093-exec-9] fetch min,max and average stock price based on company code,date C-008
2022-06-24 14:01:10,029 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-4] fetch company stocks based on code,date C-008
2022-06-24 14:01:10,029 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-9] fetch stockaggregate based on code,date C-008
2022-06-24 14:01:10,029 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-9] fetch stock query based on code C-008
2022-06-24 14:01:10,029 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-4] fetch stock query based on code C-008
2022-06-24 14:01:10,544 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks com.estockmarket.query.domain.model.Stocks@20014519
2022-06-24 14:01:10,571 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Seeking to offset 4 for partition updateStock-0
2022-06-24 14:01:10,571 ERROR org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception; nested exception is org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:206)
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:112)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2359)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2228)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2142)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2024)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1706)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2371)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2342)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2303)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2217)
	... 9 common frames omitted
Caused by: org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.data.mongodb.core.MongoExceptionTranslator.translateExceptionIfPossible(MongoExceptionTranslator.java:106)
	at org.springframework.data.mongodb.core.MongoTemplate.potentiallyConvertRuntimeException(MongoTemplate.java:2899)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:555)
	at org.springframework.data.mongodb.core.MongoTemplate.insertDocument(MongoTemplate.java:1458)
	at org.springframework.data.mongodb.core.MongoTemplate.doInsert(MongoTemplate.java:1257)
	at org.springframework.data.mongodb.core.MongoTemplate.insert(MongoTemplate.java:1172)
	at org.springframework.data.mongodb.repository.support.SimpleMongoRepository.save(SimpleMongoRepository.java:91)
	at jdk.internal.reflect.GeneratedMethodAccessor267.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:289)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:137)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:121)
	at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:529)
	at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:285)
	at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:599)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:163)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:138)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:80)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at jdk.internal.reflect.GeneratedMethodAccessor266.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:344)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:198)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:137)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at com.estockmarket.query.domain.service.StockService.updateStock(StockService.java:105)
	at com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(KafkaStocksEventListener.java:36)
	at jdk.internal.reflect.GeneratedMethodAccessor301.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:56)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:347)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:92)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:53)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2322)
	... 11 common frames omitted
Caused by: com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at com.mongodb.client.internal.MongoCollectionImpl.executeSingleWriteRequest(MongoCollectionImpl.java:1017)
	at com.mongodb.client.internal.MongoCollectionImpl.executeInsertOne(MongoCollectionImpl.java:470)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:453)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:447)
	at org.springframework.data.mongodb.core.MongoTemplate.lambda$insertDocument$16(MongoTemplate.java:1464)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:553)
	... 56 common frames omitted
2022-06-24 14:01:11,076 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks com.estockmarket.query.domain.model.Stocks@ee85a0e
2022-06-24 14:01:11,089 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Seeking to offset 4 for partition updateStock-0
2022-06-24 14:01:11,089 ERROR org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception; nested exception is org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:206)
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:112)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2359)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2228)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2142)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2024)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1706)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2371)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2342)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2303)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2217)
	... 9 common frames omitted
Caused by: org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.data.mongodb.core.MongoExceptionTranslator.translateExceptionIfPossible(MongoExceptionTranslator.java:106)
	at org.springframework.data.mongodb.core.MongoTemplate.potentiallyConvertRuntimeException(MongoTemplate.java:2899)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:555)
	at org.springframework.data.mongodb.core.MongoTemplate.insertDocument(MongoTemplate.java:1458)
	at org.springframework.data.mongodb.core.MongoTemplate.doInsert(MongoTemplate.java:1257)
	at org.springframework.data.mongodb.core.MongoTemplate.insert(MongoTemplate.java:1172)
	at org.springframework.data.mongodb.repository.support.SimpleMongoRepository.save(SimpleMongoRepository.java:91)
	at jdk.internal.reflect.GeneratedMethodAccessor267.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:289)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:137)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:121)
	at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:529)
	at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:285)
	at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:599)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:163)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:138)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:80)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at jdk.internal.reflect.GeneratedMethodAccessor266.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:344)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:198)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:137)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at com.estockmarket.query.domain.service.StockService.updateStock(StockService.java:105)
	at com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(KafkaStocksEventListener.java:36)
	at jdk.internal.reflect.GeneratedMethodAccessor301.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:56)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:347)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:92)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:53)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2322)
	... 11 common frames omitted
Caused by: com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at com.mongodb.client.internal.MongoCollectionImpl.executeSingleWriteRequest(MongoCollectionImpl.java:1017)
	at com.mongodb.client.internal.MongoCollectionImpl.executeInsertOne(MongoCollectionImpl.java:470)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:453)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:447)
	at org.springframework.data.mongodb.core.MongoTemplate.lambda$insertDocument$16(MongoTemplate.java:1464)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:553)
	... 56 common frames omitted
2022-06-24 14:01:11,606 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks com.estockmarket.query.domain.model.Stocks@3a5234bf
2022-06-24 14:01:11,633 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Seeking to offset 4 for partition updateStock-0
2022-06-24 14:01:11,633 ERROR org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception; nested exception is org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:206)
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:112)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2359)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2228)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2142)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2024)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1706)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2371)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2342)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2303)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2217)
	... 9 common frames omitted
Caused by: org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.data.mongodb.core.MongoExceptionTranslator.translateExceptionIfPossible(MongoExceptionTranslator.java:106)
	at org.springframework.data.mongodb.core.MongoTemplate.potentiallyConvertRuntimeException(MongoTemplate.java:2899)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:555)
	at org.springframework.data.mongodb.core.MongoTemplate.insertDocument(MongoTemplate.java:1458)
	at org.springframework.data.mongodb.core.MongoTemplate.doInsert(MongoTemplate.java:1257)
	at org.springframework.data.mongodb.core.MongoTemplate.insert(MongoTemplate.java:1172)
	at org.springframework.data.mongodb.repository.support.SimpleMongoRepository.save(SimpleMongoRepository.java:91)
	at jdk.internal.reflect.GeneratedMethodAccessor267.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:289)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:137)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:121)
	at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:529)
	at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:285)
	at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:599)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:163)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:138)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:80)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at jdk.internal.reflect.GeneratedMethodAccessor266.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:344)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:198)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:137)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at com.estockmarket.query.domain.service.StockService.updateStock(StockService.java:105)
	at com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(KafkaStocksEventListener.java:36)
	at jdk.internal.reflect.GeneratedMethodAccessor301.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:56)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:347)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:92)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:53)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2322)
	... 11 common frames omitted
Caused by: com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at com.mongodb.client.internal.MongoCollectionImpl.executeSingleWriteRequest(MongoCollectionImpl.java:1017)
	at com.mongodb.client.internal.MongoCollectionImpl.executeInsertOne(MongoCollectionImpl.java:470)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:453)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:447)
	at org.springframework.data.mongodb.core.MongoTemplate.lambda$insertDocument$16(MongoTemplate.java:1464)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:553)
	... 56 common frames omitted
2022-06-24 14:01:12,146 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks com.estockmarket.query.domain.model.Stocks@7b583c85
2022-06-24 14:01:12,174 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Seeking to offset 4 for partition updateStock-0
2022-06-24 14:01:12,175 ERROR org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception; nested exception is org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:206)
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:112)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2359)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2228)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2142)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2024)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1706)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2371)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2342)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2303)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2217)
	... 9 common frames omitted
Caused by: org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.data.mongodb.core.MongoExceptionTranslator.translateExceptionIfPossible(MongoExceptionTranslator.java:106)
	at org.springframework.data.mongodb.core.MongoTemplate.potentiallyConvertRuntimeException(MongoTemplate.java:2899)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:555)
	at org.springframework.data.mongodb.core.MongoTemplate.insertDocument(MongoTemplate.java:1458)
	at org.springframework.data.mongodb.core.MongoTemplate.doInsert(MongoTemplate.java:1257)
	at org.springframework.data.mongodb.core.MongoTemplate.insert(MongoTemplate.java:1172)
	at org.springframework.data.mongodb.repository.support.SimpleMongoRepository.save(SimpleMongoRepository.java:91)
	at jdk.internal.reflect.GeneratedMethodAccessor267.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:289)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:137)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:121)
	at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:529)
	at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:285)
	at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:599)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:163)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:138)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:80)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at jdk.internal.reflect.GeneratedMethodAccessor266.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:344)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:198)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:137)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at com.estockmarket.query.domain.service.StockService.updateStock(StockService.java:105)
	at com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(KafkaStocksEventListener.java:36)
	at jdk.internal.reflect.GeneratedMethodAccessor301.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:56)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:347)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:92)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:53)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2322)
	... 11 common frames omitted
Caused by: com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at com.mongodb.client.internal.MongoCollectionImpl.executeSingleWriteRequest(MongoCollectionImpl.java:1017)
	at com.mongodb.client.internal.MongoCollectionImpl.executeInsertOne(MongoCollectionImpl.java:470)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:453)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:447)
	at org.springframework.data.mongodb.core.MongoTemplate.lambda$insertDocument$16(MongoTemplate.java:1464)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:553)
	... 56 common frames omitted
2022-06-24 14:01:12,689 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks com.estockmarket.query.domain.model.Stocks@515b977a
2022-06-24 14:01:12,716 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Seeking to offset 4 for partition updateStock-0
2022-06-24 14:01:12,716 ERROR org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception; nested exception is org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:206)
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:112)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2359)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2228)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2142)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2024)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1706)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2371)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2342)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2303)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2217)
	... 9 common frames omitted
Caused by: org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.data.mongodb.core.MongoExceptionTranslator.translateExceptionIfPossible(MongoExceptionTranslator.java:106)
	at org.springframework.data.mongodb.core.MongoTemplate.potentiallyConvertRuntimeException(MongoTemplate.java:2899)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:555)
	at org.springframework.data.mongodb.core.MongoTemplate.insertDocument(MongoTemplate.java:1458)
	at org.springframework.data.mongodb.core.MongoTemplate.doInsert(MongoTemplate.java:1257)
	at org.springframework.data.mongodb.core.MongoTemplate.insert(MongoTemplate.java:1172)
	at org.springframework.data.mongodb.repository.support.SimpleMongoRepository.save(SimpleMongoRepository.java:91)
	at jdk.internal.reflect.GeneratedMethodAccessor267.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:289)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:137)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:121)
	at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:529)
	at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:285)
	at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:599)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:163)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:138)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:80)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at jdk.internal.reflect.GeneratedMethodAccessor266.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:344)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:198)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:137)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at com.estockmarket.query.domain.service.StockService.updateStock(StockService.java:105)
	at com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(KafkaStocksEventListener.java:36)
	at jdk.internal.reflect.GeneratedMethodAccessor301.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:56)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:347)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:92)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:53)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2322)
	... 11 common frames omitted
Caused by: com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at com.mongodb.client.internal.MongoCollectionImpl.executeSingleWriteRequest(MongoCollectionImpl.java:1017)
	at com.mongodb.client.internal.MongoCollectionImpl.executeInsertOne(MongoCollectionImpl.java:470)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:453)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:447)
	at org.springframework.data.mongodb.core.MongoTemplate.lambda$insertDocument$16(MongoTemplate.java:1464)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:553)
	... 56 common frames omitted
2022-06-24 14:01:13,230 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks com.estockmarket.query.domain.model.Stocks@1daf8f75
2022-06-24 14:01:13,259 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Seeking to offset 4 for partition updateStock-0
2022-06-24 14:01:13,259 ERROR org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception; nested exception is org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:206)
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:112)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2359)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2228)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2142)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2024)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1706)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2371)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2342)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2303)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2217)
	... 9 common frames omitted
Caused by: org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.data.mongodb.core.MongoExceptionTranslator.translateExceptionIfPossible(MongoExceptionTranslator.java:106)
	at org.springframework.data.mongodb.core.MongoTemplate.potentiallyConvertRuntimeException(MongoTemplate.java:2899)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:555)
	at org.springframework.data.mongodb.core.MongoTemplate.insertDocument(MongoTemplate.java:1458)
	at org.springframework.data.mongodb.core.MongoTemplate.doInsert(MongoTemplate.java:1257)
	at org.springframework.data.mongodb.core.MongoTemplate.insert(MongoTemplate.java:1172)
	at org.springframework.data.mongodb.repository.support.SimpleMongoRepository.save(SimpleMongoRepository.java:91)
	at jdk.internal.reflect.GeneratedMethodAccessor267.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:289)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:137)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:121)
	at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:529)
	at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:285)
	at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:599)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:163)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:138)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:80)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at jdk.internal.reflect.GeneratedMethodAccessor266.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:344)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:198)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:137)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at com.estockmarket.query.domain.service.StockService.updateStock(StockService.java:105)
	at com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(KafkaStocksEventListener.java:36)
	at jdk.internal.reflect.GeneratedMethodAccessor301.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:56)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:347)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:92)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:53)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2322)
	... 11 common frames omitted
Caused by: com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at com.mongodb.client.internal.MongoCollectionImpl.executeSingleWriteRequest(MongoCollectionImpl.java:1017)
	at com.mongodb.client.internal.MongoCollectionImpl.executeInsertOne(MongoCollectionImpl.java:470)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:453)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:447)
	at org.springframework.data.mongodb.core.MongoTemplate.lambda$insertDocument$16(MongoTemplate.java:1464)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:553)
	... 56 common frames omitted
2022-06-24 14:01:13,775 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks com.estockmarket.query.domain.model.Stocks@36f3a7b3
2022-06-24 14:01:13,804 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Seeking to offset 4 for partition updateStock-0
2022-06-24 14:01:13,804 ERROR org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception; nested exception is org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:206)
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:112)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2359)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2228)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2142)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2024)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1706)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2371)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2342)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2303)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2217)
	... 9 common frames omitted
Caused by: org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.data.mongodb.core.MongoExceptionTranslator.translateExceptionIfPossible(MongoExceptionTranslator.java:106)
	at org.springframework.data.mongodb.core.MongoTemplate.potentiallyConvertRuntimeException(MongoTemplate.java:2899)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:555)
	at org.springframework.data.mongodb.core.MongoTemplate.insertDocument(MongoTemplate.java:1458)
	at org.springframework.data.mongodb.core.MongoTemplate.doInsert(MongoTemplate.java:1257)
	at org.springframework.data.mongodb.core.MongoTemplate.insert(MongoTemplate.java:1172)
	at org.springframework.data.mongodb.repository.support.SimpleMongoRepository.save(SimpleMongoRepository.java:91)
	at jdk.internal.reflect.GeneratedMethodAccessor267.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:289)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:137)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:121)
	at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:529)
	at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:285)
	at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:599)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:163)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:138)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:80)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at jdk.internal.reflect.GeneratedMethodAccessor266.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:344)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:198)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:137)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at com.estockmarket.query.domain.service.StockService.updateStock(StockService.java:105)
	at com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(KafkaStocksEventListener.java:36)
	at jdk.internal.reflect.GeneratedMethodAccessor301.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:56)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:347)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:92)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:53)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2322)
	... 11 common frames omitted
Caused by: com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at com.mongodb.client.internal.MongoCollectionImpl.executeSingleWriteRequest(MongoCollectionImpl.java:1017)
	at com.mongodb.client.internal.MongoCollectionImpl.executeInsertOne(MongoCollectionImpl.java:470)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:453)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:447)
	at org.springframework.data.mongodb.core.MongoTemplate.lambda$insertDocument$16(MongoTemplate.java:1464)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:553)
	... 56 common frames omitted
2022-06-24 14:01:14,322 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks com.estockmarket.query.domain.model.Stocks@cf509fc
2022-06-24 14:01:14,350 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Seeking to offset 4 for partition updateStock-0
2022-06-24 14:01:14,350 ERROR org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception; nested exception is org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:206)
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:112)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2359)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2228)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2142)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2024)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1706)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2371)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2342)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2303)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2217)
	... 9 common frames omitted
Caused by: org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.data.mongodb.core.MongoExceptionTranslator.translateExceptionIfPossible(MongoExceptionTranslator.java:106)
	at org.springframework.data.mongodb.core.MongoTemplate.potentiallyConvertRuntimeException(MongoTemplate.java:2899)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:555)
	at org.springframework.data.mongodb.core.MongoTemplate.insertDocument(MongoTemplate.java:1458)
	at org.springframework.data.mongodb.core.MongoTemplate.doInsert(MongoTemplate.java:1257)
	at org.springframework.data.mongodb.core.MongoTemplate.insert(MongoTemplate.java:1172)
	at org.springframework.data.mongodb.repository.support.SimpleMongoRepository.save(SimpleMongoRepository.java:91)
	at jdk.internal.reflect.GeneratedMethodAccessor267.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:289)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:137)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:121)
	at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:529)
	at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:285)
	at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:599)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:163)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:138)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:80)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at jdk.internal.reflect.GeneratedMethodAccessor266.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:344)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:198)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:137)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at com.estockmarket.query.domain.service.StockService.updateStock(StockService.java:105)
	at com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(KafkaStocksEventListener.java:36)
	at jdk.internal.reflect.GeneratedMethodAccessor301.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:56)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:347)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:92)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:53)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2322)
	... 11 common frames omitted
Caused by: com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at com.mongodb.client.internal.MongoCollectionImpl.executeSingleWriteRequest(MongoCollectionImpl.java:1017)
	at com.mongodb.client.internal.MongoCollectionImpl.executeInsertOne(MongoCollectionImpl.java:470)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:453)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:447)
	at org.springframework.data.mongodb.core.MongoTemplate.lambda$insertDocument$16(MongoTemplate.java:1464)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:553)
	... 56 common frames omitted
2022-06-24 14:01:14,864 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks com.estockmarket.query.domain.model.Stocks@60ad4c3e
2022-06-24 14:01:14,867 ERROR org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Backoff FixedBackOff{interval=0, currentAttempts=10, maxAttempts=9} exhausted for updateStock-0@4
org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2371)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2342)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2303)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2217)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2142)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2024)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1706)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.data.mongodb.core.MongoExceptionTranslator.translateExceptionIfPossible(MongoExceptionTranslator.java:106)
	at org.springframework.data.mongodb.core.MongoTemplate.potentiallyConvertRuntimeException(MongoTemplate.java:2899)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:555)
	at org.springframework.data.mongodb.core.MongoTemplate.insertDocument(MongoTemplate.java:1458)
	at org.springframework.data.mongodb.core.MongoTemplate.doInsert(MongoTemplate.java:1257)
	at org.springframework.data.mongodb.core.MongoTemplate.insert(MongoTemplate.java:1172)
	at org.springframework.data.mongodb.repository.support.SimpleMongoRepository.save(SimpleMongoRepository.java:91)
	at jdk.internal.reflect.GeneratedMethodAccessor267.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:289)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:137)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:121)
	at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:529)
	at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:285)
	at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:599)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:163)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:138)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:80)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at jdk.internal.reflect.GeneratedMethodAccessor266.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:344)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:198)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:137)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at com.estockmarket.query.domain.service.StockService.updateStock(StockService.java:105)
	at com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(KafkaStocksEventListener.java:36)
	at jdk.internal.reflect.GeneratedMethodAccessor301.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:56)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:347)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:92)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:53)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2322)
	... 11 common frames omitted
Caused by: com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at com.mongodb.client.internal.MongoCollectionImpl.executeSingleWriteRequest(MongoCollectionImpl.java:1017)
	at com.mongodb.client.internal.MongoCollectionImpl.executeInsertOne(MongoCollectionImpl.java:470)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:453)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:447)
	at org.springframework.data.mongodb.core.MongoTemplate.lambda$insertDocument$16(MongoTemplate.java:1464)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:553)
	... 56 common frames omitted
2022-06-24 14:01:14,869 ERROR org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Offset commit failed on partition updateStock-0 at offset 5: The coordinator is not aware of this member.
2022-06-24 14:01:14,869 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] OffsetCommit failed with Generation{generationId=78, memberId='consumer-group_id-30-6d0093ad-7a73-4141-b5da-b2fb576e5032', protocol='range'}: The coordinator is not aware of this member.
2022-06-24 14:01:14,870 ERROR org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Consumer exception
java.lang.IllegalStateException: This error handler cannot process 'org.apache.kafka.clients.consumer.CommitFailedException's; no record information is available
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:200)
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:112)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.handleConsumerException(KafkaMessageListenerContainer.java:1602)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1210)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing max.poll.interval.ms or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:1256)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:1163)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:1173)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:1148)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:206)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:169)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:129)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:602)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:412)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:297)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:236)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:215)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.commitOffsetsSync(ConsumerCoordinator.java:1005)
	at org.apache.kafka.clients.consumer.KafkaConsumer.commitSync(KafkaConsumer.java:1495)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doCommitSync(KafkaMessageListenerContainer.java:2656)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.commitSync(KafkaMessageListenerContainer.java:2651)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.commitIfNecessary(KafkaMessageListenerContainer.java:2637)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.processCommits(KafkaMessageListenerContainer.java:2451)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1235)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	... 3 common frames omitted
2022-06-24 14:01:14,870 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Giving away all assigned partitions as lost since generation has been reset,indicating that consumer is no longer part of the group
2022-06-24 14:01:14,870 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Lost previously assigned partitions updateStock-0
2022-06-24 14:01:14,870 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] group_id: partitions lost: [updateStock-0]
2022-06-24 14:01:14,871 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] group_id: partitions revoked: [updateStock-0]
2022-06-24 14:01:14,871 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] (Re-)joining group
2022-06-24 14:01:14,872 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] (Re-)joining group
2022-06-24 14:01:14,876 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Successfully joined group with generation Generation{generationId=80, memberId='consumer-group_id-30-4c8417ac-6593-460e-b4ea-8cc61629e4e6', protocol='range'}
2022-06-24 14:01:14,876 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Finished assignment for group at generation 80: {consumer-group_id-30-4c8417ac-6593-460e-b4ea-8cc61629e4e6=Assignment(partitions=[updateStock-0])}
2022-06-24 14:01:14,879 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Successfully synced group in generation Generation{generationId=80, memberId='consumer-group_id-30-4c8417ac-6593-460e-b4ea-8cc61629e4e6', protocol='range'}
2022-06-24 14:01:14,879 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Notifying assignor about the new Assignment(partitions=[updateStock-0])
2022-06-24 14:01:14,879 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Adding newly assigned partitions: updateStock-0
2022-06-24 14:01:14,881 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Setting offset for partition updateStock-0 to the committed offset FetchPosition{offset=4, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:01:14,881 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] group_id: partitions assigned: [updateStock-0]
2022-06-24 14:01:15,372 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks com.estockmarket.query.domain.model.Stocks@2f7deecd
2022-06-24 14:01:15,401 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Seeking to offset 4 for partition updateStock-0
2022-06-24 14:01:15,401 ERROR org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception; nested exception is org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:206)
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:112)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2359)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2228)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2142)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2024)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1706)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2371)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2342)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2303)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2217)
	... 9 common frames omitted
Caused by: org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.data.mongodb.core.MongoExceptionTranslator.translateExceptionIfPossible(MongoExceptionTranslator.java:106)
	at org.springframework.data.mongodb.core.MongoTemplate.potentiallyConvertRuntimeException(MongoTemplate.java:2899)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:555)
	at org.springframework.data.mongodb.core.MongoTemplate.insertDocument(MongoTemplate.java:1458)
	at org.springframework.data.mongodb.core.MongoTemplate.doInsert(MongoTemplate.java:1257)
	at org.springframework.data.mongodb.core.MongoTemplate.insert(MongoTemplate.java:1172)
	at org.springframework.data.mongodb.repository.support.SimpleMongoRepository.save(SimpleMongoRepository.java:91)
	at jdk.internal.reflect.GeneratedMethodAccessor267.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:289)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:137)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:121)
	at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:529)
	at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:285)
	at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:599)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:163)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:138)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:80)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at jdk.internal.reflect.GeneratedMethodAccessor266.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:344)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:198)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:137)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at com.estockmarket.query.domain.service.StockService.updateStock(StockService.java:105)
	at com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(KafkaStocksEventListener.java:36)
	at jdk.internal.reflect.GeneratedMethodAccessor301.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:56)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:347)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:92)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:53)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2322)
	... 11 common frames omitted
Caused by: com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at com.mongodb.client.internal.MongoCollectionImpl.executeSingleWriteRequest(MongoCollectionImpl.java:1017)
	at com.mongodb.client.internal.MongoCollectionImpl.executeInsertOne(MongoCollectionImpl.java:470)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:453)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:447)
	at org.springframework.data.mongodb.core.MongoTemplate.lambda$insertDocument$16(MongoTemplate.java:1464)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:553)
	... 56 common frames omitted
2022-06-24 14:01:15,917 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks com.estockmarket.query.domain.model.Stocks@772dc04a
2022-06-24 14:01:15,945 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Seeking to offset 4 for partition updateStock-0
2022-06-24 14:01:15,945 ERROR org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception; nested exception is org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:206)
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:112)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2359)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2228)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2142)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2024)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1706)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2371)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2342)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2303)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2217)
	... 9 common frames omitted
Caused by: org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.data.mongodb.core.MongoExceptionTranslator.translateExceptionIfPossible(MongoExceptionTranslator.java:106)
	at org.springframework.data.mongodb.core.MongoTemplate.potentiallyConvertRuntimeException(MongoTemplate.java:2899)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:555)
	at org.springframework.data.mongodb.core.MongoTemplate.insertDocument(MongoTemplate.java:1458)
	at org.springframework.data.mongodb.core.MongoTemplate.doInsert(MongoTemplate.java:1257)
	at org.springframework.data.mongodb.core.MongoTemplate.insert(MongoTemplate.java:1172)
	at org.springframework.data.mongodb.repository.support.SimpleMongoRepository.save(SimpleMongoRepository.java:91)
	at jdk.internal.reflect.GeneratedMethodAccessor267.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:289)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:137)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:121)
	at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:529)
	at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:285)
	at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:599)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:163)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:138)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:80)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at jdk.internal.reflect.GeneratedMethodAccessor266.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:344)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:198)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:137)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at com.estockmarket.query.domain.service.StockService.updateStock(StockService.java:105)
	at com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(KafkaStocksEventListener.java:36)
	at jdk.internal.reflect.GeneratedMethodAccessor301.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:56)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:347)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:92)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:53)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2322)
	... 11 common frames omitted
Caused by: com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at com.mongodb.client.internal.MongoCollectionImpl.executeSingleWriteRequest(MongoCollectionImpl.java:1017)
	at com.mongodb.client.internal.MongoCollectionImpl.executeInsertOne(MongoCollectionImpl.java:470)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:453)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:447)
	at org.springframework.data.mongodb.core.MongoTemplate.lambda$insertDocument$16(MongoTemplate.java:1464)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:553)
	... 56 common frames omitted
2022-06-24 14:01:16,459 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks com.estockmarket.query.domain.model.Stocks@e36ad0f
2022-06-24 14:01:16,487 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Seeking to offset 4 for partition updateStock-0
2022-06-24 14:01:16,487 ERROR org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception; nested exception is org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:206)
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:112)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2359)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2228)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2142)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2024)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1706)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2371)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2342)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2303)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2217)
	... 9 common frames omitted
Caused by: org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.data.mongodb.core.MongoExceptionTranslator.translateExceptionIfPossible(MongoExceptionTranslator.java:106)
	at org.springframework.data.mongodb.core.MongoTemplate.potentiallyConvertRuntimeException(MongoTemplate.java:2899)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:555)
	at org.springframework.data.mongodb.core.MongoTemplate.insertDocument(MongoTemplate.java:1458)
	at org.springframework.data.mongodb.core.MongoTemplate.doInsert(MongoTemplate.java:1257)
	at org.springframework.data.mongodb.core.MongoTemplate.insert(MongoTemplate.java:1172)
	at org.springframework.data.mongodb.repository.support.SimpleMongoRepository.save(SimpleMongoRepository.java:91)
	at jdk.internal.reflect.GeneratedMethodAccessor267.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:289)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:137)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:121)
	at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:529)
	at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:285)
	at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:599)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:163)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:138)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:80)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at jdk.internal.reflect.GeneratedMethodAccessor266.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:344)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:198)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:137)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at com.estockmarket.query.domain.service.StockService.updateStock(StockService.java:105)
	at com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(KafkaStocksEventListener.java:36)
	at jdk.internal.reflect.GeneratedMethodAccessor301.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:56)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:347)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:92)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:53)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2322)
	... 11 common frames omitted
Caused by: com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at com.mongodb.client.internal.MongoCollectionImpl.executeSingleWriteRequest(MongoCollectionImpl.java:1017)
	at com.mongodb.client.internal.MongoCollectionImpl.executeInsertOne(MongoCollectionImpl.java:470)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:453)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:447)
	at org.springframework.data.mongodb.core.MongoTemplate.lambda$insertDocument$16(MongoTemplate.java:1464)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:553)
	... 56 common frames omitted
2022-06-24 14:01:17,004 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks com.estockmarket.query.domain.model.Stocks@17800313
2022-06-24 14:01:17,031 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Seeking to offset 4 for partition updateStock-0
2022-06-24 14:01:17,031 ERROR org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception; nested exception is org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:206)
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:112)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2359)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2228)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2142)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2024)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1706)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2371)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2342)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2303)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2217)
	... 9 common frames omitted
Caused by: org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.data.mongodb.core.MongoExceptionTranslator.translateExceptionIfPossible(MongoExceptionTranslator.java:106)
	at org.springframework.data.mongodb.core.MongoTemplate.potentiallyConvertRuntimeException(MongoTemplate.java:2899)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:555)
	at org.springframework.data.mongodb.core.MongoTemplate.insertDocument(MongoTemplate.java:1458)
	at org.springframework.data.mongodb.core.MongoTemplate.doInsert(MongoTemplate.java:1257)
	at org.springframework.data.mongodb.core.MongoTemplate.insert(MongoTemplate.java:1172)
	at org.springframework.data.mongodb.repository.support.SimpleMongoRepository.save(SimpleMongoRepository.java:91)
	at jdk.internal.reflect.GeneratedMethodAccessor267.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:289)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:137)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:121)
	at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:529)
	at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:285)
	at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:599)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:163)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:138)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:80)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at jdk.internal.reflect.GeneratedMethodAccessor266.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:344)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:198)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:137)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at com.estockmarket.query.domain.service.StockService.updateStock(StockService.java:105)
	at com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(KafkaStocksEventListener.java:36)
	at jdk.internal.reflect.GeneratedMethodAccessor301.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:56)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:347)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:92)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:53)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2322)
	... 11 common frames omitted
Caused by: com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at com.mongodb.client.internal.MongoCollectionImpl.executeSingleWriteRequest(MongoCollectionImpl.java:1017)
	at com.mongodb.client.internal.MongoCollectionImpl.executeInsertOne(MongoCollectionImpl.java:470)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:453)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:447)
	at org.springframework.data.mongodb.core.MongoTemplate.lambda$insertDocument$16(MongoTemplate.java:1464)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:553)
	... 56 common frames omitted
2022-06-24 14:01:17,544 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks com.estockmarket.query.domain.model.Stocks@1ac8aa
2022-06-24 14:01:17,573 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Seeking to offset 4 for partition updateStock-0
2022-06-24 14:01:17,573 ERROR org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception; nested exception is org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:206)
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:112)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2359)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2228)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2142)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2024)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1706)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2371)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2342)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2303)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2217)
	... 9 common frames omitted
Caused by: org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.data.mongodb.core.MongoExceptionTranslator.translateExceptionIfPossible(MongoExceptionTranslator.java:106)
	at org.springframework.data.mongodb.core.MongoTemplate.potentiallyConvertRuntimeException(MongoTemplate.java:2899)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:555)
	at org.springframework.data.mongodb.core.MongoTemplate.insertDocument(MongoTemplate.java:1458)
	at org.springframework.data.mongodb.core.MongoTemplate.doInsert(MongoTemplate.java:1257)
	at org.springframework.data.mongodb.core.MongoTemplate.insert(MongoTemplate.java:1172)
	at org.springframework.data.mongodb.repository.support.SimpleMongoRepository.save(SimpleMongoRepository.java:91)
	at jdk.internal.reflect.GeneratedMethodAccessor267.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:289)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:137)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:121)
	at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:529)
	at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:285)
	at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:599)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:163)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:138)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:80)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at jdk.internal.reflect.GeneratedMethodAccessor266.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:344)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:198)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:137)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at com.estockmarket.query.domain.service.StockService.updateStock(StockService.java:105)
	at com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(KafkaStocksEventListener.java:36)
	at jdk.internal.reflect.GeneratedMethodAccessor301.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:56)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:347)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:92)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:53)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2322)
	... 11 common frames omitted
Caused by: com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at com.mongodb.client.internal.MongoCollectionImpl.executeSingleWriteRequest(MongoCollectionImpl.java:1017)
	at com.mongodb.client.internal.MongoCollectionImpl.executeInsertOne(MongoCollectionImpl.java:470)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:453)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:447)
	at org.springframework.data.mongodb.core.MongoTemplate.lambda$insertDocument$16(MongoTemplate.java:1464)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:553)
	... 56 common frames omitted
2022-06-24 14:01:18,091 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks com.estockmarket.query.domain.model.Stocks@270c98c3
2022-06-24 14:01:18,111 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Seeking to offset 4 for partition updateStock-0
2022-06-24 14:01:18,111 ERROR org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception; nested exception is org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:206)
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:112)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2359)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2228)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2142)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2024)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1706)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2371)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2342)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2303)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2217)
	... 9 common frames omitted
Caused by: org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.data.mongodb.core.MongoExceptionTranslator.translateExceptionIfPossible(MongoExceptionTranslator.java:106)
	at org.springframework.data.mongodb.core.MongoTemplate.potentiallyConvertRuntimeException(MongoTemplate.java:2899)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:555)
	at org.springframework.data.mongodb.core.MongoTemplate.insertDocument(MongoTemplate.java:1458)
	at org.springframework.data.mongodb.core.MongoTemplate.doInsert(MongoTemplate.java:1257)
	at org.springframework.data.mongodb.core.MongoTemplate.insert(MongoTemplate.java:1172)
	at org.springframework.data.mongodb.repository.support.SimpleMongoRepository.save(SimpleMongoRepository.java:91)
	at jdk.internal.reflect.GeneratedMethodAccessor267.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:289)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:137)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:121)
	at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:529)
	at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:285)
	at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:599)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:163)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:138)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:80)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at jdk.internal.reflect.GeneratedMethodAccessor266.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:344)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:198)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:137)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at com.estockmarket.query.domain.service.StockService.updateStock(StockService.java:105)
	at com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(KafkaStocksEventListener.java:36)
	at jdk.internal.reflect.GeneratedMethodAccessor301.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:56)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:347)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:92)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:53)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2322)
	... 11 common frames omitted
Caused by: com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at com.mongodb.client.internal.MongoCollectionImpl.executeSingleWriteRequest(MongoCollectionImpl.java:1017)
	at com.mongodb.client.internal.MongoCollectionImpl.executeInsertOne(MongoCollectionImpl.java:470)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:453)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:447)
	at org.springframework.data.mongodb.core.MongoTemplate.lambda$insertDocument$16(MongoTemplate.java:1464)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:553)
	... 56 common frames omitted
2022-06-24 14:01:18,622 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks com.estockmarket.query.domain.model.Stocks@1c6917f8
2022-06-24 14:01:18,649 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Seeking to offset 4 for partition updateStock-0
2022-06-24 14:01:18,649 ERROR org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception; nested exception is org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:206)
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:112)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2359)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2228)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2142)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2024)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1706)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2371)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2342)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2303)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2217)
	... 9 common frames omitted
Caused by: org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.data.mongodb.core.MongoExceptionTranslator.translateExceptionIfPossible(MongoExceptionTranslator.java:106)
	at org.springframework.data.mongodb.core.MongoTemplate.potentiallyConvertRuntimeException(MongoTemplate.java:2899)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:555)
	at org.springframework.data.mongodb.core.MongoTemplate.insertDocument(MongoTemplate.java:1458)
	at org.springframework.data.mongodb.core.MongoTemplate.doInsert(MongoTemplate.java:1257)
	at org.springframework.data.mongodb.core.MongoTemplate.insert(MongoTemplate.java:1172)
	at org.springframework.data.mongodb.repository.support.SimpleMongoRepository.save(SimpleMongoRepository.java:91)
	at jdk.internal.reflect.GeneratedMethodAccessor267.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:289)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:137)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:121)
	at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:529)
	at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:285)
	at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:599)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:163)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:138)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:80)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at jdk.internal.reflect.GeneratedMethodAccessor266.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:344)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:198)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:137)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at com.estockmarket.query.domain.service.StockService.updateStock(StockService.java:105)
	at com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(KafkaStocksEventListener.java:36)
	at jdk.internal.reflect.GeneratedMethodAccessor301.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:56)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:347)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:92)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:53)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2322)
	... 11 common frames omitted
Caused by: com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at com.mongodb.client.internal.MongoCollectionImpl.executeSingleWriteRequest(MongoCollectionImpl.java:1017)
	at com.mongodb.client.internal.MongoCollectionImpl.executeInsertOne(MongoCollectionImpl.java:470)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:453)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:447)
	at org.springframework.data.mongodb.core.MongoTemplate.lambda$insertDocument$16(MongoTemplate.java:1464)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:553)
	... 56 common frames omitted
2022-06-24 14:01:19,165 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks com.estockmarket.query.domain.model.Stocks@be617fd
2022-06-24 14:01:19,192 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Seeking to offset 4 for partition updateStock-0
2022-06-24 14:01:19,192 ERROR org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception; nested exception is org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:206)
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:112)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2359)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2228)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2142)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2024)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1706)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2371)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2342)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2303)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2217)
	... 9 common frames omitted
Caused by: org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.data.mongodb.core.MongoExceptionTranslator.translateExceptionIfPossible(MongoExceptionTranslator.java:106)
	at org.springframework.data.mongodb.core.MongoTemplate.potentiallyConvertRuntimeException(MongoTemplate.java:2899)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:555)
	at org.springframework.data.mongodb.core.MongoTemplate.insertDocument(MongoTemplate.java:1458)
	at org.springframework.data.mongodb.core.MongoTemplate.doInsert(MongoTemplate.java:1257)
	at org.springframework.data.mongodb.core.MongoTemplate.insert(MongoTemplate.java:1172)
	at org.springframework.data.mongodb.repository.support.SimpleMongoRepository.save(SimpleMongoRepository.java:91)
	at jdk.internal.reflect.GeneratedMethodAccessor267.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:289)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:137)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:121)
	at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:529)
	at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:285)
	at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:599)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:163)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:138)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:80)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at jdk.internal.reflect.GeneratedMethodAccessor266.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:344)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:198)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:137)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at com.estockmarket.query.domain.service.StockService.updateStock(StockService.java:105)
	at com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(KafkaStocksEventListener.java:36)
	at jdk.internal.reflect.GeneratedMethodAccessor301.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:56)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:347)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:92)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:53)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2322)
	... 11 common frames omitted
Caused by: com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at com.mongodb.client.internal.MongoCollectionImpl.executeSingleWriteRequest(MongoCollectionImpl.java:1017)
	at com.mongodb.client.internal.MongoCollectionImpl.executeInsertOne(MongoCollectionImpl.java:470)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:453)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:447)
	at org.springframework.data.mongodb.core.MongoTemplate.lambda$insertDocument$16(MongoTemplate.java:1464)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:553)
	... 56 common frames omitted
2022-06-24 14:01:19,707 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks com.estockmarket.query.domain.model.Stocks@5defd1b3
2022-06-24 14:01:19,736 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Seeking to offset 4 for partition updateStock-0
2022-06-24 14:01:19,736 ERROR org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception; nested exception is org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:206)
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:112)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2359)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2228)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2142)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2024)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1706)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2371)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2342)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2303)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2217)
	... 9 common frames omitted
Caused by: org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.data.mongodb.core.MongoExceptionTranslator.translateExceptionIfPossible(MongoExceptionTranslator.java:106)
	at org.springframework.data.mongodb.core.MongoTemplate.potentiallyConvertRuntimeException(MongoTemplate.java:2899)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:555)
	at org.springframework.data.mongodb.core.MongoTemplate.insertDocument(MongoTemplate.java:1458)
	at org.springframework.data.mongodb.core.MongoTemplate.doInsert(MongoTemplate.java:1257)
	at org.springframework.data.mongodb.core.MongoTemplate.insert(MongoTemplate.java:1172)
	at org.springframework.data.mongodb.repository.support.SimpleMongoRepository.save(SimpleMongoRepository.java:91)
	at jdk.internal.reflect.GeneratedMethodAccessor267.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:289)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:137)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:121)
	at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:529)
	at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:285)
	at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:599)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:163)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:138)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:80)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at jdk.internal.reflect.GeneratedMethodAccessor266.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:344)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:198)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:137)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at com.estockmarket.query.domain.service.StockService.updateStock(StockService.java:105)
	at com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(KafkaStocksEventListener.java:36)
	at jdk.internal.reflect.GeneratedMethodAccessor301.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:56)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:347)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:92)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:53)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2322)
	... 11 common frames omitted
Caused by: com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at com.mongodb.client.internal.MongoCollectionImpl.executeSingleWriteRequest(MongoCollectionImpl.java:1017)
	at com.mongodb.client.internal.MongoCollectionImpl.executeInsertOne(MongoCollectionImpl.java:470)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:453)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:447)
	at org.springframework.data.mongodb.core.MongoTemplate.lambda$insertDocument$16(MongoTemplate.java:1464)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:553)
	... 56 common frames omitted
2022-06-24 14:01:20,255 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks com.estockmarket.query.domain.model.Stocks@1e66904d
2022-06-24 14:01:20,259 ERROR org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Backoff FixedBackOff{interval=0, currentAttempts=10, maxAttempts=9} exhausted for updateStock-0@4
org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2371)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2342)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2303)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2217)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2142)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2024)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1706)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.data.mongodb.core.MongoExceptionTranslator.translateExceptionIfPossible(MongoExceptionTranslator.java:106)
	at org.springframework.data.mongodb.core.MongoTemplate.potentiallyConvertRuntimeException(MongoTemplate.java:2899)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:555)
	at org.springframework.data.mongodb.core.MongoTemplate.insertDocument(MongoTemplate.java:1458)
	at org.springframework.data.mongodb.core.MongoTemplate.doInsert(MongoTemplate.java:1257)
	at org.springframework.data.mongodb.core.MongoTemplate.insert(MongoTemplate.java:1172)
	at org.springframework.data.mongodb.repository.support.SimpleMongoRepository.save(SimpleMongoRepository.java:91)
	at jdk.internal.reflect.GeneratedMethodAccessor267.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:289)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:137)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:121)
	at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:529)
	at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:285)
	at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:599)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:163)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:138)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:80)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at jdk.internal.reflect.GeneratedMethodAccessor266.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:344)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:198)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:137)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at com.estockmarket.query.domain.service.StockService.updateStock(StockService.java:105)
	at com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(KafkaStocksEventListener.java:36)
	at jdk.internal.reflect.GeneratedMethodAccessor301.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:56)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:347)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:92)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:53)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2322)
	... 11 common frames omitted
Caused by: com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at com.mongodb.client.internal.MongoCollectionImpl.executeSingleWriteRequest(MongoCollectionImpl.java:1017)
	at com.mongodb.client.internal.MongoCollectionImpl.executeInsertOne(MongoCollectionImpl.java:470)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:453)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:447)
	at org.springframework.data.mongodb.core.MongoTemplate.lambda$insertDocument$16(MongoTemplate.java:1464)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:553)
	... 56 common frames omitted
2022-06-24 14:02:07,914 INFO com.estockmarket.query.application.controller.StockController [http-nio-6093-exec-8] fetch min,max and average stock price based on company code,date C-008
2022-06-24 14:02:07,914 INFO com.estockmarket.query.application.controller.StockController [http-nio-6093-exec-1] fetch stocks based on code,date C-008
2022-06-24 14:02:07,915 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-8] fetch stockaggregate based on code,date C-008
2022-06-24 14:02:07,915 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-1] fetch company stocks based on code,date C-008
2022-06-24 14:02:07,915 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-8] fetch stock query based on code C-008
2022-06-24 14:02:07,915 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-1] fetch stock query based on code C-008
2022-06-24 14:02:17,353 INFO com.estockmarket.query.application.controller.StockController [http-nio-6093-exec-10] fetch min,max and average stock price based on company code,date C-008
2022-06-24 14:02:17,353 INFO com.estockmarket.query.application.controller.StockController [http-nio-6093-exec-2] fetch stocks based on code,date C-008
2022-06-24 14:02:17,355 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-10] fetch stockaggregate based on code,date C-008
2022-06-24 14:02:17,355 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-2] fetch company stocks based on code,date C-008
2022-06-24 14:02:17,355 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-10] fetch stock query based on code C-008
2022-06-24 14:02:17,356 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-2] fetch stock query based on code C-008
2022-06-24 14:05:27,176 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-32, groupId=group_id] Attempt to heartbeat with Generation{generationId=74, memberId='consumer-group_id-32-ce77434d-a973-4cec-bd3c-f6f33a8765f9', protocol='range'} and group instance id Optional.empty failed due to UNKNOWN_MEMBER_ID, resetting generation
2022-06-24 14:05:27,176 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-28, groupId=group_id] Attempt to heartbeat with Generation{generationId=74, memberId='consumer-group_id-28-7b67cce9-b390-4abc-ac08-c81adc5085fd', protocol='range'} and group instance id Optional.empty failed due to UNKNOWN_MEMBER_ID, resetting generation
2022-06-24 14:05:27,176 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-28, groupId=group_id] Giving away all assigned partitions as lost since generation has been reset,indicating that consumer is no longer part of the group
2022-06-24 14:05:27,176 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-28, groupId=group_id] Lost previously assigned partitions createSector-0
2022-06-24 14:05:27,176 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-32, groupId=group_id] Giving away all assigned partitions as lost since generation has been reset,indicating that consumer is no longer part of the group
2022-06-24 14:05:27,177 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] group_id: partitions lost: [createSector-0]
2022-06-24 14:05:27,177 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-32, groupId=group_id] Lost previously assigned partitions createCompany-0
2022-06-24 14:05:27,177 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] group_id: partitions revoked: [createSector-0]
2022-06-24 14:05:27,177 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] group_id: partitions lost: [createCompany-0]
2022-06-24 14:05:27,177 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-28, groupId=group_id] (Re-)joining group
2022-06-24 14:05:27,177 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] group_id: partitions revoked: [createCompany-0]
2022-06-24 14:05:27,177 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-32, groupId=group_id] (Re-)joining group
2022-06-24 14:05:27,181 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-32, groupId=group_id] (Re-)joining group
2022-06-24 14:05:27,181 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-28, groupId=group_id] (Re-)joining group
2022-06-24 14:05:27,185 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-28, groupId=group_id] Successfully joined group with generation Generation{generationId=82, memberId='consumer-group_id-28-c909bfcd-8df9-4a5e-978c-5f9b2a336ab6', protocol='range'}
2022-06-24 14:05:27,185 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-32, groupId=group_id] Successfully joined group with generation Generation{generationId=82, memberId='consumer-group_id-32-437b1c86-4893-46ac-ba15-c343de5f80c8', protocol='range'}
2022-06-24 14:05:27,251 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-25, groupId=group_id] Attempt to heartbeat with Generation{generationId=74, memberId='consumer-group_id-25-5e005694-2858-42fd-86a4-ac53305779e8', protocol='range'} and group instance id Optional.empty failed due to UNKNOWN_MEMBER_ID, resetting generation
2022-06-24 14:05:27,251 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-26, groupId=group_id] Attempt to heartbeat with Generation{generationId=74, memberId='consumer-group_id-26-0904ec6a-7069-4ec8-b3ec-caad88f61f23', protocol='range'} and group instance id Optional.empty failed due to UNKNOWN_MEMBER_ID, resetting generation
2022-06-24 14:05:27,251 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-29, groupId=group_id] Attempt to heartbeat with Generation{generationId=74, memberId='consumer-group_id-29-01bbfa1a-d699-4c4e-b7ac-e478e6c32ee8', protocol='range'} and group instance id Optional.empty failed due to UNKNOWN_MEMBER_ID, resetting generation
2022-06-24 14:05:27,251 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-27, groupId=group_id] Attempt to heartbeat with Generation{generationId=74, memberId='consumer-group_id-27-ea30e505-29b8-4e6e-a792-7a468bc9cbd3', protocol='range'} and group instance id Optional.empty failed due to UNKNOWN_MEMBER_ID, resetting generation
2022-06-24 14:05:27,251 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-31, groupId=group_id] Attempt to heartbeat with Generation{generationId=74, memberId='consumer-group_id-31-90215034-e069-45c5-9a13-2ef057d6d7f1', protocol='range'} and group instance id Optional.empty failed due to UNKNOWN_MEMBER_ID, resetting generation
2022-06-24 14:05:27,251 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-26, groupId=group_id] Giving away all assigned partitions as lost since generation has been reset,indicating that consumer is no longer part of the group
2022-06-24 14:05:27,251 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-25, groupId=group_id] Giving away all assigned partitions as lost since generation has been reset,indicating that consumer is no longer part of the group
2022-06-24 14:05:27,252 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-25, groupId=group_id] Lost previously assigned partitions createStock-0
2022-06-24 14:05:27,252 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-27, groupId=group_id] Giving away all assigned partitions as lost since generation has been reset,indicating that consumer is no longer part of the group
2022-06-24 14:05:27,252 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-27, groupId=group_id] Lost previously assigned partitions removeSector-0
2022-06-24 14:05:27,252 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] group_id: partitions lost: [createStock-0]
2022-06-24 14:05:27,252 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-31, groupId=group_id] Giving away all assigned partitions as lost since generation has been reset,indicating that consumer is no longer part of the group
2022-06-24 14:05:27,252 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] group_id: partitions lost: [removeSector-0]
2022-06-24 14:05:27,252 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-26, groupId=group_id] Lost previously assigned partitions createUser-0
2022-06-24 14:05:27,252 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] group_id: partitions revoked: [removeSector-0]
2022-06-24 14:05:27,252 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-31, groupId=group_id] Lost previously assigned partitions removeCompany-0
2022-06-24 14:05:27,252 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] group_id: partitions lost: [createUser-0]
2022-06-24 14:05:27,252 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-27, groupId=group_id] (Re-)joining group
2022-06-24 14:05:27,251 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-29, groupId=group_id] Giving away all assigned partitions as lost since generation has been reset,indicating that consumer is no longer part of the group
2022-06-24 14:05:27,252 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] group_id: partitions revoked: [createStock-0]
2022-06-24 14:05:27,252 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-29, groupId=group_id] Lost previously assigned partitions removeStock-0
2022-06-24 14:05:27,252 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] group_id: partitions lost: [removeStock-0]
2022-06-24 14:05:27,252 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-25, groupId=group_id] (Re-)joining group
2022-06-24 14:05:27,252 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] group_id: partitions revoked: [removeStock-0]
2022-06-24 14:05:27,252 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-29, groupId=group_id] (Re-)joining group
2022-06-24 14:05:27,252 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] group_id: partitions revoked: [createUser-0]
2022-06-24 14:05:27,252 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-26, groupId=group_id] (Re-)joining group
2022-06-24 14:05:27,252 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] group_id: partitions lost: [removeCompany-0]
2022-06-24 14:05:27,253 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] group_id: partitions revoked: [removeCompany-0]
2022-06-24 14:05:27,253 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-31, groupId=group_id] (Re-)joining group
2022-06-24 14:05:27,254 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-29, groupId=group_id] (Re-)joining group
2022-06-24 14:05:27,254 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-27, groupId=group_id] (Re-)joining group
2022-06-24 14:05:27,255 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-25, groupId=group_id] (Re-)joining group
2022-06-24 14:05:27,255 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-28, groupId=group_id] SyncGroup failed: The group began another rebalance. Need to re-join the group. Sent generation was Generation{generationId=82, memberId='consumer-group_id-28-c909bfcd-8df9-4a5e-978c-5f9b2a336ab6', protocol='range'}
2022-06-24 14:05:27,255 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-28, groupId=group_id] Rebalance failed.
org.apache.kafka.common.errors.RebalanceInProgressException: The group is rebalancing, so a rejoin is needed.
2022-06-24 14:05:27,256 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-28, groupId=group_id] (Re-)joining group
2022-06-24 14:05:27,257 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-26, groupId=group_id] (Re-)joining group
2022-06-24 14:05:27,257 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-31, groupId=group_id] (Re-)joining group
2022-06-24 14:05:27,328 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-32, groupId=group_id] Finished assignment for group at generation 82: {consumer-group_id-28-c909bfcd-8df9-4a5e-978c-5f9b2a336ab6=Assignment(partitions=[createSector-0]), consumer-group_id-32-437b1c86-4893-46ac-ba15-c343de5f80c8=Assignment(partitions=[createCompany-0])}
2022-06-24 14:05:27,329 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-32, groupId=group_id] SyncGroup failed: The group began another rebalance. Need to re-join the group. Sent generation was Generation{generationId=82, memberId='consumer-group_id-32-437b1c86-4893-46ac-ba15-c343de5f80c8', protocol='range'}
2022-06-24 14:05:27,329 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-32, groupId=group_id] Rebalance failed.
org.apache.kafka.common.errors.RebalanceInProgressException: The group is rebalancing, so a rejoin is needed.
2022-06-24 14:05:27,329 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-32, groupId=group_id] (Re-)joining group
2022-06-24 14:05:27,333 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-31, groupId=group_id] Successfully joined group with generation Generation{generationId=83, memberId='consumer-group_id-31-2f5de27f-62cb-47b3-83a2-71b547b99584', protocol='range'}
2022-06-24 14:05:27,333 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-25, groupId=group_id] Successfully joined group with generation Generation{generationId=83, memberId='consumer-group_id-25-03a419c5-2ff2-4345-a103-98407f977d0b', protocol='range'}
2022-06-24 14:05:27,333 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-29, groupId=group_id] Successfully joined group with generation Generation{generationId=83, memberId='consumer-group_id-29-260a5215-44c3-49dc-810d-60f26bfb9c7f', protocol='range'}
2022-06-24 14:05:27,333 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-28, groupId=group_id] Successfully joined group with generation Generation{generationId=83, memberId='consumer-group_id-28-c909bfcd-8df9-4a5e-978c-5f9b2a336ab6', protocol='range'}
2022-06-24 14:05:27,333 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-27, groupId=group_id] Successfully joined group with generation Generation{generationId=83, memberId='consumer-group_id-27-4c2528ec-168e-42a1-b2bb-026799880307', protocol='range'}
2022-06-24 14:05:27,333 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-26, groupId=group_id] Successfully joined group with generation Generation{generationId=83, memberId='consumer-group_id-26-5fdcbd7e-a77d-4673-b11c-267938f2b5c8', protocol='range'}
2022-06-24 14:05:27,334 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-32, groupId=group_id] Successfully joined group with generation Generation{generationId=83, memberId='consumer-group_id-32-437b1c86-4893-46ac-ba15-c343de5f80c8', protocol='range'}
2022-06-24 14:05:27,335 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-32, groupId=group_id] Finished assignment for group at generation 83: {consumer-group_id-28-c909bfcd-8df9-4a5e-978c-5f9b2a336ab6=Assignment(partitions=[createSector-0]), consumer-group_id-32-437b1c86-4893-46ac-ba15-c343de5f80c8=Assignment(partitions=[createCompany-0]), consumer-group_id-31-2f5de27f-62cb-47b3-83a2-71b547b99584=Assignment(partitions=[removeCompany-0]), consumer-group_id-27-4c2528ec-168e-42a1-b2bb-026799880307=Assignment(partitions=[removeSector-0]), consumer-group_id-26-5fdcbd7e-a77d-4673-b11c-267938f2b5c8=Assignment(partitions=[createUser-0]), consumer-group_id-29-260a5215-44c3-49dc-810d-60f26bfb9c7f=Assignment(partitions=[removeStock-0]), consumer-group_id-25-03a419c5-2ff2-4345-a103-98407f977d0b=Assignment(partitions=[createStock-0])}
2022-06-24 14:05:27,339 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-31, groupId=group_id] Successfully synced group in generation Generation{generationId=83, memberId='consumer-group_id-31-2f5de27f-62cb-47b3-83a2-71b547b99584', protocol='range'}
2022-06-24 14:05:27,339 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-28, groupId=group_id] Successfully synced group in generation Generation{generationId=83, memberId='consumer-group_id-28-c909bfcd-8df9-4a5e-978c-5f9b2a336ab6', protocol='range'}
2022-06-24 14:05:27,339 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-29, groupId=group_id] Successfully synced group in generation Generation{generationId=83, memberId='consumer-group_id-29-260a5215-44c3-49dc-810d-60f26bfb9c7f', protocol='range'}
2022-06-24 14:05:27,339 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-27, groupId=group_id] Successfully synced group in generation Generation{generationId=83, memberId='consumer-group_id-27-4c2528ec-168e-42a1-b2bb-026799880307', protocol='range'}
2022-06-24 14:05:27,339 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-26, groupId=group_id] Successfully synced group in generation Generation{generationId=83, memberId='consumer-group_id-26-5fdcbd7e-a77d-4673-b11c-267938f2b5c8', protocol='range'}
2022-06-24 14:05:27,339 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-31, groupId=group_id] Notifying assignor about the new Assignment(partitions=[removeCompany-0])
2022-06-24 14:05:27,339 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-32, groupId=group_id] Successfully synced group in generation Generation{generationId=83, memberId='consumer-group_id-32-437b1c86-4893-46ac-ba15-c343de5f80c8', protocol='range'}
2022-06-24 14:05:27,339 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-25, groupId=group_id] Successfully synced group in generation Generation{generationId=83, memberId='consumer-group_id-25-03a419c5-2ff2-4345-a103-98407f977d0b', protocol='range'}
2022-06-24 14:05:27,339 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-28, groupId=group_id] Notifying assignor about the new Assignment(partitions=[createSector-0])
2022-06-24 14:05:27,340 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-27, groupId=group_id] Notifying assignor about the new Assignment(partitions=[removeSector-0])
2022-06-24 14:05:27,340 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-32, groupId=group_id] Notifying assignor about the new Assignment(partitions=[createCompany-0])
2022-06-24 14:05:27,339 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-29, groupId=group_id] Notifying assignor about the new Assignment(partitions=[removeStock-0])
2022-06-24 14:05:27,340 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-32, groupId=group_id] Adding newly assigned partitions: createCompany-0
2022-06-24 14:05:27,340 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-29, groupId=group_id] Adding newly assigned partitions: removeStock-0
2022-06-24 14:05:27,340 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-26, groupId=group_id] Notifying assignor about the new Assignment(partitions=[createUser-0])
2022-06-24 14:05:27,340 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-31, groupId=group_id] Adding newly assigned partitions: removeCompany-0
2022-06-24 14:05:27,340 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-26, groupId=group_id] Adding newly assigned partitions: createUser-0
2022-06-24 14:05:27,340 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-28, groupId=group_id] Adding newly assigned partitions: createSector-0
2022-06-24 14:05:27,340 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-27, groupId=group_id] Adding newly assigned partitions: removeSector-0
2022-06-24 14:05:27,340 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-25, groupId=group_id] Notifying assignor about the new Assignment(partitions=[createStock-0])
2022-06-24 14:05:27,340 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-25, groupId=group_id] Adding newly assigned partitions: createStock-0
2022-06-24 14:05:27,342 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-29, groupId=group_id] Setting offset for partition removeStock-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:05:27,343 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] group_id: partitions assigned: [removeStock-0]
2022-06-24 14:05:27,343 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-31, groupId=group_id] Setting offset for partition removeCompany-0 to the committed offset FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:05:27,343 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-25, groupId=group_id] Setting offset for partition createStock-0 to the committed offset FetchPosition{offset=3, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:05:27,343 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] group_id: partitions assigned: [removeCompany-0]
2022-06-24 14:05:27,343 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-27, groupId=group_id] Setting offset for partition removeSector-0 to the committed offset FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:05:27,343 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] group_id: partitions assigned: [createStock-0]
2022-06-24 14:05:27,343 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] group_id: partitions assigned: [removeSector-0]
2022-06-24 14:05:27,343 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-26, groupId=group_id] Setting offset for partition createUser-0 to the committed offset FetchPosition{offset=1, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:05:27,344 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] group_id: partitions assigned: [createUser-0]
2022-06-24 14:05:27,343 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-32, groupId=group_id] Setting offset for partition createCompany-0 to the committed offset FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:05:27,344 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] group_id: partitions assigned: [createCompany-0]
2022-06-24 14:05:27,344 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-28, groupId=group_id] Setting offset for partition createSector-0 to the committed offset FetchPosition{offset=1, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:05:27,344 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] group_id: partitions assigned: [createSector-0]
2022-06-24 14:05:28,929 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Attempt to heartbeat with Generation{generationId=80, memberId='consumer-group_id-30-4c8417ac-6593-460e-b4ea-8cc61629e4e6', protocol='range'} and group instance id Optional.empty failed due to UNKNOWN_MEMBER_ID, resetting generation
2022-06-24 14:05:28,930 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Giving away all assigned partitions as lost since generation has been reset,indicating that consumer is no longer part of the group
2022-06-24 14:05:28,930 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Lost previously assigned partitions updateStock-0
2022-06-24 14:05:28,930 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] group_id: partitions lost: [updateStock-0]
2022-06-24 14:05:28,930 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] group_id: partitions revoked: [updateStock-0]
2022-06-24 14:05:28,930 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] (Re-)joining group
2022-06-24 14:05:28,932 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] (Re-)joining group
2022-06-24 14:05:30,340 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-26, groupId=group_id] Attempt to heartbeat failed since group is rebalancing
2022-06-24 14:05:30,340 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-25, groupId=group_id] Attempt to heartbeat failed since group is rebalancing
2022-06-24 14:05:30,340 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-29, groupId=group_id] Attempt to heartbeat failed since group is rebalancing
2022-06-24 14:05:30,341 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-26, groupId=group_id] Revoke previously assigned partitions createUser-0
2022-06-24 14:05:30,341 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-25, groupId=group_id] Revoke previously assigned partitions createStock-0
2022-06-24 14:05:30,341 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-27, groupId=group_id] Attempt to heartbeat failed since group is rebalancing
2022-06-24 14:05:30,341 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] group_id: partitions revoked: [createUser-0]
2022-06-24 14:05:30,341 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] group_id: partitions revoked: [createStock-0]
2022-06-24 14:05:30,340 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-31, groupId=group_id] Attempt to heartbeat failed since group is rebalancing
2022-06-24 14:05:30,341 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-27, groupId=group_id] Revoke previously assigned partitions removeSector-0
2022-06-24 14:05:30,341 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-25, groupId=group_id] (Re-)joining group
2022-06-24 14:05:30,341 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-31, groupId=group_id] Revoke previously assigned partitions removeCompany-0
2022-06-24 14:05:30,340 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-28, groupId=group_id] Attempt to heartbeat failed since group is rebalancing
2022-06-24 14:05:30,341 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] group_id: partitions revoked: [removeCompany-0]
2022-06-24 14:05:30,341 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-26, groupId=group_id] (Re-)joining group
2022-06-24 14:05:30,341 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-28, groupId=group_id] Revoke previously assigned partitions createSector-0
2022-06-24 14:05:30,341 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-29, groupId=group_id] Revoke previously assigned partitions removeStock-0
2022-06-24 14:05:30,341 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] group_id: partitions revoked: [removeSector-0]
2022-06-24 14:05:30,342 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-31, groupId=group_id] (Re-)joining group
2022-06-24 14:05:30,342 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-32, groupId=group_id] Attempt to heartbeat failed since group is rebalancing
2022-06-24 14:05:30,342 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] group_id: partitions revoked: [removeStock-0]
2022-06-24 14:05:30,342 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-32, groupId=group_id] Revoke previously assigned partitions createCompany-0
2022-06-24 14:05:30,342 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] group_id: partitions revoked: [createCompany-0]
2022-06-24 14:05:30,342 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-29, groupId=group_id] (Re-)joining group
2022-06-24 14:05:30,342 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-32, groupId=group_id] (Re-)joining group
2022-06-24 14:05:30,342 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] group_id: partitions revoked: [createSector-0]
2022-06-24 14:05:30,342 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-28, groupId=group_id] (Re-)joining group
2022-06-24 14:05:30,342 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-27, groupId=group_id] (Re-)joining group
2022-06-24 14:05:30,345 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-29, groupId=group_id] Successfully joined group with generation Generation{generationId=84, memberId='consumer-group_id-29-260a5215-44c3-49dc-810d-60f26bfb9c7f', protocol='range'}
2022-06-24 14:05:30,345 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-26, groupId=group_id] Successfully joined group with generation Generation{generationId=84, memberId='consumer-group_id-26-5fdcbd7e-a77d-4673-b11c-267938f2b5c8', protocol='range'}
2022-06-24 14:05:30,345 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-31, groupId=group_id] Successfully joined group with generation Generation{generationId=84, memberId='consumer-group_id-31-2f5de27f-62cb-47b3-83a2-71b547b99584', protocol='range'}
2022-06-24 14:05:30,346 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-25, groupId=group_id] Successfully joined group with generation Generation{generationId=84, memberId='consumer-group_id-25-03a419c5-2ff2-4345-a103-98407f977d0b', protocol='range'}
2022-06-24 14:05:30,346 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-27, groupId=group_id] Successfully joined group with generation Generation{generationId=84, memberId='consumer-group_id-27-4c2528ec-168e-42a1-b2bb-026799880307', protocol='range'}
2022-06-24 14:05:30,346 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-28, groupId=group_id] Successfully joined group with generation Generation{generationId=84, memberId='consumer-group_id-28-c909bfcd-8df9-4a5e-978c-5f9b2a336ab6', protocol='range'}
2022-06-24 14:05:30,347 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Successfully joined group with generation Generation{generationId=84, memberId='consumer-group_id-30-64390239-52bd-450b-a827-7fcde96eca58', protocol='range'}
2022-06-24 14:05:30,349 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-32, groupId=group_id] Successfully joined group with generation Generation{generationId=84, memberId='consumer-group_id-32-437b1c86-4893-46ac-ba15-c343de5f80c8', protocol='range'}
2022-06-24 14:05:30,401 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-32, groupId=group_id] Finished assignment for group at generation 84: {consumer-group_id-28-c909bfcd-8df9-4a5e-978c-5f9b2a336ab6=Assignment(partitions=[createSector-0]), consumer-group_id-32-437b1c86-4893-46ac-ba15-c343de5f80c8=Assignment(partitions=[createCompany-0]), consumer-group_id-31-2f5de27f-62cb-47b3-83a2-71b547b99584=Assignment(partitions=[removeCompany-0]), consumer-group_id-27-4c2528ec-168e-42a1-b2bb-026799880307=Assignment(partitions=[removeSector-0]), consumer-group_id-26-5fdcbd7e-a77d-4673-b11c-267938f2b5c8=Assignment(partitions=[createUser-0]), consumer-group_id-29-260a5215-44c3-49dc-810d-60f26bfb9c7f=Assignment(partitions=[removeStock-0]), consumer-group_id-25-03a419c5-2ff2-4345-a103-98407f977d0b=Assignment(partitions=[createStock-0]), consumer-group_id-30-64390239-52bd-450b-a827-7fcde96eca58=Assignment(partitions=[updateStock-0])}
2022-06-24 14:05:30,404 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-29, groupId=group_id] Successfully synced group in generation Generation{generationId=84, memberId='consumer-group_id-29-260a5215-44c3-49dc-810d-60f26bfb9c7f', protocol='range'}
2022-06-24 14:05:30,404 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Successfully synced group in generation Generation{generationId=84, memberId='consumer-group_id-30-64390239-52bd-450b-a827-7fcde96eca58', protocol='range'}
2022-06-24 14:05:30,404 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-27, groupId=group_id] Successfully synced group in generation Generation{generationId=84, memberId='consumer-group_id-27-4c2528ec-168e-42a1-b2bb-026799880307', protocol='range'}
2022-06-24 14:05:30,404 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-29, groupId=group_id] Notifying assignor about the new Assignment(partitions=[removeStock-0])
2022-06-24 14:05:30,405 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-28, groupId=group_id] Successfully synced group in generation Generation{generationId=84, memberId='consumer-group_id-28-c909bfcd-8df9-4a5e-978c-5f9b2a336ab6', protocol='range'}
2022-06-24 14:05:30,405 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-29, groupId=group_id] Adding newly assigned partitions: removeStock-0
2022-06-24 14:05:30,405 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-27, groupId=group_id] Notifying assignor about the new Assignment(partitions=[removeSector-0])
2022-06-24 14:05:30,404 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-31, groupId=group_id] Successfully synced group in generation Generation{generationId=84, memberId='consumer-group_id-31-2f5de27f-62cb-47b3-83a2-71b547b99584', protocol='range'}
2022-06-24 14:05:30,405 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-27, groupId=group_id] Adding newly assigned partitions: removeSector-0
2022-06-24 14:05:30,405 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-28, groupId=group_id] Notifying assignor about the new Assignment(partitions=[createSector-0])
2022-06-24 14:05:30,405 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-31, groupId=group_id] Notifying assignor about the new Assignment(partitions=[removeCompany-0])
2022-06-24 14:05:30,404 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-32, groupId=group_id] Successfully synced group in generation Generation{generationId=84, memberId='consumer-group_id-32-437b1c86-4893-46ac-ba15-c343de5f80c8', protocol='range'}
2022-06-24 14:05:30,405 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-28, groupId=group_id] Adding newly assigned partitions: createSector-0
2022-06-24 14:05:30,405 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Notifying assignor about the new Assignment(partitions=[updateStock-0])
2022-06-24 14:05:30,405 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-26, groupId=group_id] Successfully synced group in generation Generation{generationId=84, memberId='consumer-group_id-26-5fdcbd7e-a77d-4673-b11c-267938f2b5c8', protocol='range'}
2022-06-24 14:05:30,405 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-31, groupId=group_id] Adding newly assigned partitions: removeCompany-0
2022-06-24 14:05:30,405 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Adding newly assigned partitions: updateStock-0
2022-06-24 14:05:30,405 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-32, groupId=group_id] Notifying assignor about the new Assignment(partitions=[createCompany-0])
2022-06-24 14:05:30,405 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-32, groupId=group_id] Adding newly assigned partitions: createCompany-0
2022-06-24 14:05:30,405 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-26, groupId=group_id] Notifying assignor about the new Assignment(partitions=[createUser-0])
2022-06-24 14:05:30,405 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-26, groupId=group_id] Adding newly assigned partitions: createUser-0
2022-06-24 14:05:30,406 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-25, groupId=group_id] Successfully synced group in generation Generation{generationId=84, memberId='consumer-group_id-25-03a419c5-2ff2-4345-a103-98407f977d0b', protocol='range'}
2022-06-24 14:05:30,406 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-25, groupId=group_id] Notifying assignor about the new Assignment(partitions=[createStock-0])
2022-06-24 14:05:30,406 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-25, groupId=group_id] Adding newly assigned partitions: createStock-0
2022-06-24 14:05:30,407 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-29, groupId=group_id] Setting offset for partition removeStock-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:05:30,407 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-31, groupId=group_id] Setting offset for partition removeCompany-0 to the committed offset FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:05:30,407 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] group_id: partitions assigned: [removeStock-0]
2022-06-24 14:05:30,408 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] group_id: partitions assigned: [removeCompany-0]
2022-06-24 14:05:30,413 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-32, groupId=group_id] Setting offset for partition createCompany-0 to the committed offset FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:05:30,407 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-26, groupId=group_id] Setting offset for partition createUser-0 to the committed offset FetchPosition{offset=1, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:05:30,414 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] group_id: partitions assigned: [createCompany-0]
2022-06-24 14:05:30,414 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-25, groupId=group_id] Setting offset for partition createStock-0 to the committed offset FetchPosition{offset=3, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:05:30,414 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Setting offset for partition updateStock-0 to the committed offset FetchPosition{offset=5, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:05:30,414 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-27, groupId=group_id] Setting offset for partition removeSector-0 to the committed offset FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:05:30,414 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-28, groupId=group_id] Setting offset for partition createSector-0 to the committed offset FetchPosition{offset=1, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:05:30,414 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] group_id: partitions assigned: [removeSector-0]
2022-06-24 14:05:30,415 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] group_id: partitions assigned: [createSector-0]
2022-06-24 14:05:30,414 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] group_id: partitions assigned: [createUser-0]
2022-06-24 14:05:30,414 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] group_id: partitions assigned: [createStock-0]
2022-06-24 14:05:30,414 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] group_id: partitions assigned: [updateStock-0]
2022-06-24 14:05:31,324 INFO com.netflix.discovery.shared.resolver.aws.ConfigClusterResolver [AsyncResolver-bootstrap-executor-0] Resolving eureka endpoints via configuration
2022-06-24 14:05:53,015 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks com.estockmarket.query.domain.model.Stocks@52c7183a
2022-06-24 14:05:53,035 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Seeking to offset 5 for partition updateStock-0
2022-06-24 14:05:53,035 ERROR org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception; nested exception is org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:206)
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:112)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2359)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2228)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2142)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2024)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1706)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2371)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2342)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2303)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2217)
	... 9 common frames omitted
Caused by: org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.data.mongodb.core.MongoExceptionTranslator.translateExceptionIfPossible(MongoExceptionTranslator.java:106)
	at org.springframework.data.mongodb.core.MongoTemplate.potentiallyConvertRuntimeException(MongoTemplate.java:2899)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:555)
	at org.springframework.data.mongodb.core.MongoTemplate.insertDocument(MongoTemplate.java:1458)
	at org.springframework.data.mongodb.core.MongoTemplate.doInsert(MongoTemplate.java:1257)
	at org.springframework.data.mongodb.core.MongoTemplate.insert(MongoTemplate.java:1172)
	at org.springframework.data.mongodb.repository.support.SimpleMongoRepository.save(SimpleMongoRepository.java:91)
	at jdk.internal.reflect.GeneratedMethodAccessor267.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:289)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:137)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:121)
	at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:529)
	at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:285)
	at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:599)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:163)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:138)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:80)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at jdk.internal.reflect.GeneratedMethodAccessor266.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:344)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:198)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:137)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at com.estockmarket.query.domain.service.StockService.updateStock(StockService.java:105)
	at com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(KafkaStocksEventListener.java:36)
	at jdk.internal.reflect.GeneratedMethodAccessor301.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:56)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:347)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:92)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:53)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2322)
	... 11 common frames omitted
Caused by: com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at com.mongodb.client.internal.MongoCollectionImpl.executeSingleWriteRequest(MongoCollectionImpl.java:1017)
	at com.mongodb.client.internal.MongoCollectionImpl.executeInsertOne(MongoCollectionImpl.java:470)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:453)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:447)
	at org.springframework.data.mongodb.core.MongoTemplate.lambda$insertDocument$16(MongoTemplate.java:1464)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:553)
	... 56 common frames omitted
2022-06-24 14:05:53,552 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks com.estockmarket.query.domain.model.Stocks@563f4415
2022-06-24 14:05:53,580 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Seeking to offset 5 for partition updateStock-0
2022-06-24 14:05:53,580 ERROR org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception; nested exception is org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:206)
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:112)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2359)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2228)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2142)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2024)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1706)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2371)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2342)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2303)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2217)
	... 9 common frames omitted
Caused by: org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.data.mongodb.core.MongoExceptionTranslator.translateExceptionIfPossible(MongoExceptionTranslator.java:106)
	at org.springframework.data.mongodb.core.MongoTemplate.potentiallyConvertRuntimeException(MongoTemplate.java:2899)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:555)
	at org.springframework.data.mongodb.core.MongoTemplate.insertDocument(MongoTemplate.java:1458)
	at org.springframework.data.mongodb.core.MongoTemplate.doInsert(MongoTemplate.java:1257)
	at org.springframework.data.mongodb.core.MongoTemplate.insert(MongoTemplate.java:1172)
	at org.springframework.data.mongodb.repository.support.SimpleMongoRepository.save(SimpleMongoRepository.java:91)
	at jdk.internal.reflect.GeneratedMethodAccessor267.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:289)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:137)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:121)
	at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:529)
	at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:285)
	at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:599)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:163)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:138)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:80)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at jdk.internal.reflect.GeneratedMethodAccessor266.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:344)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:198)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:137)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at com.estockmarket.query.domain.service.StockService.updateStock(StockService.java:105)
	at com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(KafkaStocksEventListener.java:36)
	at jdk.internal.reflect.GeneratedMethodAccessor301.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:56)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:347)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:92)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:53)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2322)
	... 11 common frames omitted
Caused by: com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at com.mongodb.client.internal.MongoCollectionImpl.executeSingleWriteRequest(MongoCollectionImpl.java:1017)
	at com.mongodb.client.internal.MongoCollectionImpl.executeInsertOne(MongoCollectionImpl.java:470)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:453)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:447)
	at org.springframework.data.mongodb.core.MongoTemplate.lambda$insertDocument$16(MongoTemplate.java:1464)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:553)
	... 56 common frames omitted
2022-06-24 14:05:54,096 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks com.estockmarket.query.domain.model.Stocks@3c538020
2022-06-24 14:05:54,125 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Seeking to offset 5 for partition updateStock-0
2022-06-24 14:05:54,125 ERROR org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception; nested exception is org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:206)
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:112)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2359)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2228)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2142)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2024)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1706)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2371)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2342)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2303)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2217)
	... 9 common frames omitted
Caused by: org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.data.mongodb.core.MongoExceptionTranslator.translateExceptionIfPossible(MongoExceptionTranslator.java:106)
	at org.springframework.data.mongodb.core.MongoTemplate.potentiallyConvertRuntimeException(MongoTemplate.java:2899)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:555)
	at org.springframework.data.mongodb.core.MongoTemplate.insertDocument(MongoTemplate.java:1458)
	at org.springframework.data.mongodb.core.MongoTemplate.doInsert(MongoTemplate.java:1257)
	at org.springframework.data.mongodb.core.MongoTemplate.insert(MongoTemplate.java:1172)
	at org.springframework.data.mongodb.repository.support.SimpleMongoRepository.save(SimpleMongoRepository.java:91)
	at jdk.internal.reflect.GeneratedMethodAccessor267.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:289)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:137)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:121)
	at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:529)
	at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:285)
	at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:599)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:163)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:138)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:80)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at jdk.internal.reflect.GeneratedMethodAccessor266.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:344)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:198)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:137)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at com.estockmarket.query.domain.service.StockService.updateStock(StockService.java:105)
	at com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(KafkaStocksEventListener.java:36)
	at jdk.internal.reflect.GeneratedMethodAccessor301.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:56)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:347)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:92)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:53)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2322)
	... 11 common frames omitted
Caused by: com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at com.mongodb.client.internal.MongoCollectionImpl.executeSingleWriteRequest(MongoCollectionImpl.java:1017)
	at com.mongodb.client.internal.MongoCollectionImpl.executeInsertOne(MongoCollectionImpl.java:470)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:453)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:447)
	at org.springframework.data.mongodb.core.MongoTemplate.lambda$insertDocument$16(MongoTemplate.java:1464)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:553)
	... 56 common frames omitted
2022-06-24 14:05:54,630 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks com.estockmarket.query.domain.model.Stocks@c1c4ecb
2022-06-24 14:05:54,658 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Seeking to offset 5 for partition updateStock-0
2022-06-24 14:05:54,659 ERROR org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception; nested exception is org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:206)
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:112)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2359)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2228)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2142)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2024)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1706)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2371)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2342)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2303)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2217)
	... 9 common frames omitted
Caused by: org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.data.mongodb.core.MongoExceptionTranslator.translateExceptionIfPossible(MongoExceptionTranslator.java:106)
	at org.springframework.data.mongodb.core.MongoTemplate.potentiallyConvertRuntimeException(MongoTemplate.java:2899)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:555)
	at org.springframework.data.mongodb.core.MongoTemplate.insertDocument(MongoTemplate.java:1458)
	at org.springframework.data.mongodb.core.MongoTemplate.doInsert(MongoTemplate.java:1257)
	at org.springframework.data.mongodb.core.MongoTemplate.insert(MongoTemplate.java:1172)
	at org.springframework.data.mongodb.repository.support.SimpleMongoRepository.save(SimpleMongoRepository.java:91)
	at jdk.internal.reflect.GeneratedMethodAccessor267.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:289)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:137)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:121)
	at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:529)
	at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:285)
	at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:599)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:163)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:138)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:80)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at jdk.internal.reflect.GeneratedMethodAccessor266.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:344)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:198)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:137)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at com.estockmarket.query.domain.service.StockService.updateStock(StockService.java:105)
	at com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(KafkaStocksEventListener.java:36)
	at jdk.internal.reflect.GeneratedMethodAccessor301.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:56)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:347)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:92)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:53)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2322)
	... 11 common frames omitted
Caused by: com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at com.mongodb.client.internal.MongoCollectionImpl.executeSingleWriteRequest(MongoCollectionImpl.java:1017)
	at com.mongodb.client.internal.MongoCollectionImpl.executeInsertOne(MongoCollectionImpl.java:470)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:453)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:447)
	at org.springframework.data.mongodb.core.MongoTemplate.lambda$insertDocument$16(MongoTemplate.java:1464)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:553)
	... 56 common frames omitted
2022-06-24 14:05:55,167 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks com.estockmarket.query.domain.model.Stocks@2e3c8f90
2022-06-24 14:05:55,180 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Seeking to offset 5 for partition updateStock-0
2022-06-24 14:05:55,181 ERROR org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception; nested exception is org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:206)
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:112)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2359)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2228)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2142)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2024)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1706)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2371)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2342)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2303)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2217)
	... 9 common frames omitted
Caused by: org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.data.mongodb.core.MongoExceptionTranslator.translateExceptionIfPossible(MongoExceptionTranslator.java:106)
	at org.springframework.data.mongodb.core.MongoTemplate.potentiallyConvertRuntimeException(MongoTemplate.java:2899)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:555)
	at org.springframework.data.mongodb.core.MongoTemplate.insertDocument(MongoTemplate.java:1458)
	at org.springframework.data.mongodb.core.MongoTemplate.doInsert(MongoTemplate.java:1257)
	at org.springframework.data.mongodb.core.MongoTemplate.insert(MongoTemplate.java:1172)
	at org.springframework.data.mongodb.repository.support.SimpleMongoRepository.save(SimpleMongoRepository.java:91)
	at jdk.internal.reflect.GeneratedMethodAccessor267.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:289)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:137)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:121)
	at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:529)
	at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:285)
	at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:599)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:163)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:138)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:80)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at jdk.internal.reflect.GeneratedMethodAccessor266.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:344)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:198)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:137)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at com.estockmarket.query.domain.service.StockService.updateStock(StockService.java:105)
	at com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(KafkaStocksEventListener.java:36)
	at jdk.internal.reflect.GeneratedMethodAccessor301.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:56)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:347)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:92)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:53)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2322)
	... 11 common frames omitted
Caused by: com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at com.mongodb.client.internal.MongoCollectionImpl.executeSingleWriteRequest(MongoCollectionImpl.java:1017)
	at com.mongodb.client.internal.MongoCollectionImpl.executeInsertOne(MongoCollectionImpl.java:470)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:453)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:447)
	at org.springframework.data.mongodb.core.MongoTemplate.lambda$insertDocument$16(MongoTemplate.java:1464)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:553)
	... 56 common frames omitted
2022-06-24 14:05:55,696 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks com.estockmarket.query.domain.model.Stocks@28f69ae
2022-06-24 14:05:55,723 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Seeking to offset 5 for partition updateStock-0
2022-06-24 14:05:55,724 ERROR org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception; nested exception is org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:206)
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:112)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2359)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2228)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2142)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2024)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1706)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2371)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2342)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2303)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2217)
	... 9 common frames omitted
Caused by: org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.data.mongodb.core.MongoExceptionTranslator.translateExceptionIfPossible(MongoExceptionTranslator.java:106)
	at org.springframework.data.mongodb.core.MongoTemplate.potentiallyConvertRuntimeException(MongoTemplate.java:2899)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:555)
	at org.springframework.data.mongodb.core.MongoTemplate.insertDocument(MongoTemplate.java:1458)
	at org.springframework.data.mongodb.core.MongoTemplate.doInsert(MongoTemplate.java:1257)
	at org.springframework.data.mongodb.core.MongoTemplate.insert(MongoTemplate.java:1172)
	at org.springframework.data.mongodb.repository.support.SimpleMongoRepository.save(SimpleMongoRepository.java:91)
	at jdk.internal.reflect.GeneratedMethodAccessor267.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:289)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:137)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:121)
	at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:529)
	at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:285)
	at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:599)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:163)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:138)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:80)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at jdk.internal.reflect.GeneratedMethodAccessor266.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:344)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:198)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:137)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at com.estockmarket.query.domain.service.StockService.updateStock(StockService.java:105)
	at com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(KafkaStocksEventListener.java:36)
	at jdk.internal.reflect.GeneratedMethodAccessor301.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:56)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:347)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:92)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:53)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2322)
	... 11 common frames omitted
Caused by: com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at com.mongodb.client.internal.MongoCollectionImpl.executeSingleWriteRequest(MongoCollectionImpl.java:1017)
	at com.mongodb.client.internal.MongoCollectionImpl.executeInsertOne(MongoCollectionImpl.java:470)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:453)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:447)
	at org.springframework.data.mongodb.core.MongoTemplate.lambda$insertDocument$16(MongoTemplate.java:1464)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:553)
	... 56 common frames omitted
2022-06-24 14:05:56,241 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks com.estockmarket.query.domain.model.Stocks@227dbb5f
2022-06-24 14:05:56,269 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Seeking to offset 5 for partition updateStock-0
2022-06-24 14:05:56,270 ERROR org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception; nested exception is org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:206)
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:112)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2359)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2228)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2142)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2024)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1706)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2371)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2342)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2303)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2217)
	... 9 common frames omitted
Caused by: org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.data.mongodb.core.MongoExceptionTranslator.translateExceptionIfPossible(MongoExceptionTranslator.java:106)
	at org.springframework.data.mongodb.core.MongoTemplate.potentiallyConvertRuntimeException(MongoTemplate.java:2899)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:555)
	at org.springframework.data.mongodb.core.MongoTemplate.insertDocument(MongoTemplate.java:1458)
	at org.springframework.data.mongodb.core.MongoTemplate.doInsert(MongoTemplate.java:1257)
	at org.springframework.data.mongodb.core.MongoTemplate.insert(MongoTemplate.java:1172)
	at org.springframework.data.mongodb.repository.support.SimpleMongoRepository.save(SimpleMongoRepository.java:91)
	at jdk.internal.reflect.GeneratedMethodAccessor267.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:289)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:137)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:121)
	at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:529)
	at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:285)
	at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:599)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:163)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:138)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:80)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at jdk.internal.reflect.GeneratedMethodAccessor266.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:344)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:198)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:137)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at com.estockmarket.query.domain.service.StockService.updateStock(StockService.java:105)
	at com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(KafkaStocksEventListener.java:36)
	at jdk.internal.reflect.GeneratedMethodAccessor301.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:56)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:347)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:92)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:53)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2322)
	... 11 common frames omitted
Caused by: com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at com.mongodb.client.internal.MongoCollectionImpl.executeSingleWriteRequest(MongoCollectionImpl.java:1017)
	at com.mongodb.client.internal.MongoCollectionImpl.executeInsertOne(MongoCollectionImpl.java:470)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:453)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:447)
	at org.springframework.data.mongodb.core.MongoTemplate.lambda$insertDocument$16(MongoTemplate.java:1464)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:553)
	... 56 common frames omitted
2022-06-24 14:05:56,787 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks com.estockmarket.query.domain.model.Stocks@4cf9d3cd
2022-06-24 14:05:56,814 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Seeking to offset 5 for partition updateStock-0
2022-06-24 14:05:56,814 ERROR org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception; nested exception is org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:206)
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:112)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2359)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2228)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2142)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2024)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1706)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2371)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2342)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2303)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2217)
	... 9 common frames omitted
Caused by: org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.data.mongodb.core.MongoExceptionTranslator.translateExceptionIfPossible(MongoExceptionTranslator.java:106)
	at org.springframework.data.mongodb.core.MongoTemplate.potentiallyConvertRuntimeException(MongoTemplate.java:2899)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:555)
	at org.springframework.data.mongodb.core.MongoTemplate.insertDocument(MongoTemplate.java:1458)
	at org.springframework.data.mongodb.core.MongoTemplate.doInsert(MongoTemplate.java:1257)
	at org.springframework.data.mongodb.core.MongoTemplate.insert(MongoTemplate.java:1172)
	at org.springframework.data.mongodb.repository.support.SimpleMongoRepository.save(SimpleMongoRepository.java:91)
	at jdk.internal.reflect.GeneratedMethodAccessor267.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:289)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:137)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:121)
	at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:529)
	at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:285)
	at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:599)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:163)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:138)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:80)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at jdk.internal.reflect.GeneratedMethodAccessor266.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:344)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:198)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:137)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at com.estockmarket.query.domain.service.StockService.updateStock(StockService.java:105)
	at com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(KafkaStocksEventListener.java:36)
	at jdk.internal.reflect.GeneratedMethodAccessor301.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:56)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:347)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:92)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:53)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2322)
	... 11 common frames omitted
Caused by: com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at com.mongodb.client.internal.MongoCollectionImpl.executeSingleWriteRequest(MongoCollectionImpl.java:1017)
	at com.mongodb.client.internal.MongoCollectionImpl.executeInsertOne(MongoCollectionImpl.java:470)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:453)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:447)
	at org.springframework.data.mongodb.core.MongoTemplate.lambda$insertDocument$16(MongoTemplate.java:1464)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:553)
	... 56 common frames omitted
2022-06-24 14:05:57,331 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks com.estockmarket.query.domain.model.Stocks@6f395cd
2022-06-24 14:05:57,359 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Seeking to offset 5 for partition updateStock-0
2022-06-24 14:05:57,359 ERROR org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception; nested exception is org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:206)
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:112)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2359)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2228)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2142)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2024)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1706)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2371)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2342)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2303)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2217)
	... 9 common frames omitted
Caused by: org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.data.mongodb.core.MongoExceptionTranslator.translateExceptionIfPossible(MongoExceptionTranslator.java:106)
	at org.springframework.data.mongodb.core.MongoTemplate.potentiallyConvertRuntimeException(MongoTemplate.java:2899)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:555)
	at org.springframework.data.mongodb.core.MongoTemplate.insertDocument(MongoTemplate.java:1458)
	at org.springframework.data.mongodb.core.MongoTemplate.doInsert(MongoTemplate.java:1257)
	at org.springframework.data.mongodb.core.MongoTemplate.insert(MongoTemplate.java:1172)
	at org.springframework.data.mongodb.repository.support.SimpleMongoRepository.save(SimpleMongoRepository.java:91)
	at jdk.internal.reflect.GeneratedMethodAccessor267.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:289)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:137)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:121)
	at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:529)
	at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:285)
	at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:599)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:163)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:138)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:80)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at jdk.internal.reflect.GeneratedMethodAccessor266.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:344)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:198)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:137)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at com.estockmarket.query.domain.service.StockService.updateStock(StockService.java:105)
	at com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(KafkaStocksEventListener.java:36)
	at jdk.internal.reflect.GeneratedMethodAccessor301.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:56)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:347)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:92)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:53)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2322)
	... 11 common frames omitted
Caused by: com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at com.mongodb.client.internal.MongoCollectionImpl.executeSingleWriteRequest(MongoCollectionImpl.java:1017)
	at com.mongodb.client.internal.MongoCollectionImpl.executeInsertOne(MongoCollectionImpl.java:470)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:453)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:447)
	at org.springframework.data.mongodb.core.MongoTemplate.lambda$insertDocument$16(MongoTemplate.java:1464)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:553)
	... 56 common frames omitted
2022-06-24 14:05:57,875 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks com.estockmarket.query.domain.model.Stocks@7f484ea4
2022-06-24 14:05:57,879 ERROR org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Backoff FixedBackOff{interval=0, currentAttempts=10, maxAttempts=9} exhausted for updateStock-0@5
org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2371)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2342)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2303)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2217)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2142)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2024)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1706)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.data.mongodb.core.MongoExceptionTranslator.translateExceptionIfPossible(MongoExceptionTranslator.java:106)
	at org.springframework.data.mongodb.core.MongoTemplate.potentiallyConvertRuntimeException(MongoTemplate.java:2899)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:555)
	at org.springframework.data.mongodb.core.MongoTemplate.insertDocument(MongoTemplate.java:1458)
	at org.springframework.data.mongodb.core.MongoTemplate.doInsert(MongoTemplate.java:1257)
	at org.springframework.data.mongodb.core.MongoTemplate.insert(MongoTemplate.java:1172)
	at org.springframework.data.mongodb.repository.support.SimpleMongoRepository.save(SimpleMongoRepository.java:91)
	at jdk.internal.reflect.GeneratedMethodAccessor267.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:289)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:137)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:121)
	at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:529)
	at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:285)
	at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:599)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:163)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:138)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:80)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at jdk.internal.reflect.GeneratedMethodAccessor266.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:344)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:198)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:137)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy167.save(Unknown Source)
	at com.estockmarket.query.domain.service.StockService.updateStock(StockService.java:105)
	at com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(KafkaStocksEventListener.java:36)
	at jdk.internal.reflect.GeneratedMethodAccessor301.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:56)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:347)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:92)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:53)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2322)
	... 11 common frames omitted
Caused by: com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at com.mongodb.client.internal.MongoCollectionImpl.executeSingleWriteRequest(MongoCollectionImpl.java:1017)
	at com.mongodb.client.internal.MongoCollectionImpl.executeInsertOne(MongoCollectionImpl.java:470)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:453)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:447)
	at org.springframework.data.mongodb.core.MongoTemplate.lambda$insertDocument$16(MongoTemplate.java:1464)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:553)
	... 56 common frames omitted
2022-06-24 14:07:22,050 INFO org.springframework.cloud.netflix.eureka.serviceregistry.EurekaServiceRegistry [Thread-21] Unregistering application ESTOCKMARKET-QUERY with eureka with status DOWN
2022-06-24 14:07:22,054 INFO com.netflix.discovery.DiscoveryClient$3 [Thread-21] Saw local status change event StatusChangeEvent [timestamp=1656059842054, current=DOWN, previous=UP]
2022-06-24 14:07:22,056 INFO com.netflix.discovery.DiscoveryClient [DiscoveryClient-InstanceInfoReplicator-0] DiscoveryClient_ESTOCKMARKET-QUERY/host.docker.internal:estockmarket-query:6093: registering service...
2022-06-24 14:07:22,149 INFO com.netflix.discovery.DiscoveryClient [DiscoveryClient-InstanceInfoReplicator-0] DiscoveryClient_ESTOCKMARKET-QUERY/host.docker.internal:estockmarket-query:6093 - registration status: 204
2022-06-24 14:07:22,221 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-25, groupId=group_id] Revoke previously assigned partitions createStock-0
2022-06-24 14:07:22,222 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] group_id: partitions revoked: [createStock-0]
2022-06-24 14:07:22,222 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-25, groupId=group_id] Member consumer-group_id-25-03a419c5-2ff2-4345-a103-98407f977d0b sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2022-06-24 14:07:22,222 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Revoke previously assigned partitions updateStock-0
2022-06-24 14:07:22,222 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-28, groupId=group_id] Revoke previously assigned partitions createSector-0
2022-06-24 14:07:22,222 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-27, groupId=group_id] Revoke previously assigned partitions removeSector-0
2022-06-24 14:07:22,222 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-29, groupId=group_id] Revoke previously assigned partitions removeStock-0
2022-06-24 14:07:22,222 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-31, groupId=group_id] Revoke previously assigned partitions removeCompany-0
2022-06-24 14:07:22,222 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] group_id: partitions revoked: [removeSector-0]
2022-06-24 14:07:22,222 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-32, groupId=group_id] Revoke previously assigned partitions createCompany-0
2022-06-24 14:07:22,222 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-25, groupId=group_id] Unsubscribed all topics or patterns and assigned partitions
2022-06-24 14:07:22,222 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] group_id: partitions revoked: [createSector-0]
2022-06-24 14:07:22,223 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] group_id: partitions revoked: [removeStock-0]
2022-06-24 14:07:22,222 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] group_id: partitions revoked: [updateStock-0]
2022-06-24 14:07:22,223 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-29, groupId=group_id] Member consumer-group_id-29-260a5215-44c3-49dc-810d-60f26bfb9c7f sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2022-06-24 14:07:22,223 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-28, groupId=group_id] Member consumer-group_id-28-c909bfcd-8df9-4a5e-978c-5f9b2a336ab6 sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2022-06-24 14:07:22,223 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Member consumer-group_id-30-64390239-52bd-450b-a827-7fcde96eca58 sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2022-06-24 14:07:22,223 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-29, groupId=group_id] Unsubscribed all topics or patterns and assigned partitions
2022-06-24 14:07:22,222 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-26, groupId=group_id] Revoke previously assigned partitions createUser-0
2022-06-24 14:07:22,223 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-30, groupId=group_id] Unsubscribed all topics or patterns and assigned partitions
2022-06-24 14:07:22,223 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] group_id: partitions revoked: [createUser-0]
2022-06-24 14:07:22,223 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-27, groupId=group_id] Member consumer-group_id-27-4c2528ec-168e-42a1-b2bb-026799880307 sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2022-06-24 14:07:22,223 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-26, groupId=group_id] Member consumer-group_id-26-5fdcbd7e-a77d-4673-b11c-267938f2b5c8 sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2022-06-24 14:07:22,224 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-27, groupId=group_id] Unsubscribed all topics or patterns and assigned partitions
2022-06-24 14:07:22,223 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] group_id: partitions revoked: [createCompany-0]
2022-06-24 14:07:22,224 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-32, groupId=group_id] Member consumer-group_id-32-437b1c86-4893-46ac-ba15-c343de5f80c8 sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2022-06-24 14:07:22,224 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-26, groupId=group_id] Unsubscribed all topics or patterns and assigned partitions
2022-06-24 14:07:22,223 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] group_id: partitions revoked: [removeCompany-0]
2022-06-24 14:07:22,224 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-32, groupId=group_id] Unsubscribed all topics or patterns and assigned partitions
2022-06-24 14:07:22,223 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-28, groupId=group_id] Unsubscribed all topics or patterns and assigned partitions
2022-06-24 14:07:22,224 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-31, groupId=group_id] Member consumer-group_id-31-2f5de27f-62cb-47b3-83a2-71b547b99584 sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2022-06-24 14:07:22,224 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-31, groupId=group_id] Unsubscribed all topics or patterns and assigned partitions
2022-06-24 14:07:22,232 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] Metrics scheduler closed
2022-06-24 14:07:22,233 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-06-24 14:07:22,233 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] Metrics scheduler closed
2022-06-24 14:07:22,233 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] Metrics reporters closed
2022-06-24 14:07:22,233 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-06-24 14:07:22,233 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] Metrics reporters closed
2022-06-24 14:07:22,234 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Metrics scheduler closed
2022-06-24 14:07:22,234 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-06-24 14:07:22,234 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Metrics reporters closed
2022-06-24 14:07:22,235 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] Metrics scheduler closed
2022-06-24 14:07:22,235 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-06-24 14:07:22,235 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] Metrics reporters closed
2022-06-24 14:07:22,236 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] Metrics scheduler closed
2022-06-24 14:07:22,236 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-06-24 14:07:22,236 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] Metrics reporters closed
2022-06-24 14:07:22,237 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] Metrics scheduler closed
2022-06-24 14:07:22,237 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-06-24 14:07:22,237 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] Metrics reporters closed
2022-06-24 14:07:22,238 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Metrics scheduler closed
2022-06-24 14:07:22,238 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-06-24 14:07:22,238 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Metrics reporters closed
2022-06-24 14:07:22,238 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] App info kafka.consumer for consumer-group_id-27 unregistered
2022-06-24 14:07:22,238 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] group_id: Consumer stopped
2022-06-24 14:07:22,239 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] App info kafka.consumer for consumer-group_id-32 unregistered
2022-06-24 14:07:22,240 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] group_id: Consumer stopped
2022-06-24 14:07:22,240 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] App info kafka.consumer for consumer-group_id-30 unregistered
2022-06-24 14:07:22,240 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] group_id: Consumer stopped
2022-06-24 14:07:22,241 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] App info kafka.consumer for consumer-group_id-25 unregistered
2022-06-24 14:07:22,241 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] Metrics scheduler closed
2022-06-24 14:07:22,241 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-06-24 14:07:22,241 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] group_id: Consumer stopped
2022-06-24 14:07:22,241 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] Metrics reporters closed
2022-06-24 14:07:22,241 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] App info kafka.consumer for consumer-group_id-29 unregistered
2022-06-24 14:07:22,241 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] group_id: Consumer stopped
2022-06-24 14:07:22,241 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] App info kafka.consumer for consumer-group_id-31 unregistered
2022-06-24 14:07:22,241 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] group_id: Consumer stopped
2022-06-24 14:07:22,241 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] App info kafka.consumer for consumer-group_id-28 unregistered
2022-06-24 14:07:22,241 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] group_id: Consumer stopped
2022-06-24 14:07:22,250 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] App info kafka.consumer for consumer-group_id-26 unregistered
2022-06-24 14:07:22,250 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] group_id: Consumer stopped
2022-06-24 14:07:22,270 INFO com.mongodb.diagnostics.logging.SLF4JLogger [Thread-21] Closed connection [connectionId{localValue:19, serverValue:57}] to localhost:27017 because the pool has been closed.
2022-06-24 14:07:22,271 INFO com.mongodb.diagnostics.logging.SLF4JLogger [Thread-21] Closed connection [connectionId{localValue:20, serverValue:58}] to localhost:27017 because the pool has been closed.
2022-06-24 14:07:22,272 INFO com.netflix.discovery.DiscoveryClient [Thread-21] Shutting down DiscoveryClient ...
2022-06-24 14:07:25,280 INFO com.netflix.discovery.DiscoveryClient [Thread-21] Unregistering ...
2022-06-24 14:07:25,319 INFO com.netflix.discovery.DiscoveryClient [Thread-21] DiscoveryClient_ESTOCKMARKET-QUERY/host.docker.internal:estockmarket-query:6093 - deregister  status: 200
2022-06-24 14:07:25,339 INFO com.netflix.discovery.DiscoveryClient [Thread-21] Completed shut down of DiscoveryClient
2022-06-24 14:07:25,830 INFO org.springframework.boot.StartupInfoLogger [restartedMain] Starting EstockmarketQueryApplication using Java 11.0.12 on ctsjava91 with PID 18180 (C:\Repo\elk-stock\ESTOCKMARKET-FSE\estockmarket-query\target\classes started by cogjava182 in C:\Repo\elk-stock\ESTOCKMARKET-FSE\estockmarket-query)
2022-06-24 14:07:25,831 DEBUG org.springframework.boot.StartupInfoLogger [restartedMain] Running with Spring Boot v2.5.3, Spring v5.3.9
2022-06-24 14:07:25,831 INFO org.springframework.boot.SpringApplication [restartedMain] No active profile set, falling back to default profiles: default
2022-06-24 14:07:26,655 INFO org.springframework.data.repository.config.RepositoryConfigurationDelegate [restartedMain] Bootstrapping Spring Data MongoDB repositories in DEFAULT mode.
2022-06-24 14:07:26,713 INFO org.springframework.data.repository.config.RepositoryConfigurationDelegate [restartedMain] Finished Spring Data repository scanning in 57 ms. Found 4 MongoDB repository interfaces.
2022-06-24 14:07:26,854 INFO org.springframework.cloud.context.scope.GenericScope [restartedMain] BeanFactory id=fff56ba9-7131-3a6a-a033-f341357c57c5
2022-06-24 14:07:27,215 INFO org.springframework.boot.web.embedded.tomcat.TomcatWebServer [restartedMain] Tomcat initialized with port(s): 6093 (http)
2022-06-24 14:07:27,216 INFO org.apache.juli.logging.DirectJDKLog [restartedMain] Initializing ProtocolHandler ["http-nio-6093"]
2022-06-24 14:07:27,217 INFO org.apache.juli.logging.DirectJDKLog [restartedMain] Starting service [Tomcat]
2022-06-24 14:07:27,217 INFO org.apache.juli.logging.DirectJDKLog [restartedMain] Starting Servlet engine: [Apache Tomcat/9.0.50]
2022-06-24 14:07:27,298 INFO org.apache.juli.logging.DirectJDKLog [restartedMain] Initializing Spring embedded WebApplicationContext
2022-06-24 14:07:27,298 INFO org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext [restartedMain] Root WebApplicationContext: initialization completed in 1462 ms
2022-06-24 14:07:27,365 INFO com.mongodb.diagnostics.logging.SLF4JLogger [restartedMain] Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2022-06-24 14:07:27,373 INFO com.mongodb.diagnostics.logging.SLF4JLogger [cluster-ClusterId{value='62b577c752d23d5bec795f98', description='null'}-localhost:27017] Opened connection [connectionId{localValue:21, serverValue:59}] to localhost:27017
2022-06-24 14:07:27,374 INFO com.mongodb.diagnostics.logging.SLF4JLogger [cluster-rtt-ClusterId{value='62b577c752d23d5bec795f98', description='null'}-localhost:27017] Opened connection [connectionId{localValue:22, serverValue:60}] to localhost:27017
2022-06-24 14:07:27,376 INFO com.mongodb.diagnostics.logging.SLF4JLogger [cluster-ClusterId{value='62b577c752d23d5bec795f98', description='null'}-localhost:27017] Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=2739400}
2022-06-24 14:07:27,376 WARN org.springframework.data.convert.CustomConversions [restartedMain] Registering converter from class java.time.LocalDateTime to class org.joda.time.LocalDateTime as reading converter although it doesn't convert from a store-supported type! You might want to check your annotation setup at the converter implementation.
2022-06-24 14:07:27,432 INFO org.springframework.boot.devtools.autoconfigure.OptionalLiveReloadServer [restartedMain] LiveReload server is running on port 35729
2022-06-24 14:07:27,455 WARN org.springframework.data.convert.CustomConversions [restartedMain] Registering converter from class java.time.LocalDateTime to class org.joda.time.LocalDateTime as reading converter although it doesn't convert from a store-supported type! You might want to check your annotation setup at the converter implementation.
2022-06-24 14:07:27,510 INFO com.estockmarket.query.domain.service.OTPService [restartedMain] inside otp servcie {}
2022-06-24 14:07:27,882 INFO org.springframework.cloud.netflix.eureka.config.DiscoveryClientOptionalArgsConfiguration [restartedMain] Eureka HTTP Client uses RestTemplate.
2022-06-24 14:07:27,946 WARN org.springframework.cloud.loadbalancer.config.LoadBalancerCacheAutoConfiguration$LoadBalancerCaffeineWarnLogger [restartedMain] Spring Cloud LoadBalancer is currently working with the default cache. You can switch to using Caffeine cache, by adding it and org.springframework.cache.caffeine.CaffeineCacheManager to the classpath.
2022-06-24 14:07:28,002 INFO org.springframework.cloud.netflix.eureka.InstanceInfoFactory [restartedMain] Setting initial instance status as: STARTING
2022-06-24 14:07:28,004 INFO com.netflix.discovery.DiscoveryClient [restartedMain] Initializing Eureka in region us-east-1
2022-06-24 14:07:28,004 INFO com.netflix.discovery.shared.resolver.aws.ConfigClusterResolver [restartedMain] Resolving eureka endpoints via configuration
2022-06-24 14:07:28,006 INFO com.netflix.discovery.DiscoveryClient [restartedMain] Disable delta property : false
2022-06-24 14:07:28,006 INFO com.netflix.discovery.DiscoveryClient [restartedMain] Single vip registry refresh property : null
2022-06-24 14:07:28,006 INFO com.netflix.discovery.DiscoveryClient [restartedMain] Force full registry fetch : false
2022-06-24 14:07:28,007 INFO com.netflix.discovery.DiscoveryClient [restartedMain] Application is null : false
2022-06-24 14:07:28,007 INFO com.netflix.discovery.DiscoveryClient [restartedMain] Registered Applications size is zero : true
2022-06-24 14:07:28,007 INFO com.netflix.discovery.DiscoveryClient [restartedMain] Application version is -1: true
2022-06-24 14:07:28,007 INFO com.netflix.discovery.DiscoveryClient [restartedMain] Getting all instance registry info from the eureka server
2022-06-24 14:07:28,076 INFO com.netflix.discovery.DiscoveryClient [restartedMain] The response status is 200
2022-06-24 14:07:28,077 INFO com.netflix.discovery.DiscoveryClient [restartedMain] Starting heartbeat executor: renew interval is: 30
2022-06-24 14:07:28,078 INFO com.netflix.discovery.InstanceInfoReplicator [restartedMain] InstanceInfoReplicator onDemand update allowed rate per min is 4
2022-06-24 14:07:28,079 INFO com.netflix.discovery.DiscoveryClient [restartedMain] Discovery Client initialized at timestamp 1656059848079 with initial instances count: 3
2022-06-24 14:07:28,081 INFO org.springframework.cloud.netflix.eureka.serviceregistry.EurekaServiceRegistry [restartedMain] Registering application ESTOCKMARKET-QUERY with eureka with status UP
2022-06-24 14:07:28,082 INFO com.netflix.discovery.DiscoveryClient$3 [restartedMain] Saw local status change event StatusChangeEvent [timestamp=1656059848082, current=UP, previous=STARTING]
2022-06-24 14:07:28,083 INFO com.netflix.discovery.DiscoveryClient [DiscoveryClient-InstanceInfoReplicator-0] DiscoveryClient_ESTOCKMARKET-QUERY/host.docker.internal:estockmarket-query:6093: registering service...
2022-06-24 14:07:28,084 INFO org.apache.kafka.common.config.AbstractConfig [restartedMain] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-group_id-33
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = group_id
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2022-06-24 14:07:28,087 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka version: 2.7.1
2022-06-24 14:07:28,087 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka commitId: 61dbce85d0d41457
2022-06-24 14:07:28,087 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka startTimeMs: 1656059848087
2022-06-24 14:07:28,088 INFO org.apache.kafka.clients.consumer.KafkaConsumer [restartedMain] [Consumer clientId=consumer-group_id-33, groupId=group_id] Subscribed to topic(s): createStock
2022-06-24 14:07:28,092 INFO com.netflix.discovery.DiscoveryClient [DiscoveryClient-InstanceInfoReplicator-0] DiscoveryClient_ESTOCKMARKET-QUERY/host.docker.internal:estockmarket-query:6093 - registration status: 204
2022-06-24 14:07:28,135 INFO org.apache.kafka.common.config.AbstractConfig [restartedMain] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-group_id-34
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = group_id
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2022-06-24 14:07:28,139 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka version: 2.7.1
2022-06-24 14:07:28,139 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka commitId: 61dbce85d0d41457
2022-06-24 14:07:28,139 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka startTimeMs: 1656059848139
2022-06-24 14:07:28,140 INFO org.apache.kafka.clients.consumer.KafkaConsumer [restartedMain] [Consumer clientId=consumer-group_id-34, groupId=group_id] Subscribed to topic(s): createUser
2022-06-24 14:07:28,140 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-33, groupId=group_id] Cluster ID: 5kpO7pNKTFqroM6YwsJ86A
2022-06-24 14:07:28,140 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-33, groupId=group_id] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2022-06-24 14:07:28,142 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-33, groupId=group_id] (Re-)joining group
2022-06-24 14:07:28,143 INFO org.apache.kafka.common.config.AbstractConfig [restartedMain] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-group_id-35
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = group_id
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2022-06-24 14:07:28,145 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-34, groupId=group_id] Cluster ID: 5kpO7pNKTFqroM6YwsJ86A
2022-06-24 14:07:28,145 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-34, groupId=group_id] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2022-06-24 14:07:28,145 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-33, groupId=group_id] (Re-)joining group
2022-06-24 14:07:28,146 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-34, groupId=group_id] (Re-)joining group
2022-06-24 14:07:28,148 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka version: 2.7.1
2022-06-24 14:07:28,148 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka commitId: 61dbce85d0d41457
2022-06-24 14:07:28,148 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka startTimeMs: 1656059848148
2022-06-24 14:07:28,149 INFO org.apache.kafka.clients.consumer.KafkaConsumer [restartedMain] [Consumer clientId=consumer-group_id-35, groupId=group_id] Subscribed to topic(s): removeSector
2022-06-24 14:07:28,150 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-33, groupId=group_id] Successfully joined group with generation Generation{generationId=86, memberId='consumer-group_id-33-7aab9548-85d4-4e3f-8eab-8ffb956f9139', protocol='range'}
2022-06-24 14:07:28,150 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-33, groupId=group_id] Finished assignment for group at generation 86: {consumer-group_id-33-7aab9548-85d4-4e3f-8eab-8ffb956f9139=Assignment(partitions=[createStock-0])}
2022-06-24 14:07:28,151 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-34, groupId=group_id] (Re-)joining group
2022-06-24 14:07:28,151 INFO org.apache.kafka.common.config.AbstractConfig [restartedMain] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-group_id-36
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = group_id
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2022-06-24 14:07:28,153 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-33, groupId=group_id] SyncGroup failed: The group began another rebalance. Need to re-join the group. Sent generation was Generation{generationId=86, memberId='consumer-group_id-33-7aab9548-85d4-4e3f-8eab-8ffb956f9139', protocol='range'}
2022-06-24 14:07:28,153 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-33, groupId=group_id] Rebalance failed.
org.apache.kafka.common.errors.RebalanceInProgressException: The group is rebalancing, so a rejoin is needed.
2022-06-24 14:07:28,153 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-33, groupId=group_id] (Re-)joining group
2022-06-24 14:07:28,155 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-35, groupId=group_id] Cluster ID: 5kpO7pNKTFqroM6YwsJ86A
2022-06-24 14:07:28,155 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-35, groupId=group_id] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2022-06-24 14:07:28,155 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-33, groupId=group_id] Successfully joined group with generation Generation{generationId=87, memberId='consumer-group_id-33-7aab9548-85d4-4e3f-8eab-8ffb956f9139', protocol='range'}
2022-06-24 14:07:28,156 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-34, groupId=group_id] Successfully joined group with generation Generation{generationId=87, memberId='consumer-group_id-34-d7487a31-12cf-48b7-9c66-9fd7495adcce', protocol='range'}
2022-06-24 14:07:28,157 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-35, groupId=group_id] (Re-)joining group
2022-06-24 14:07:28,158 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka version: 2.7.1
2022-06-24 14:07:28,159 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka commitId: 61dbce85d0d41457
2022-06-24 14:07:28,159 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka startTimeMs: 1656059848158
2022-06-24 14:07:28,159 INFO org.apache.kafka.clients.consumer.KafkaConsumer [restartedMain] [Consumer clientId=consumer-group_id-36, groupId=group_id] Subscribed to topic(s): createSector
2022-06-24 14:07:28,160 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-33, groupId=group_id] Finished assignment for group at generation 87: {consumer-group_id-33-7aab9548-85d4-4e3f-8eab-8ffb956f9139=Assignment(partitions=[createStock-0]), consumer-group_id-34-d7487a31-12cf-48b7-9c66-9fd7495adcce=Assignment(partitions=[createUser-0])}
2022-06-24 14:07:28,164 INFO org.apache.kafka.common.config.AbstractConfig [restartedMain] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-group_id-37
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = group_id
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2022-06-24 14:07:28,166 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-34, groupId=group_id] Successfully synced group in generation Generation{generationId=87, memberId='consumer-group_id-34-d7487a31-12cf-48b7-9c66-9fd7495adcce', protocol='range'}
2022-06-24 14:07:28,166 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-33, groupId=group_id] Successfully synced group in generation Generation{generationId=87, memberId='consumer-group_id-33-7aab9548-85d4-4e3f-8eab-8ffb956f9139', protocol='range'}
2022-06-24 14:07:28,166 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-34, groupId=group_id] Notifying assignor about the new Assignment(partitions=[createUser-0])
2022-06-24 14:07:28,166 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-33, groupId=group_id] Notifying assignor about the new Assignment(partitions=[createStock-0])
2022-06-24 14:07:28,166 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-34, groupId=group_id] Adding newly assigned partitions: createUser-0
2022-06-24 14:07:28,166 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-33, groupId=group_id] Adding newly assigned partitions: createStock-0
2022-06-24 14:07:28,167 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-36, groupId=group_id] Cluster ID: 5kpO7pNKTFqroM6YwsJ86A
2022-06-24 14:07:28,167 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-35, groupId=group_id] (Re-)joining group
2022-06-24 14:07:28,167 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-36, groupId=group_id] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2022-06-24 14:07:28,169 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-34, groupId=group_id] Setting offset for partition createUser-0 to the committed offset FetchPosition{offset=1, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:07:28,169 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-33, groupId=group_id] Setting offset for partition createStock-0 to the committed offset FetchPosition{offset=3, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:07:28,169 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] group_id: partitions assigned: [createUser-0]
2022-06-24 14:07:28,169 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] group_id: partitions assigned: [createStock-0]
2022-06-24 14:07:28,169 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-36, groupId=group_id] (Re-)joining group
2022-06-24 14:07:28,172 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka version: 2.7.1
2022-06-24 14:07:28,172 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka commitId: 61dbce85d0d41457
2022-06-24 14:07:28,172 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-36, groupId=group_id] (Re-)joining group
2022-06-24 14:07:28,172 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka startTimeMs: 1656059848172
2022-06-24 14:07:28,173 INFO org.apache.kafka.clients.consumer.KafkaConsumer [restartedMain] [Consumer clientId=consumer-group_id-37, groupId=group_id] Subscribed to topic(s): removeStock
2022-06-24 14:07:28,176 INFO org.apache.kafka.common.config.AbstractConfig [restartedMain] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-group_id-38
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = group_id
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2022-06-24 14:07:28,177 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-37, groupId=group_id] Cluster ID: 5kpO7pNKTFqroM6YwsJ86A
2022-06-24 14:07:28,178 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-37, groupId=group_id] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2022-06-24 14:07:28,178 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-37, groupId=group_id] (Re-)joining group
2022-06-24 14:07:28,181 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-37, groupId=group_id] (Re-)joining group
2022-06-24 14:07:28,182 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka version: 2.7.1
2022-06-24 14:07:28,182 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka commitId: 61dbce85d0d41457
2022-06-24 14:07:28,182 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka startTimeMs: 1656059848182
2022-06-24 14:07:28,183 INFO org.apache.kafka.clients.consumer.KafkaConsumer [restartedMain] [Consumer clientId=consumer-group_id-38, groupId=group_id] Subscribed to topic(s): updateStock
2022-06-24 14:07:28,186 INFO org.apache.kafka.common.config.AbstractConfig [restartedMain] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-group_id-39
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = group_id
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2022-06-24 14:07:28,188 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-38, groupId=group_id] Cluster ID: 5kpO7pNKTFqroM6YwsJ86A
2022-06-24 14:07:28,188 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-38, groupId=group_id] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2022-06-24 14:07:28,190 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-38, groupId=group_id] (Re-)joining group
2022-06-24 14:07:28,190 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka version: 2.7.1
2022-06-24 14:07:28,190 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka commitId: 61dbce85d0d41457
2022-06-24 14:07:28,190 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka startTimeMs: 1656059848190
2022-06-24 14:07:28,191 INFO org.apache.kafka.clients.consumer.KafkaConsumer [restartedMain] [Consumer clientId=consumer-group_id-39, groupId=group_id] Subscribed to topic(s): removeCompany
2022-06-24 14:07:28,193 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-38, groupId=group_id] (Re-)joining group
2022-06-24 14:07:28,193 INFO org.apache.kafka.common.config.AbstractConfig [restartedMain] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-group_id-40
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = group_id
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2022-06-24 14:07:28,195 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-39, groupId=group_id] Cluster ID: 5kpO7pNKTFqroM6YwsJ86A
2022-06-24 14:07:28,195 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-39, groupId=group_id] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2022-06-24 14:07:28,197 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-39, groupId=group_id] (Re-)joining group
2022-06-24 14:07:28,198 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka version: 2.7.1
2022-06-24 14:07:28,198 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka commitId: 61dbce85d0d41457
2022-06-24 14:07:28,198 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka startTimeMs: 1656059848198
2022-06-24 14:07:28,199 INFO org.apache.kafka.clients.consumer.KafkaConsumer [restartedMain] [Consumer clientId=consumer-group_id-40, groupId=group_id] Subscribed to topic(s): createCompany
2022-06-24 14:07:28,199 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-39, groupId=group_id] (Re-)joining group
2022-06-24 14:07:28,200 INFO org.apache.juli.logging.DirectJDKLog [restartedMain] Starting ProtocolHandler ["http-nio-6093"]
2022-06-24 14:07:28,203 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-40, groupId=group_id] Cluster ID: 5kpO7pNKTFqroM6YwsJ86A
2022-06-24 14:07:28,203 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-40, groupId=group_id] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2022-06-24 14:07:28,203 INFO org.springframework.boot.web.embedded.tomcat.TomcatWebServer [restartedMain] Tomcat started on port(s): 6093 (http) with context path ''
2022-06-24 14:07:28,204 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-40, groupId=group_id] (Re-)joining group
2022-06-24 14:07:28,204 INFO org.springframework.cloud.netflix.eureka.serviceregistry.EurekaAutoServiceRegistration [restartedMain] Updating port to 6093
2022-06-24 14:07:28,206 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-40, groupId=group_id] (Re-)joining group
2022-06-24 14:07:28,239 INFO springfox.documentation.spring.web.plugins.DocumentationPluginsBootstrapper [restartedMain] Context refreshed
2022-06-24 14:07:28,241 INFO springfox.documentation.spring.web.plugins.DocumentationPluginsBootstrapper [restartedMain] Found 1 custom documentation plugin(s)
2022-06-24 14:07:28,245 INFO springfox.documentation.spring.web.scanners.ApiListingReferenceScanner [restartedMain] Scanning for api listing references
2022-06-24 14:07:28,288 INFO org.springframework.boot.StartupInfoLogger [restartedMain] Started EstockmarketQueryApplication in 2.645 seconds (JVM running for 53562.597)
2022-06-24 14:07:28,291 INFO org.springframework.boot.devtools.autoconfigure.ConditionEvaluationDeltaLoggingListener [restartedMain] Condition evaluation unchanged
2022-06-24 14:07:31,163 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-33, groupId=group_id] Attempt to heartbeat failed since group is rebalancing
2022-06-24 14:07:31,163 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-34, groupId=group_id] Attempt to heartbeat failed since group is rebalancing
2022-06-24 14:07:31,164 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-33, groupId=group_id] Revoke previously assigned partitions createStock-0
2022-06-24 14:07:31,164 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-34, groupId=group_id] Revoke previously assigned partitions createUser-0
2022-06-24 14:07:31,164 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] group_id: partitions revoked: [createStock-0]
2022-06-24 14:07:31,164 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] group_id: partitions revoked: [createUser-0]
2022-06-24 14:07:31,164 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-33, groupId=group_id] (Re-)joining group
2022-06-24 14:07:31,164 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-34, groupId=group_id] (Re-)joining group
2022-06-24 14:07:31,166 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-40, groupId=group_id] Successfully joined group with generation Generation{generationId=88, memberId='consumer-group_id-40-94844b20-704c-4377-a3a5-8469cef32a15', protocol='range'}
2022-06-24 14:07:31,167 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-37, groupId=group_id] Successfully joined group with generation Generation{generationId=88, memberId='consumer-group_id-37-fd4938eb-49cd-4489-8c45-b7854a0cea72', protocol='range'}
2022-06-24 14:07:31,167 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-38, groupId=group_id] Successfully joined group with generation Generation{generationId=88, memberId='consumer-group_id-38-2ad0175a-0f6f-483c-acb0-07f9b75b796a', protocol='range'}
2022-06-24 14:07:31,167 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-39, groupId=group_id] Successfully joined group with generation Generation{generationId=88, memberId='consumer-group_id-39-a370e00f-d4c3-4a4c-9fe0-05bafdc4e555', protocol='range'}
2022-06-24 14:07:31,167 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-36, groupId=group_id] Successfully joined group with generation Generation{generationId=88, memberId='consumer-group_id-36-6f53a0e8-8fba-4a3a-b3ab-5d2cf2d241d1', protocol='range'}
2022-06-24 14:07:31,166 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-33, groupId=group_id] Successfully joined group with generation Generation{generationId=88, memberId='consumer-group_id-33-7aab9548-85d4-4e3f-8eab-8ffb956f9139', protocol='range'}
2022-06-24 14:07:31,167 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-34, groupId=group_id] Successfully joined group with generation Generation{generationId=88, memberId='consumer-group_id-34-d7487a31-12cf-48b7-9c66-9fd7495adcce', protocol='range'}
2022-06-24 14:07:31,167 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-35, groupId=group_id] Successfully joined group with generation Generation{generationId=88, memberId='consumer-group_id-35-81b5b402-e8dc-4318-800f-096f8f4e31f4', protocol='range'}
2022-06-24 14:07:31,243 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-33, groupId=group_id] Finished assignment for group at generation 88: {consumer-group_id-33-7aab9548-85d4-4e3f-8eab-8ffb956f9139=Assignment(partitions=[createStock-0]), consumer-group_id-37-fd4938eb-49cd-4489-8c45-b7854a0cea72=Assignment(partitions=[removeStock-0]), consumer-group_id-36-6f53a0e8-8fba-4a3a-b3ab-5d2cf2d241d1=Assignment(partitions=[createSector-0]), consumer-group_id-35-81b5b402-e8dc-4318-800f-096f8f4e31f4=Assignment(partitions=[removeSector-0]), consumer-group_id-39-a370e00f-d4c3-4a4c-9fe0-05bafdc4e555=Assignment(partitions=[removeCompany-0]), consumer-group_id-40-94844b20-704c-4377-a3a5-8469cef32a15=Assignment(partitions=[createCompany-0]), consumer-group_id-34-d7487a31-12cf-48b7-9c66-9fd7495adcce=Assignment(partitions=[createUser-0]), consumer-group_id-38-2ad0175a-0f6f-483c-acb0-07f9b75b796a=Assignment(partitions=[updateStock-0])}
2022-06-24 14:07:31,246 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-33, groupId=group_id] Successfully synced group in generation Generation{generationId=88, memberId='consumer-group_id-33-7aab9548-85d4-4e3f-8eab-8ffb956f9139', protocol='range'}
2022-06-24 14:07:31,246 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-35, groupId=group_id] Successfully synced group in generation Generation{generationId=88, memberId='consumer-group_id-35-81b5b402-e8dc-4318-800f-096f8f4e31f4', protocol='range'}
2022-06-24 14:07:31,246 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-34, groupId=group_id] Successfully synced group in generation Generation{generationId=88, memberId='consumer-group_id-34-d7487a31-12cf-48b7-9c66-9fd7495adcce', protocol='range'}
2022-06-24 14:07:31,248 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-33, groupId=group_id] Notifying assignor about the new Assignment(partitions=[createStock-0])
2022-06-24 14:07:31,248 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-35, groupId=group_id] Notifying assignor about the new Assignment(partitions=[removeSector-0])
2022-06-24 14:07:31,248 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-33, groupId=group_id] Adding newly assigned partitions: createStock-0
2022-06-24 14:07:31,248 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-34, groupId=group_id] Notifying assignor about the new Assignment(partitions=[createUser-0])
2022-06-24 14:07:31,249 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-35, groupId=group_id] Adding newly assigned partitions: removeSector-0
2022-06-24 14:07:31,250 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-34, groupId=group_id] Adding newly assigned partitions: createUser-0
2022-06-24 14:07:31,250 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-38, groupId=group_id] Successfully synced group in generation Generation{generationId=88, memberId='consumer-group_id-38-2ad0175a-0f6f-483c-acb0-07f9b75b796a', protocol='range'}
2022-06-24 14:07:31,250 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-39, groupId=group_id] Successfully synced group in generation Generation{generationId=88, memberId='consumer-group_id-39-a370e00f-d4c3-4a4c-9fe0-05bafdc4e555', protocol='range'}
2022-06-24 14:07:31,250 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-37, groupId=group_id] Successfully synced group in generation Generation{generationId=88, memberId='consumer-group_id-37-fd4938eb-49cd-4489-8c45-b7854a0cea72', protocol='range'}
2022-06-24 14:07:31,251 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-38, groupId=group_id] Notifying assignor about the new Assignment(partitions=[updateStock-0])
2022-06-24 14:07:31,251 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-40, groupId=group_id] Successfully synced group in generation Generation{generationId=88, memberId='consumer-group_id-40-94844b20-704c-4377-a3a5-8469cef32a15', protocol='range'}
2022-06-24 14:07:31,251 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-38, groupId=group_id] Adding newly assigned partitions: updateStock-0
2022-06-24 14:07:31,251 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-37, groupId=group_id] Notifying assignor about the new Assignment(partitions=[removeStock-0])
2022-06-24 14:07:31,251 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-39, groupId=group_id] Notifying assignor about the new Assignment(partitions=[removeCompany-0])
2022-06-24 14:07:31,251 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-37, groupId=group_id] Adding newly assigned partitions: removeStock-0
2022-06-24 14:07:31,251 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-39, groupId=group_id] Adding newly assigned partitions: removeCompany-0
2022-06-24 14:07:31,251 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-40, groupId=group_id] Notifying assignor about the new Assignment(partitions=[createCompany-0])
2022-06-24 14:07:31,251 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-40, groupId=group_id] Adding newly assigned partitions: createCompany-0
2022-06-24 14:07:31,251 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-36, groupId=group_id] Successfully synced group in generation Generation{generationId=88, memberId='consumer-group_id-36-6f53a0e8-8fba-4a3a-b3ab-5d2cf2d241d1', protocol='range'}
2022-06-24 14:07:31,251 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-36, groupId=group_id] Notifying assignor about the new Assignment(partitions=[createSector-0])
2022-06-24 14:07:31,251 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-36, groupId=group_id] Adding newly assigned partitions: createSector-0
2022-06-24 14:07:31,252 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-33, groupId=group_id] Setting offset for partition createStock-0 to the committed offset FetchPosition{offset=3, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:07:31,252 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] group_id: partitions assigned: [createStock-0]
2022-06-24 14:07:31,252 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-35, groupId=group_id] Setting offset for partition removeSector-0 to the committed offset FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:07:31,252 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-34, groupId=group_id] Setting offset for partition createUser-0 to the committed offset FetchPosition{offset=1, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:07:31,252 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-37, groupId=group_id] Setting offset for partition removeStock-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:07:31,253 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] group_id: partitions assigned: [removeSector-0]
2022-06-24 14:07:31,253 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] group_id: partitions assigned: [createUser-0]
2022-06-24 14:07:31,253 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] group_id: partitions assigned: [removeStock-0]
2022-06-24 14:07:31,253 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-39, groupId=group_id] Setting offset for partition removeCompany-0 to the committed offset FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:07:31,253 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] group_id: partitions assigned: [removeCompany-0]
2022-06-24 14:07:31,253 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-40, groupId=group_id] Setting offset for partition createCompany-0 to the committed offset FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:07:31,253 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-38, groupId=group_id] Setting offset for partition updateStock-0 to the committed offset FetchPosition{offset=6, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:07:31,253 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] group_id: partitions assigned: [createCompany-0]
2022-06-24 14:07:31,254 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] group_id: partitions assigned: [updateStock-0]
2022-06-24 14:07:31,254 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-36, groupId=group_id] Setting offset for partition createSector-0 to the committed offset FetchPosition{offset=1, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:07:31,254 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] group_id: partitions assigned: [createSector-0]
2022-06-24 14:07:36,902 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks C-008
2022-06-24 14:07:36,976 INFO com.mongodb.diagnostics.logging.SLF4JLogger [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Opened connection [connectionId{localValue:23, serverValue:61}] to localhost:27017
2022-06-24 14:07:37,037 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-38, groupId=group_id] Seeking to offset 6 for partition updateStock-0
2022-06-24 14:07:37,038 ERROR org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception; nested exception is org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:206)
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:112)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2359)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2228)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2142)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2024)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1706)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2371)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2342)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2303)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2217)
	... 9 common frames omitted
Caused by: org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.data.mongodb.core.MongoExceptionTranslator.translateExceptionIfPossible(MongoExceptionTranslator.java:106)
	at org.springframework.data.mongodb.core.MongoTemplate.potentiallyConvertRuntimeException(MongoTemplate.java:2899)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:555)
	at org.springframework.data.mongodb.core.MongoTemplate.insertDocument(MongoTemplate.java:1458)
	at org.springframework.data.mongodb.core.MongoTemplate.doInsert(MongoTemplate.java:1257)
	at org.springframework.data.mongodb.core.MongoTemplate.insert(MongoTemplate.java:1172)
	at org.springframework.data.mongodb.repository.support.SimpleMongoRepository.save(SimpleMongoRepository.java:91)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:289)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:137)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:121)
	at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:529)
	at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:285)
	at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:599)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:163)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:138)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:80)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy174.save(Unknown Source)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:344)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:198)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:137)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy174.save(Unknown Source)
	at com.estockmarket.query.domain.service.StockService.updateStock(StockService.java:105)
	at com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(KafkaStocksEventListener.java:36)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:56)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:347)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:92)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:53)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2322)
	... 11 common frames omitted
Caused by: com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at com.mongodb.client.internal.MongoCollectionImpl.executeSingleWriteRequest(MongoCollectionImpl.java:1017)
	at com.mongodb.client.internal.MongoCollectionImpl.executeInsertOne(MongoCollectionImpl.java:470)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:453)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:447)
	at org.springframework.data.mongodb.core.MongoTemplate.lambda$insertDocument$16(MongoTemplate.java:1464)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:553)
	... 59 common frames omitted
2022-06-24 14:07:37,492 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks C-008
2022-06-24 14:07:37,509 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-38, groupId=group_id] Seeking to offset 6 for partition updateStock-0
2022-06-24 14:07:37,509 ERROR org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception; nested exception is org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:206)
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:112)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2359)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2228)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2142)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2024)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1706)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2371)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2342)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2303)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2217)
	... 9 common frames omitted
Caused by: org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.data.mongodb.core.MongoExceptionTranslator.translateExceptionIfPossible(MongoExceptionTranslator.java:106)
	at org.springframework.data.mongodb.core.MongoTemplate.potentiallyConvertRuntimeException(MongoTemplate.java:2899)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:555)
	at org.springframework.data.mongodb.core.MongoTemplate.insertDocument(MongoTemplate.java:1458)
	at org.springframework.data.mongodb.core.MongoTemplate.doInsert(MongoTemplate.java:1257)
	at org.springframework.data.mongodb.core.MongoTemplate.insert(MongoTemplate.java:1172)
	at org.springframework.data.mongodb.repository.support.SimpleMongoRepository.save(SimpleMongoRepository.java:91)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:289)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:137)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:121)
	at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:529)
	at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:285)
	at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:599)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:163)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:138)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:80)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy174.save(Unknown Source)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:344)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:198)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:137)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy174.save(Unknown Source)
	at com.estockmarket.query.domain.service.StockService.updateStock(StockService.java:105)
	at com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(KafkaStocksEventListener.java:36)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:56)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:347)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:92)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:53)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2322)
	... 11 common frames omitted
Caused by: com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at com.mongodb.client.internal.MongoCollectionImpl.executeSingleWriteRequest(MongoCollectionImpl.java:1017)
	at com.mongodb.client.internal.MongoCollectionImpl.executeInsertOne(MongoCollectionImpl.java:470)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:453)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:447)
	at org.springframework.data.mongodb.core.MongoTemplate.lambda$insertDocument$16(MongoTemplate.java:1464)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:553)
	... 59 common frames omitted
2022-06-24 14:07:38,023 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks C-008
2022-06-24 14:07:38,052 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-38, groupId=group_id] Seeking to offset 6 for partition updateStock-0
2022-06-24 14:07:38,052 ERROR org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception; nested exception is org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:206)
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:112)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2359)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2228)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2142)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2024)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1706)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2371)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2342)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2303)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2217)
	... 9 common frames omitted
Caused by: org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.data.mongodb.core.MongoExceptionTranslator.translateExceptionIfPossible(MongoExceptionTranslator.java:106)
	at org.springframework.data.mongodb.core.MongoTemplate.potentiallyConvertRuntimeException(MongoTemplate.java:2899)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:555)
	at org.springframework.data.mongodb.core.MongoTemplate.insertDocument(MongoTemplate.java:1458)
	at org.springframework.data.mongodb.core.MongoTemplate.doInsert(MongoTemplate.java:1257)
	at org.springframework.data.mongodb.core.MongoTemplate.insert(MongoTemplate.java:1172)
	at org.springframework.data.mongodb.repository.support.SimpleMongoRepository.save(SimpleMongoRepository.java:91)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:289)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:137)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:121)
	at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:529)
	at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:285)
	at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:599)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:163)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:138)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:80)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy174.save(Unknown Source)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:344)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:198)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:137)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy174.save(Unknown Source)
	at com.estockmarket.query.domain.service.StockService.updateStock(StockService.java:105)
	at com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(KafkaStocksEventListener.java:36)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:56)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:347)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:92)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:53)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2322)
	... 11 common frames omitted
Caused by: com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at com.mongodb.client.internal.MongoCollectionImpl.executeSingleWriteRequest(MongoCollectionImpl.java:1017)
	at com.mongodb.client.internal.MongoCollectionImpl.executeInsertOne(MongoCollectionImpl.java:470)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:453)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:447)
	at org.springframework.data.mongodb.core.MongoTemplate.lambda$insertDocument$16(MongoTemplate.java:1464)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:553)
	... 59 common frames omitted
2022-06-24 14:07:38,567 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks C-008
2022-06-24 14:07:38,595 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-38, groupId=group_id] Seeking to offset 6 for partition updateStock-0
2022-06-24 14:07:38,595 ERROR org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception; nested exception is org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:206)
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:112)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2359)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2228)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2142)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2024)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1706)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2371)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2342)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2303)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2217)
	... 9 common frames omitted
Caused by: org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.data.mongodb.core.MongoExceptionTranslator.translateExceptionIfPossible(MongoExceptionTranslator.java:106)
	at org.springframework.data.mongodb.core.MongoTemplate.potentiallyConvertRuntimeException(MongoTemplate.java:2899)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:555)
	at org.springframework.data.mongodb.core.MongoTemplate.insertDocument(MongoTemplate.java:1458)
	at org.springframework.data.mongodb.core.MongoTemplate.doInsert(MongoTemplate.java:1257)
	at org.springframework.data.mongodb.core.MongoTemplate.insert(MongoTemplate.java:1172)
	at org.springframework.data.mongodb.repository.support.SimpleMongoRepository.save(SimpleMongoRepository.java:91)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:289)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:137)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:121)
	at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:529)
	at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:285)
	at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:599)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:163)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:138)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:80)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy174.save(Unknown Source)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:344)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:198)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:137)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy174.save(Unknown Source)
	at com.estockmarket.query.domain.service.StockService.updateStock(StockService.java:105)
	at com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(KafkaStocksEventListener.java:36)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:56)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:347)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:92)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:53)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2322)
	... 11 common frames omitted
Caused by: com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at com.mongodb.client.internal.MongoCollectionImpl.executeSingleWriteRequest(MongoCollectionImpl.java:1017)
	at com.mongodb.client.internal.MongoCollectionImpl.executeInsertOne(MongoCollectionImpl.java:470)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:453)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:447)
	at org.springframework.data.mongodb.core.MongoTemplate.lambda$insertDocument$16(MongoTemplate.java:1464)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:553)
	... 59 common frames omitted
2022-06-24 14:07:39,113 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks C-008
2022-06-24 14:07:39,142 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-38, groupId=group_id] Seeking to offset 6 for partition updateStock-0
2022-06-24 14:07:39,142 ERROR org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception; nested exception is org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:206)
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:112)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2359)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2228)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2142)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2024)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1706)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2371)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2342)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2303)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2217)
	... 9 common frames omitted
Caused by: org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.data.mongodb.core.MongoExceptionTranslator.translateExceptionIfPossible(MongoExceptionTranslator.java:106)
	at org.springframework.data.mongodb.core.MongoTemplate.potentiallyConvertRuntimeException(MongoTemplate.java:2899)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:555)
	at org.springframework.data.mongodb.core.MongoTemplate.insertDocument(MongoTemplate.java:1458)
	at org.springframework.data.mongodb.core.MongoTemplate.doInsert(MongoTemplate.java:1257)
	at org.springframework.data.mongodb.core.MongoTemplate.insert(MongoTemplate.java:1172)
	at org.springframework.data.mongodb.repository.support.SimpleMongoRepository.save(SimpleMongoRepository.java:91)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:289)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:137)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:121)
	at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:529)
	at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:285)
	at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:599)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:163)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:138)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:80)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy174.save(Unknown Source)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:344)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:198)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:137)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy174.save(Unknown Source)
	at com.estockmarket.query.domain.service.StockService.updateStock(StockService.java:105)
	at com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(KafkaStocksEventListener.java:36)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:56)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:347)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:92)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:53)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2322)
	... 11 common frames omitted
Caused by: com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at com.mongodb.client.internal.MongoCollectionImpl.executeSingleWriteRequest(MongoCollectionImpl.java:1017)
	at com.mongodb.client.internal.MongoCollectionImpl.executeInsertOne(MongoCollectionImpl.java:470)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:453)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:447)
	at org.springframework.data.mongodb.core.MongoTemplate.lambda$insertDocument$16(MongoTemplate.java:1464)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:553)
	... 59 common frames omitted
2022-06-24 14:07:39,655 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks C-008
2022-06-24 14:07:39,683 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-38, groupId=group_id] Seeking to offset 6 for partition updateStock-0
2022-06-24 14:07:39,683 ERROR org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception; nested exception is org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:206)
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:112)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2359)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2228)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2142)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2024)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1706)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2371)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2342)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2303)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2217)
	... 9 common frames omitted
Caused by: org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.data.mongodb.core.MongoExceptionTranslator.translateExceptionIfPossible(MongoExceptionTranslator.java:106)
	at org.springframework.data.mongodb.core.MongoTemplate.potentiallyConvertRuntimeException(MongoTemplate.java:2899)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:555)
	at org.springframework.data.mongodb.core.MongoTemplate.insertDocument(MongoTemplate.java:1458)
	at org.springframework.data.mongodb.core.MongoTemplate.doInsert(MongoTemplate.java:1257)
	at org.springframework.data.mongodb.core.MongoTemplate.insert(MongoTemplate.java:1172)
	at org.springframework.data.mongodb.repository.support.SimpleMongoRepository.save(SimpleMongoRepository.java:91)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:289)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:137)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:121)
	at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:529)
	at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:285)
	at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:599)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:163)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:138)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:80)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy174.save(Unknown Source)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:344)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:198)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:137)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy174.save(Unknown Source)
	at com.estockmarket.query.domain.service.StockService.updateStock(StockService.java:105)
	at com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(KafkaStocksEventListener.java:36)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:56)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:347)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:92)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:53)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2322)
	... 11 common frames omitted
Caused by: com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at com.mongodb.client.internal.MongoCollectionImpl.executeSingleWriteRequest(MongoCollectionImpl.java:1017)
	at com.mongodb.client.internal.MongoCollectionImpl.executeInsertOne(MongoCollectionImpl.java:470)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:453)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:447)
	at org.springframework.data.mongodb.core.MongoTemplate.lambda$insertDocument$16(MongoTemplate.java:1464)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:553)
	... 59 common frames omitted
2022-06-24 14:07:40,199 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks C-008
2022-06-24 14:07:40,229 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-38, groupId=group_id] Seeking to offset 6 for partition updateStock-0
2022-06-24 14:07:40,229 ERROR org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception; nested exception is org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:206)
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:112)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2359)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2228)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2142)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2024)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1706)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2371)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2342)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2303)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2217)
	... 9 common frames omitted
Caused by: org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.data.mongodb.core.MongoExceptionTranslator.translateExceptionIfPossible(MongoExceptionTranslator.java:106)
	at org.springframework.data.mongodb.core.MongoTemplate.potentiallyConvertRuntimeException(MongoTemplate.java:2899)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:555)
	at org.springframework.data.mongodb.core.MongoTemplate.insertDocument(MongoTemplate.java:1458)
	at org.springframework.data.mongodb.core.MongoTemplate.doInsert(MongoTemplate.java:1257)
	at org.springframework.data.mongodb.core.MongoTemplate.insert(MongoTemplate.java:1172)
	at org.springframework.data.mongodb.repository.support.SimpleMongoRepository.save(SimpleMongoRepository.java:91)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:289)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:137)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:121)
	at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:529)
	at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:285)
	at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:599)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:163)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:138)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:80)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy174.save(Unknown Source)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:344)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:198)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:137)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy174.save(Unknown Source)
	at com.estockmarket.query.domain.service.StockService.updateStock(StockService.java:105)
	at com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(KafkaStocksEventListener.java:36)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:56)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:347)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:92)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:53)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2322)
	... 11 common frames omitted
Caused by: com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at com.mongodb.client.internal.MongoCollectionImpl.executeSingleWriteRequest(MongoCollectionImpl.java:1017)
	at com.mongodb.client.internal.MongoCollectionImpl.executeInsertOne(MongoCollectionImpl.java:470)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:453)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:447)
	at org.springframework.data.mongodb.core.MongoTemplate.lambda$insertDocument$16(MongoTemplate.java:1464)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:553)
	... 59 common frames omitted
2022-06-24 14:07:40,743 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks C-008
2022-06-24 14:07:40,771 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-38, groupId=group_id] Seeking to offset 6 for partition updateStock-0
2022-06-24 14:07:40,772 ERROR org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception; nested exception is org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:206)
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:112)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2359)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2228)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2142)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2024)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1706)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2371)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2342)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2303)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2217)
	... 9 common frames omitted
Caused by: org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.data.mongodb.core.MongoExceptionTranslator.translateExceptionIfPossible(MongoExceptionTranslator.java:106)
	at org.springframework.data.mongodb.core.MongoTemplate.potentiallyConvertRuntimeException(MongoTemplate.java:2899)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:555)
	at org.springframework.data.mongodb.core.MongoTemplate.insertDocument(MongoTemplate.java:1458)
	at org.springframework.data.mongodb.core.MongoTemplate.doInsert(MongoTemplate.java:1257)
	at org.springframework.data.mongodb.core.MongoTemplate.insert(MongoTemplate.java:1172)
	at org.springframework.data.mongodb.repository.support.SimpleMongoRepository.save(SimpleMongoRepository.java:91)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:289)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:137)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:121)
	at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:529)
	at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:285)
	at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:599)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:163)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:138)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:80)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy174.save(Unknown Source)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:344)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:198)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:137)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy174.save(Unknown Source)
	at com.estockmarket.query.domain.service.StockService.updateStock(StockService.java:105)
	at com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(KafkaStocksEventListener.java:36)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:56)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:347)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:92)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:53)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2322)
	... 11 common frames omitted
Caused by: com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at com.mongodb.client.internal.MongoCollectionImpl.executeSingleWriteRequest(MongoCollectionImpl.java:1017)
	at com.mongodb.client.internal.MongoCollectionImpl.executeInsertOne(MongoCollectionImpl.java:470)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:453)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:447)
	at org.springframework.data.mongodb.core.MongoTemplate.lambda$insertDocument$16(MongoTemplate.java:1464)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:553)
	... 59 common frames omitted
2022-06-24 14:07:41,287 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks C-008
2022-06-24 14:07:41,315 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-38, groupId=group_id] Seeking to offset 6 for partition updateStock-0
2022-06-24 14:07:41,315 ERROR org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception; nested exception is org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:206)
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:112)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2359)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2228)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2142)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2024)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1706)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2371)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2342)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2303)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2217)
	... 9 common frames omitted
Caused by: org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.data.mongodb.core.MongoExceptionTranslator.translateExceptionIfPossible(MongoExceptionTranslator.java:106)
	at org.springframework.data.mongodb.core.MongoTemplate.potentiallyConvertRuntimeException(MongoTemplate.java:2899)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:555)
	at org.springframework.data.mongodb.core.MongoTemplate.insertDocument(MongoTemplate.java:1458)
	at org.springframework.data.mongodb.core.MongoTemplate.doInsert(MongoTemplate.java:1257)
	at org.springframework.data.mongodb.core.MongoTemplate.insert(MongoTemplate.java:1172)
	at org.springframework.data.mongodb.repository.support.SimpleMongoRepository.save(SimpleMongoRepository.java:91)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:289)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:137)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:121)
	at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:529)
	at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:285)
	at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:599)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:163)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:138)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:80)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy174.save(Unknown Source)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:344)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:198)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:137)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy174.save(Unknown Source)
	at com.estockmarket.query.domain.service.StockService.updateStock(StockService.java:105)
	at com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(KafkaStocksEventListener.java:36)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:56)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:347)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:92)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:53)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2322)
	... 11 common frames omitted
Caused by: com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at com.mongodb.client.internal.MongoCollectionImpl.executeSingleWriteRequest(MongoCollectionImpl.java:1017)
	at com.mongodb.client.internal.MongoCollectionImpl.executeInsertOne(MongoCollectionImpl.java:470)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:453)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:447)
	at org.springframework.data.mongodb.core.MongoTemplate.lambda$insertDocument$16(MongoTemplate.java:1464)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:553)
	... 59 common frames omitted
2022-06-24 14:07:41,828 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks C-008
2022-06-24 14:07:41,833 ERROR org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Backoff FixedBackOff{interval=0, currentAttempts=10, maxAttempts=9} exhausted for updateStock-0@6
org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(java.lang.String) throws java.lang.Exception' threw exception; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2371)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2342)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2303)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2217)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2142)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2024)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1706)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.springframework.dao.DuplicateKeyException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }; nested exception is com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at org.springframework.data.mongodb.core.MongoExceptionTranslator.translateExceptionIfPossible(MongoExceptionTranslator.java:106)
	at org.springframework.data.mongodb.core.MongoTemplate.potentiallyConvertRuntimeException(MongoTemplate.java:2899)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:555)
	at org.springframework.data.mongodb.core.MongoTemplate.insertDocument(MongoTemplate.java:1458)
	at org.springframework.data.mongodb.core.MongoTemplate.doInsert(MongoTemplate.java:1257)
	at org.springframework.data.mongodb.core.MongoTemplate.insert(MongoTemplate.java:1172)
	at org.springframework.data.mongodb.repository.support.SimpleMongoRepository.save(SimpleMongoRepository.java:91)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:289)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:137)
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:121)
	at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:529)
	at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:285)
	at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:599)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:163)
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:138)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:80)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy174.save(Unknown Source)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:344)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:198)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:137)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
	at com.sun.proxy.$Proxy174.save(Unknown Source)
	at com.estockmarket.query.domain.service.StockService.updateStock(StockService.java:105)
	at com.estockmarket.query.infrastructure.eventsourcing.KafkaStocksEventListener.updateStock(KafkaStocksEventListener.java:36)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:56)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:347)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:92)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:53)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2322)
	... 11 common frames omitted
Caused by: com.mongodb.MongoWriteException: E11000 duplicate key error collection: estockmarket.stocks index: _id_ dup key: { _id: 23 }
	at com.mongodb.client.internal.MongoCollectionImpl.executeSingleWriteRequest(MongoCollectionImpl.java:1017)
	at com.mongodb.client.internal.MongoCollectionImpl.executeInsertOne(MongoCollectionImpl.java:470)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:453)
	at com.mongodb.client.internal.MongoCollectionImpl.insertOne(MongoCollectionImpl.java:447)
	at org.springframework.data.mongodb.core.MongoTemplate.lambda$insertDocument$16(MongoTemplate.java:1464)
	at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:553)
	... 59 common frames omitted
2022-06-24 14:08:11,282 INFO org.springframework.cloud.netflix.eureka.serviceregistry.EurekaServiceRegistry [Thread-27] Unregistering application ESTOCKMARKET-QUERY with eureka with status DOWN
2022-06-24 14:08:11,282 INFO com.netflix.discovery.DiscoveryClient$3 [Thread-27] Saw local status change event StatusChangeEvent [timestamp=1656059891282, current=DOWN, previous=UP]
2022-06-24 14:08:11,283 INFO com.netflix.discovery.DiscoveryClient [DiscoveryClient-InstanceInfoReplicator-0] DiscoveryClient_ESTOCKMARKET-QUERY/host.docker.internal:estockmarket-query:6093: registering service...
2022-06-24 14:08:11,295 INFO com.netflix.discovery.DiscoveryClient [DiscoveryClient-InstanceInfoReplicator-0] DiscoveryClient_ESTOCKMARKET-QUERY/host.docker.internal:estockmarket-query:6093 - registration status: 204
2022-06-24 14:08:11,326 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-33, groupId=group_id] Revoke previously assigned partitions createStock-0
2022-06-24 14:08:11,326 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-34, groupId=group_id] Revoke previously assigned partitions createUser-0
2022-06-24 14:08:11,326 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-39, groupId=group_id] Revoke previously assigned partitions removeCompany-0
2022-06-24 14:08:11,326 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-40, groupId=group_id] Revoke previously assigned partitions createCompany-0
2022-06-24 14:08:11,326 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] group_id: partitions revoked: [removeCompany-0]
2022-06-24 14:08:11,326 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] group_id: partitions revoked: [createUser-0]
2022-06-24 14:08:11,326 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] group_id: partitions revoked: [createCompany-0]
2022-06-24 14:08:11,326 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-38, groupId=group_id] Revoke previously assigned partitions updateStock-0
2022-06-24 14:08:11,326 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-37, groupId=group_id] Revoke previously assigned partitions removeStock-0
2022-06-24 14:08:11,326 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-36, groupId=group_id] Revoke previously assigned partitions createSector-0
2022-06-24 14:08:11,326 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] group_id: partitions revoked: [createStock-0]
2022-06-24 14:08:11,326 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] group_id: partitions revoked: [removeStock-0]
2022-06-24 14:08:11,326 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-34, groupId=group_id] Member consumer-group_id-34-d7487a31-12cf-48b7-9c66-9fd7495adcce sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2022-06-24 14:08:11,327 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-33, groupId=group_id] Member consumer-group_id-33-7aab9548-85d4-4e3f-8eab-8ffb956f9139 sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2022-06-24 14:08:11,327 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-37, groupId=group_id] Member consumer-group_id-37-fd4938eb-49cd-4489-8c45-b7854a0cea72 sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2022-06-24 14:08:11,326 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-39, groupId=group_id] Member consumer-group_id-39-a370e00f-d4c3-4a4c-9fe0-05bafdc4e555 sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2022-06-24 14:08:11,327 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-33, groupId=group_id] Unsubscribed all topics or patterns and assigned partitions
2022-06-24 14:08:11,326 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-35, groupId=group_id] Revoke previously assigned partitions removeSector-0
2022-06-24 14:08:11,326 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] group_id: partitions revoked: [updateStock-0]
2022-06-24 14:08:11,326 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] group_id: partitions revoked: [createSector-0]
2022-06-24 14:08:11,326 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-40, groupId=group_id] Member consumer-group_id-40-94844b20-704c-4377-a3a5-8469cef32a15 sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2022-06-24 14:08:11,327 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-37, groupId=group_id] Unsubscribed all topics or patterns and assigned partitions
2022-06-24 14:08:11,327 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] group_id: partitions revoked: [removeSector-0]
2022-06-24 14:08:11,327 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-38, groupId=group_id] Member consumer-group_id-38-2ad0175a-0f6f-483c-acb0-07f9b75b796a sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2022-06-24 14:08:11,327 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-34, groupId=group_id] Unsubscribed all topics or patterns and assigned partitions
2022-06-24 14:08:11,327 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-35, groupId=group_id] Member consumer-group_id-35-81b5b402-e8dc-4318-800f-096f8f4e31f4 sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2022-06-24 14:08:11,327 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-38, groupId=group_id] Unsubscribed all topics or patterns and assigned partitions
2022-06-24 14:08:11,328 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-35, groupId=group_id] Unsubscribed all topics or patterns and assigned partitions
2022-06-24 14:08:11,327 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-36, groupId=group_id] Member consumer-group_id-36-6f53a0e8-8fba-4a3a-b3ab-5d2cf2d241d1 sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2022-06-24 14:08:11,327 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-39, groupId=group_id] Unsubscribed all topics or patterns and assigned partitions
2022-06-24 14:08:11,327 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-40, groupId=group_id] Unsubscribed all topics or patterns and assigned partitions
2022-06-24 14:08:11,328 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-36, groupId=group_id] Unsubscribed all topics or patterns and assigned partitions
2022-06-24 14:08:11,332 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] Metrics scheduler closed
2022-06-24 14:08:11,333 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-06-24 14:08:11,333 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] Metrics reporters closed
2022-06-24 14:08:11,333 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] Metrics scheduler closed
2022-06-24 14:08:11,333 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-06-24 14:08:11,333 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] Metrics reporters closed
2022-06-24 14:08:11,335 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Metrics scheduler closed
2022-06-24 14:08:11,335 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-06-24 14:08:11,335 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Metrics reporters closed
2022-06-24 14:08:11,335 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] Metrics scheduler closed
2022-06-24 14:08:11,335 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-06-24 14:08:11,336 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] Metrics reporters closed
2022-06-24 14:08:11,339 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] App info kafka.consumer for consumer-group_id-33 unregistered
2022-06-24 14:08:11,339 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] group_id: Consumer stopped
2022-06-24 14:08:11,340 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] Metrics scheduler closed
2022-06-24 14:08:11,340 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-06-24 14:08:11,340 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] Metrics reporters closed
2022-06-24 14:08:11,340 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] App info kafka.consumer for consumer-group_id-37 unregistered
2022-06-24 14:08:11,340 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] group_id: Consumer stopped
2022-06-24 14:08:11,342 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Metrics scheduler closed
2022-06-24 14:08:11,342 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-06-24 14:08:11,342 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Metrics reporters closed
2022-06-24 14:08:11,344 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] App info kafka.consumer for consumer-group_id-38 unregistered
2022-06-24 14:08:11,344 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] group_id: Consumer stopped
2022-06-24 14:08:11,344 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] App info kafka.consumer for consumer-group_id-34 unregistered
2022-06-24 14:08:11,345 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] group_id: Consumer stopped
2022-06-24 14:08:11,345 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] App info kafka.consumer for consumer-group_id-35 unregistered
2022-06-24 14:08:11,345 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] Metrics scheduler closed
2022-06-24 14:08:11,346 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-06-24 14:08:11,346 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] group_id: Consumer stopped
2022-06-24 14:08:11,346 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] App info kafka.consumer for consumer-group_id-39 unregistered
2022-06-24 14:08:11,346 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] Metrics reporters closed
2022-06-24 14:08:11,346 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] group_id: Consumer stopped
2022-06-24 14:08:11,347 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] App info kafka.consumer for consumer-group_id-40 unregistered
2022-06-24 14:08:11,347 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] group_id: Consumer stopped
2022-06-24 14:08:11,353 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] Metrics scheduler closed
2022-06-24 14:08:11,353 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-06-24 14:08:11,353 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] Metrics reporters closed
2022-06-24 14:08:11,355 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] App info kafka.consumer for consumer-group_id-36 unregistered
2022-06-24 14:08:11,355 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] group_id: Consumer stopped
2022-06-24 14:08:11,361 INFO com.mongodb.diagnostics.logging.SLF4JLogger [Thread-27] Closed connection [connectionId{localValue:23, serverValue:61}] to localhost:27017 because the pool has been closed.
2022-06-24 14:08:11,363 INFO com.netflix.discovery.DiscoveryClient [Thread-27] Shutting down DiscoveryClient ...
2022-06-24 14:08:14,372 INFO com.netflix.discovery.DiscoveryClient [Thread-27] Unregistering ...
2022-06-24 14:08:14,387 INFO com.netflix.discovery.DiscoveryClient [Thread-27] DiscoveryClient_ESTOCKMARKET-QUERY/host.docker.internal:estockmarket-query:6093 - deregister  status: 200
2022-06-24 14:08:14,390 INFO com.netflix.discovery.DiscoveryClient [Thread-27] Completed shut down of DiscoveryClient
2022-06-24 14:08:14,582 INFO org.springframework.boot.StartupInfoLogger [restartedMain] Starting EstockmarketQueryApplication using Java 11.0.12 on ctsjava91 with PID 18180 (C:\Repo\elk-stock\ESTOCKMARKET-FSE\estockmarket-query\target\classes started by cogjava182 in C:\Repo\elk-stock\ESTOCKMARKET-FSE\estockmarket-query)
2022-06-24 14:08:14,583 DEBUG org.springframework.boot.StartupInfoLogger [restartedMain] Running with Spring Boot v2.5.3, Spring v5.3.9
2022-06-24 14:08:14,583 INFO org.springframework.boot.SpringApplication [restartedMain] No active profile set, falling back to default profiles: default
2022-06-24 14:08:15,043 INFO org.springframework.data.repository.config.RepositoryConfigurationDelegate [restartedMain] Bootstrapping Spring Data MongoDB repositories in DEFAULT mode.
2022-06-24 14:08:15,082 INFO org.springframework.data.repository.config.RepositoryConfigurationDelegate [restartedMain] Finished Spring Data repository scanning in 39 ms. Found 4 MongoDB repository interfaces.
2022-06-24 14:08:15,196 INFO org.springframework.cloud.context.scope.GenericScope [restartedMain] BeanFactory id=fff56ba9-7131-3a6a-a033-f341357c57c5
2022-06-24 14:08:15,281 INFO org.springframework.boot.web.embedded.tomcat.TomcatWebServer [restartedMain] Tomcat initialized with port(s): 6093 (http)
2022-06-24 14:08:15,282 INFO org.apache.juli.logging.DirectJDKLog [restartedMain] Initializing ProtocolHandler ["http-nio-6093"]
2022-06-24 14:08:15,282 INFO org.apache.juli.logging.DirectJDKLog [restartedMain] Starting service [Tomcat]
2022-06-24 14:08:15,282 INFO org.apache.juli.logging.DirectJDKLog [restartedMain] Starting Servlet engine: [Apache Tomcat/9.0.50]
2022-06-24 14:08:15,322 INFO org.apache.juli.logging.DirectJDKLog [restartedMain] Initializing Spring embedded WebApplicationContext
2022-06-24 14:08:15,322 INFO org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext [restartedMain] Root WebApplicationContext: initialization completed in 736 ms
2022-06-24 14:08:15,355 INFO com.mongodb.diagnostics.logging.SLF4JLogger [restartedMain] Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2022-06-24 14:08:15,360 INFO com.mongodb.diagnostics.logging.SLF4JLogger [cluster-ClusterId{value='62b577f752d23d5bec795f99', description='null'}-localhost:27017] Opened connection [connectionId{localValue:24, serverValue:62}] to localhost:27017
2022-06-24 14:08:15,361 INFO com.mongodb.diagnostics.logging.SLF4JLogger [cluster-rtt-ClusterId{value='62b577f752d23d5bec795f99', description='null'}-localhost:27017] Opened connection [connectionId{localValue:25, serverValue:63}] to localhost:27017
2022-06-24 14:08:15,363 INFO com.mongodb.diagnostics.logging.SLF4JLogger [cluster-ClusterId{value='62b577f752d23d5bec795f99', description='null'}-localhost:27017] Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1617500}
2022-06-24 14:08:15,365 WARN org.springframework.data.convert.CustomConversions [restartedMain] Registering converter from class java.time.LocalDateTime to class org.joda.time.LocalDateTime as reading converter although it doesn't convert from a store-supported type! You might want to check your annotation setup at the converter implementation.
2022-06-24 14:08:15,390 INFO org.springframework.boot.devtools.autoconfigure.OptionalLiveReloadServer [restartedMain] LiveReload server is running on port 35729
2022-06-24 14:08:15,426 WARN org.springframework.data.convert.CustomConversions [restartedMain] Registering converter from class java.time.LocalDateTime to class org.joda.time.LocalDateTime as reading converter although it doesn't convert from a store-supported type! You might want to check your annotation setup at the converter implementation.
2022-06-24 14:08:15,470 INFO com.estockmarket.query.domain.service.OTPService [restartedMain] inside otp servcie {}
2022-06-24 14:08:15,727 INFO org.springframework.cloud.netflix.eureka.config.DiscoveryClientOptionalArgsConfiguration [restartedMain] Eureka HTTP Client uses RestTemplate.
2022-06-24 14:08:15,788 WARN org.springframework.cloud.loadbalancer.config.LoadBalancerCacheAutoConfiguration$LoadBalancerCaffeineWarnLogger [restartedMain] Spring Cloud LoadBalancer is currently working with the default cache. You can switch to using Caffeine cache, by adding it and org.springframework.cache.caffeine.CaffeineCacheManager to the classpath.
2022-06-24 14:08:15,833 INFO org.springframework.cloud.netflix.eureka.InstanceInfoFactory [restartedMain] Setting initial instance status as: STARTING
2022-06-24 14:08:15,836 INFO com.netflix.discovery.DiscoveryClient [restartedMain] Initializing Eureka in region us-east-1
2022-06-24 14:08:15,837 INFO com.netflix.discovery.shared.resolver.aws.ConfigClusterResolver [restartedMain] Resolving eureka endpoints via configuration
2022-06-24 14:08:15,838 INFO com.netflix.discovery.DiscoveryClient [restartedMain] Disable delta property : false
2022-06-24 14:08:15,839 INFO com.netflix.discovery.DiscoveryClient [restartedMain] Single vip registry refresh property : null
2022-06-24 14:08:15,839 INFO com.netflix.discovery.DiscoveryClient [restartedMain] Force full registry fetch : false
2022-06-24 14:08:15,839 INFO com.netflix.discovery.DiscoveryClient [restartedMain] Application is null : false
2022-06-24 14:08:15,839 INFO com.netflix.discovery.DiscoveryClient [restartedMain] Registered Applications size is zero : true
2022-06-24 14:08:15,839 INFO com.netflix.discovery.DiscoveryClient [restartedMain] Application version is -1: true
2022-06-24 14:08:15,839 INFO com.netflix.discovery.DiscoveryClient [restartedMain] Getting all instance registry info from the eureka server
2022-06-24 14:08:15,857 INFO com.netflix.discovery.DiscoveryClient [restartedMain] The response status is 200
2022-06-24 14:08:15,858 INFO com.netflix.discovery.DiscoveryClient [restartedMain] Starting heartbeat executor: renew interval is: 30
2022-06-24 14:08:15,858 INFO com.netflix.discovery.InstanceInfoReplicator [restartedMain] InstanceInfoReplicator onDemand update allowed rate per min is 4
2022-06-24 14:08:15,859 INFO com.netflix.discovery.DiscoveryClient [restartedMain] Discovery Client initialized at timestamp 1656059895859 with initial instances count: 3
2022-06-24 14:08:15,860 INFO org.springframework.cloud.netflix.eureka.serviceregistry.EurekaServiceRegistry [restartedMain] Registering application ESTOCKMARKET-QUERY with eureka with status UP
2022-06-24 14:08:15,861 INFO com.netflix.discovery.DiscoveryClient$3 [restartedMain] Saw local status change event StatusChangeEvent [timestamp=1656059895861, current=UP, previous=STARTING]
2022-06-24 14:08:15,862 INFO com.netflix.discovery.DiscoveryClient [DiscoveryClient-InstanceInfoReplicator-0] DiscoveryClient_ESTOCKMARKET-QUERY/host.docker.internal:estockmarket-query:6093: registering service...
2022-06-24 14:08:15,864 INFO org.apache.kafka.common.config.AbstractConfig [restartedMain] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-group_id-41
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = group_id
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2022-06-24 14:08:15,867 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka version: 2.7.1
2022-06-24 14:08:15,867 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka commitId: 61dbce85d0d41457
2022-06-24 14:08:15,867 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka startTimeMs: 1656059895867
2022-06-24 14:08:15,867 INFO org.apache.kafka.clients.consumer.KafkaConsumer [restartedMain] [Consumer clientId=consumer-group_id-41, groupId=group_id] Subscribed to topic(s): createStock
2022-06-24 14:08:15,870 INFO org.apache.kafka.common.config.AbstractConfig [restartedMain] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-group_id-42
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = group_id
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2022-06-24 14:08:15,873 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka version: 2.7.1
2022-06-24 14:08:15,874 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka commitId: 61dbce85d0d41457
2022-06-24 14:08:15,874 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka startTimeMs: 1656059895873
2022-06-24 14:08:15,874 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-41, groupId=group_id] Cluster ID: 5kpO7pNKTFqroM6YwsJ86A
2022-06-24 14:08:15,874 INFO org.apache.kafka.clients.consumer.KafkaConsumer [restartedMain] [Consumer clientId=consumer-group_id-42, groupId=group_id] Subscribed to topic(s): createUser
2022-06-24 14:08:15,875 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-41, groupId=group_id] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2022-06-24 14:08:15,876 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-41, groupId=group_id] (Re-)joining group
2022-06-24 14:08:15,876 INFO com.netflix.discovery.DiscoveryClient [DiscoveryClient-InstanceInfoReplicator-0] DiscoveryClient_ESTOCKMARKET-QUERY/host.docker.internal:estockmarket-query:6093 - registration status: 204
2022-06-24 14:08:15,876 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-41, groupId=group_id] (Re-)joining group
2022-06-24 14:08:15,876 INFO org.apache.kafka.common.config.AbstractConfig [restartedMain] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-group_id-43
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = group_id
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2022-06-24 14:08:15,886 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka version: 2.7.1
2022-06-24 14:08:15,886 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka commitId: 61dbce85d0d41457
2022-06-24 14:08:15,886 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka startTimeMs: 1656059895886
2022-06-24 14:08:15,887 INFO org.apache.kafka.clients.consumer.KafkaConsumer [restartedMain] [Consumer clientId=consumer-group_id-43, groupId=group_id] Subscribed to topic(s): removeSector
2022-06-24 14:08:15,887 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-42, groupId=group_id] Cluster ID: 5kpO7pNKTFqroM6YwsJ86A
2022-06-24 14:08:15,887 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-42, groupId=group_id] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2022-06-24 14:08:15,889 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-41, groupId=group_id] Successfully joined group with generation Generation{generationId=90, memberId='consumer-group_id-41-374979d7-3fdc-4a89-a189-b06d61405d27', protocol='range'}
2022-06-24 14:08:15,889 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-42, groupId=group_id] (Re-)joining group
2022-06-24 14:08:15,889 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-41, groupId=group_id] Finished assignment for group at generation 90: {consumer-group_id-41-374979d7-3fdc-4a89-a189-b06d61405d27=Assignment(partitions=[createStock-0])}
2022-06-24 14:08:15,891 INFO org.apache.kafka.common.config.AbstractConfig [restartedMain] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-group_id-44
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = group_id
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2022-06-24 14:08:15,894 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-43, groupId=group_id] Cluster ID: 5kpO7pNKTFqroM6YwsJ86A
2022-06-24 14:08:15,894 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-42, groupId=group_id] (Re-)joining group
2022-06-24 14:08:15,894 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka version: 2.7.1
2022-06-24 14:08:15,894 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka commitId: 61dbce85d0d41457
2022-06-24 14:08:15,894 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-43, groupId=group_id] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2022-06-24 14:08:15,894 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka startTimeMs: 1656059895894
2022-06-24 14:08:15,895 INFO org.apache.kafka.clients.consumer.KafkaConsumer [restartedMain] [Consumer clientId=consumer-group_id-44, groupId=group_id] Subscribed to topic(s): createSector
2022-06-24 14:08:15,897 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-43, groupId=group_id] (Re-)joining group
2022-06-24 14:08:15,898 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-41, groupId=group_id] Successfully synced group in generation Generation{generationId=90, memberId='consumer-group_id-41-374979d7-3fdc-4a89-a189-b06d61405d27', protocol='range'}
2022-06-24 14:08:15,898 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-41, groupId=group_id] Notifying assignor about the new Assignment(partitions=[createStock-0])
2022-06-24 14:08:15,899 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-41, groupId=group_id] Adding newly assigned partitions: createStock-0
2022-06-24 14:08:15,899 INFO org.apache.kafka.common.config.AbstractConfig [restartedMain] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-group_id-45
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = group_id
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2022-06-24 14:08:15,900 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-43, groupId=group_id] (Re-)joining group
2022-06-24 14:08:15,901 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-41, groupId=group_id] Setting offset for partition createStock-0 to the committed offset FetchPosition{offset=3, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:08:15,901 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] group_id: partitions assigned: [createStock-0]
2022-06-24 14:08:15,902 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-44, groupId=group_id] Cluster ID: 5kpO7pNKTFqroM6YwsJ86A
2022-06-24 14:08:15,903 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-44, groupId=group_id] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2022-06-24 14:08:15,903 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka version: 2.7.1
2022-06-24 14:08:15,903 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka commitId: 61dbce85d0d41457
2022-06-24 14:08:15,903 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka startTimeMs: 1656059895903
2022-06-24 14:08:15,903 INFO org.apache.kafka.clients.consumer.KafkaConsumer [restartedMain] [Consumer clientId=consumer-group_id-45, groupId=group_id] Subscribed to topic(s): removeStock
2022-06-24 14:08:15,903 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-44, groupId=group_id] (Re-)joining group
2022-06-24 14:08:15,906 INFO org.apache.kafka.common.config.AbstractConfig [restartedMain] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-group_id-46
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = group_id
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2022-06-24 14:08:15,906 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-44, groupId=group_id] (Re-)joining group
2022-06-24 14:08:15,907 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-45, groupId=group_id] Cluster ID: 5kpO7pNKTFqroM6YwsJ86A
2022-06-24 14:08:15,908 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-45, groupId=group_id] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2022-06-24 14:08:15,908 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-45, groupId=group_id] (Re-)joining group
2022-06-24 14:08:15,910 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka version: 2.7.1
2022-06-24 14:08:15,910 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka commitId: 61dbce85d0d41457
2022-06-24 14:08:15,910 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka startTimeMs: 1656059895910
2022-06-24 14:08:15,911 INFO org.apache.kafka.clients.consumer.KafkaConsumer [restartedMain] [Consumer clientId=consumer-group_id-46, groupId=group_id] Subscribed to topic(s): updateStock
2022-06-24 14:08:15,913 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-45, groupId=group_id] (Re-)joining group
2022-06-24 14:08:15,914 INFO org.apache.kafka.common.config.AbstractConfig [restartedMain] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-group_id-47
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = group_id
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2022-06-24 14:08:15,916 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-46, groupId=group_id] Cluster ID: 5kpO7pNKTFqroM6YwsJ86A
2022-06-24 14:08:15,916 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-46, groupId=group_id] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2022-06-24 14:08:15,917 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-46, groupId=group_id] (Re-)joining group
2022-06-24 14:08:15,918 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka version: 2.7.1
2022-06-24 14:08:15,918 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka commitId: 61dbce85d0d41457
2022-06-24 14:08:15,918 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka startTimeMs: 1656059895918
2022-06-24 14:08:15,919 INFO org.apache.kafka.clients.consumer.KafkaConsumer [restartedMain] [Consumer clientId=consumer-group_id-47, groupId=group_id] Subscribed to topic(s): removeCompany
2022-06-24 14:08:15,919 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-46, groupId=group_id] (Re-)joining group
2022-06-24 14:08:15,922 INFO org.apache.kafka.common.config.AbstractConfig [restartedMain] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-group_id-48
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = group_id
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2022-06-24 14:08:15,923 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-47, groupId=group_id] Cluster ID: 5kpO7pNKTFqroM6YwsJ86A
2022-06-24 14:08:15,923 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-47, groupId=group_id] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2022-06-24 14:08:15,924 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-47, groupId=group_id] (Re-)joining group
2022-06-24 14:08:15,926 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka version: 2.7.1
2022-06-24 14:08:15,926 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka commitId: 61dbce85d0d41457
2022-06-24 14:08:15,926 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka startTimeMs: 1656059895926
2022-06-24 14:08:15,926 INFO org.apache.kafka.clients.consumer.KafkaConsumer [restartedMain] [Consumer clientId=consumer-group_id-48, groupId=group_id] Subscribed to topic(s): createCompany
2022-06-24 14:08:15,926 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-47, groupId=group_id] (Re-)joining group
2022-06-24 14:08:15,928 INFO org.apache.juli.logging.DirectJDKLog [restartedMain] Starting ProtocolHandler ["http-nio-6093"]
2022-06-24 14:08:15,933 INFO org.springframework.boot.web.embedded.tomcat.TomcatWebServer [restartedMain] Tomcat started on port(s): 6093 (http) with context path ''
2022-06-24 14:08:15,933 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-48, groupId=group_id] Cluster ID: 5kpO7pNKTFqroM6YwsJ86A
2022-06-24 14:08:15,933 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-48, groupId=group_id] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2022-06-24 14:08:15,933 INFO org.springframework.cloud.netflix.eureka.serviceregistry.EurekaAutoServiceRegistration [restartedMain] Updating port to 6093
2022-06-24 14:08:15,934 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-48, groupId=group_id] (Re-)joining group
2022-06-24 14:08:15,936 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-48, groupId=group_id] (Re-)joining group
2022-06-24 14:08:15,970 INFO springfox.documentation.spring.web.plugins.DocumentationPluginsBootstrapper [restartedMain] Context refreshed
2022-06-24 14:08:15,971 INFO springfox.documentation.spring.web.plugins.DocumentationPluginsBootstrapper [restartedMain] Found 1 custom documentation plugin(s)
2022-06-24 14:08:15,973 INFO springfox.documentation.spring.web.scanners.ApiListingReferenceScanner [restartedMain] Scanning for api listing references
2022-06-24 14:08:16,008 INFO org.springframework.boot.StartupInfoLogger [restartedMain] Started EstockmarketQueryApplication in 1.518 seconds (JVM running for 53610.316)
2022-06-24 14:08:16,012 INFO org.springframework.boot.devtools.autoconfigure.ConditionEvaluationDeltaLoggingListener [restartedMain] Condition evaluation unchanged
2022-06-24 14:08:18,902 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-41, groupId=group_id] Attempt to heartbeat failed since group is rebalancing
2022-06-24 14:08:18,903 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-41, groupId=group_id] Revoke previously assigned partitions createStock-0
2022-06-24 14:08:18,903 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] group_id: partitions revoked: [createStock-0]
2022-06-24 14:08:18,903 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-41, groupId=group_id] (Re-)joining group
2022-06-24 14:08:18,907 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-45, groupId=group_id] Successfully joined group with generation Generation{generationId=91, memberId='consumer-group_id-45-28917435-2ce4-4a3d-9e63-14e4a818013d', protocol='range'}
2022-06-24 14:08:18,907 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-42, groupId=group_id] Successfully joined group with generation Generation{generationId=91, memberId='consumer-group_id-42-14bbaf0b-5f52-4cfd-85ed-2dcf040bdca5', protocol='range'}
2022-06-24 14:08:18,907 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-47, groupId=group_id] Successfully joined group with generation Generation{generationId=91, memberId='consumer-group_id-47-808c2914-f32d-4df2-9875-66f1946fcdae', protocol='range'}
2022-06-24 14:08:18,907 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-46, groupId=group_id] Successfully joined group with generation Generation{generationId=91, memberId='consumer-group_id-46-a5e7f59e-dca5-4a2d-a5b4-4467c22312ae', protocol='range'}
2022-06-24 14:08:18,907 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-44, groupId=group_id] Successfully joined group with generation Generation{generationId=91, memberId='consumer-group_id-44-15579497-cc1b-4655-8e9b-9ac575567f91', protocol='range'}
2022-06-24 14:08:18,907 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-48, groupId=group_id] Successfully joined group with generation Generation{generationId=91, memberId='consumer-group_id-48-f6bc06b1-755e-4b01-bef6-6aa8770b8d30', protocol='range'}
2022-06-24 14:08:18,907 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-41, groupId=group_id] Successfully joined group with generation Generation{generationId=91, memberId='consumer-group_id-41-374979d7-3fdc-4a89-a189-b06d61405d27', protocol='range'}
2022-06-24 14:08:18,907 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-43, groupId=group_id] Successfully joined group with generation Generation{generationId=91, memberId='consumer-group_id-43-c572de02-af12-4703-81bb-450a287f441e', protocol='range'}
2022-06-24 14:08:18,964 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-41, groupId=group_id] Finished assignment for group at generation 91: {consumer-group_id-43-c572de02-af12-4703-81bb-450a287f441e=Assignment(partitions=[removeSector-0]), consumer-group_id-46-a5e7f59e-dca5-4a2d-a5b4-4467c22312ae=Assignment(partitions=[updateStock-0]), consumer-group_id-44-15579497-cc1b-4655-8e9b-9ac575567f91=Assignment(partitions=[createSector-0]), consumer-group_id-47-808c2914-f32d-4df2-9875-66f1946fcdae=Assignment(partitions=[removeCompany-0]), consumer-group_id-41-374979d7-3fdc-4a89-a189-b06d61405d27=Assignment(partitions=[createStock-0]), consumer-group_id-45-28917435-2ce4-4a3d-9e63-14e4a818013d=Assignment(partitions=[removeStock-0]), consumer-group_id-48-f6bc06b1-755e-4b01-bef6-6aa8770b8d30=Assignment(partitions=[createCompany-0]), consumer-group_id-42-14bbaf0b-5f52-4cfd-85ed-2dcf040bdca5=Assignment(partitions=[createUser-0])}
2022-06-24 14:08:18,969 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-43, groupId=group_id] Successfully synced group in generation Generation{generationId=91, memberId='consumer-group_id-43-c572de02-af12-4703-81bb-450a287f441e', protocol='range'}
2022-06-24 14:08:18,969 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-41, groupId=group_id] Successfully synced group in generation Generation{generationId=91, memberId='consumer-group_id-41-374979d7-3fdc-4a89-a189-b06d61405d27', protocol='range'}
2022-06-24 14:08:18,971 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-43, groupId=group_id] Notifying assignor about the new Assignment(partitions=[removeSector-0])
2022-06-24 14:08:18,971 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-41, groupId=group_id] Notifying assignor about the new Assignment(partitions=[createStock-0])
2022-06-24 14:08:18,974 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-41, groupId=group_id] Adding newly assigned partitions: createStock-0
2022-06-24 14:08:18,974 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-42, groupId=group_id] Successfully synced group in generation Generation{generationId=91, memberId='consumer-group_id-42-14bbaf0b-5f52-4cfd-85ed-2dcf040bdca5', protocol='range'}
2022-06-24 14:08:18,974 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-48, groupId=group_id] Successfully synced group in generation Generation{generationId=91, memberId='consumer-group_id-48-f6bc06b1-755e-4b01-bef6-6aa8770b8d30', protocol='range'}
2022-06-24 14:08:18,974 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-43, groupId=group_id] Adding newly assigned partitions: removeSector-0
2022-06-24 14:08:18,974 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-46, groupId=group_id] Successfully synced group in generation Generation{generationId=91, memberId='consumer-group_id-46-a5e7f59e-dca5-4a2d-a5b4-4467c22312ae', protocol='range'}
2022-06-24 14:08:18,974 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-45, groupId=group_id] Successfully synced group in generation Generation{generationId=91, memberId='consumer-group_id-45-28917435-2ce4-4a3d-9e63-14e4a818013d', protocol='range'}
2022-06-24 14:08:18,975 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-44, groupId=group_id] Successfully synced group in generation Generation{generationId=91, memberId='consumer-group_id-44-15579497-cc1b-4655-8e9b-9ac575567f91', protocol='range'}
2022-06-24 14:08:18,975 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-42, groupId=group_id] Notifying assignor about the new Assignment(partitions=[createUser-0])
2022-06-24 14:08:18,975 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-48, groupId=group_id] Notifying assignor about the new Assignment(partitions=[createCompany-0])
2022-06-24 14:08:18,975 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-42, groupId=group_id] Adding newly assigned partitions: createUser-0
2022-06-24 14:08:18,976 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-48, groupId=group_id] Adding newly assigned partitions: createCompany-0
2022-06-24 14:08:18,976 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-46, groupId=group_id] Notifying assignor about the new Assignment(partitions=[updateStock-0])
2022-06-24 14:08:18,976 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-46, groupId=group_id] Adding newly assigned partitions: updateStock-0
2022-06-24 14:08:18,975 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-47, groupId=group_id] Successfully synced group in generation Generation{generationId=91, memberId='consumer-group_id-47-808c2914-f32d-4df2-9875-66f1946fcdae', protocol='range'}
2022-06-24 14:08:18,976 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-45, groupId=group_id] Notifying assignor about the new Assignment(partitions=[removeStock-0])
2022-06-24 14:08:18,976 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-44, groupId=group_id] Notifying assignor about the new Assignment(partitions=[createSector-0])
2022-06-24 14:08:18,977 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-44, groupId=group_id] Adding newly assigned partitions: createSector-0
2022-06-24 14:08:18,977 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-45, groupId=group_id] Adding newly assigned partitions: removeStock-0
2022-06-24 14:08:18,977 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-47, groupId=group_id] Notifying assignor about the new Assignment(partitions=[removeCompany-0])
2022-06-24 14:08:18,977 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-47, groupId=group_id] Adding newly assigned partitions: removeCompany-0
2022-06-24 14:08:18,977 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-41, groupId=group_id] Setting offset for partition createStock-0 to the committed offset FetchPosition{offset=3, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:08:18,977 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-43, groupId=group_id] Setting offset for partition removeSector-0 to the committed offset FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:08:18,977 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] group_id: partitions assigned: [createStock-0]
2022-06-24 14:08:18,978 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] group_id: partitions assigned: [removeSector-0]
2022-06-24 14:08:18,979 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-45, groupId=group_id] Setting offset for partition removeStock-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:08:18,979 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-42, groupId=group_id] Setting offset for partition createUser-0 to the committed offset FetchPosition{offset=1, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:08:18,979 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] group_id: partitions assigned: [removeStock-0]
2022-06-24 14:08:18,979 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] group_id: partitions assigned: [createUser-0]
2022-06-24 14:08:18,979 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-46, groupId=group_id] Setting offset for partition updateStock-0 to the committed offset FetchPosition{offset=7, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:08:18,980 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] group_id: partitions assigned: [updateStock-0]
2022-06-24 14:08:18,981 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-48, groupId=group_id] Setting offset for partition createCompany-0 to the committed offset FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:08:18,981 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] group_id: partitions assigned: [createCompany-0]
2022-06-24 14:08:18,982 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-47, groupId=group_id] Setting offset for partition removeCompany-0 to the committed offset FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:08:18,982 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-44, groupId=group_id] Setting offset for partition createSector-0 to the committed offset FetchPosition{offset=1, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:08:18,982 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] group_id: partitions assigned: [removeCompany-0]
2022-06-24 14:08:18,982 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] group_id: partitions assigned: [createSector-0]
2022-06-24 14:13:15,853 INFO com.netflix.discovery.shared.resolver.aws.ConfigClusterResolver [AsyncResolver-bootstrap-executor-0] Resolving eureka endpoints via configuration
2022-06-24 14:13:41,773 INFO org.springframework.cloud.netflix.eureka.serviceregistry.EurekaServiceRegistry [Thread-33] Unregistering application ESTOCKMARKET-QUERY with eureka with status DOWN
2022-06-24 14:13:41,774 INFO com.netflix.discovery.DiscoveryClient$3 [Thread-33] Saw local status change event StatusChangeEvent [timestamp=1656060221774, current=DOWN, previous=UP]
2022-06-24 14:13:41,774 INFO com.netflix.discovery.DiscoveryClient [DiscoveryClient-InstanceInfoReplicator-0] DiscoveryClient_ESTOCKMARKET-QUERY/host.docker.internal:estockmarket-query:6093: registering service...
2022-06-24 14:13:41,786 INFO com.netflix.discovery.DiscoveryClient [DiscoveryClient-InstanceInfoReplicator-0] DiscoveryClient_ESTOCKMARKET-QUERY/host.docker.internal:estockmarket-query:6093 - registration status: 204
2022-06-24 14:13:41,819 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-41, groupId=group_id] Revoke previously assigned partitions createStock-0
2022-06-24 14:13:41,819 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-43, groupId=group_id] Revoke previously assigned partitions removeSector-0
2022-06-24 14:13:41,819 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-42, groupId=group_id] Revoke previously assigned partitions createUser-0
2022-06-24 14:13:41,819 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-44, groupId=group_id] Revoke previously assigned partitions createSector-0
2022-06-24 14:13:41,819 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-45, groupId=group_id] Revoke previously assigned partitions removeStock-0
2022-06-24 14:13:41,819 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] group_id: partitions revoked: [createStock-0]
2022-06-24 14:13:41,819 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] group_id: partitions revoked: [removeSector-0]
2022-06-24 14:13:41,820 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-41, groupId=group_id] Member consumer-group_id-41-374979d7-3fdc-4a89-a189-b06d61405d27 sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2022-06-24 14:13:41,820 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-46, groupId=group_id] Revoke previously assigned partitions updateStock-0
2022-06-24 14:13:41,820 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] group_id: partitions revoked: [removeStock-0]
2022-06-24 14:13:41,820 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-43, groupId=group_id] Member consumer-group_id-43-c572de02-af12-4703-81bb-450a287f441e sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2022-06-24 14:13:41,819 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] group_id: partitions revoked: [createSector-0]
2022-06-24 14:13:41,819 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] group_id: partitions revoked: [createUser-0]
2022-06-24 14:13:41,820 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-44, groupId=group_id] Member consumer-group_id-44-15579497-cc1b-4655-8e9b-9ac575567f91 sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2022-06-24 14:13:41,820 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] group_id: partitions revoked: [updateStock-0]
2022-06-24 14:13:41,820 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-41, groupId=group_id] Unsubscribed all topics or patterns and assigned partitions
2022-06-24 14:13:41,820 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-45, groupId=group_id] Member consumer-group_id-45-28917435-2ce4-4a3d-9e63-14e4a818013d sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2022-06-24 14:13:41,820 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-44, groupId=group_id] Unsubscribed all topics or patterns and assigned partitions
2022-06-24 14:13:41,820 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-42, groupId=group_id] Member consumer-group_id-42-14bbaf0b-5f52-4cfd-85ed-2dcf040bdca5 sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2022-06-24 14:13:41,820 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-46, groupId=group_id] Member consumer-group_id-46-a5e7f59e-dca5-4a2d-a5b4-4467c22312ae sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2022-06-24 14:13:41,820 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-43, groupId=group_id] Unsubscribed all topics or patterns and assigned partitions
2022-06-24 14:13:41,820 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-47, groupId=group_id] Revoke previously assigned partitions removeCompany-0
2022-06-24 14:13:41,820 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-45, groupId=group_id] Unsubscribed all topics or patterns and assigned partitions
2022-06-24 14:13:41,820 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] group_id: partitions revoked: [removeCompany-0]
2022-06-24 14:13:41,820 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-48, groupId=group_id] Revoke previously assigned partitions createCompany-0
2022-06-24 14:13:41,821 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-47, groupId=group_id] Member consumer-group_id-47-808c2914-f32d-4df2-9875-66f1946fcdae sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2022-06-24 14:13:41,820 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-46, groupId=group_id] Unsubscribed all topics or patterns and assigned partitions
2022-06-24 14:13:41,820 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-42, groupId=group_id] Unsubscribed all topics or patterns and assigned partitions
2022-06-24 14:13:41,821 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] group_id: partitions revoked: [createCompany-0]
2022-06-24 14:13:41,822 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-47, groupId=group_id] Unsubscribed all topics or patterns and assigned partitions
2022-06-24 14:13:41,822 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-48, groupId=group_id] Member consumer-group_id-48-f6bc06b1-755e-4b01-bef6-6aa8770b8d30 sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2022-06-24 14:13:41,823 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-48, groupId=group_id] Unsubscribed all topics or patterns and assigned partitions
2022-06-24 14:13:41,825 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] Metrics scheduler closed
2022-06-24 14:13:41,825 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-06-24 14:13:41,825 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] Metrics reporters closed
2022-06-24 14:13:41,826 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] Metrics scheduler closed
2022-06-24 14:13:41,826 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-06-24 14:13:41,826 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] Metrics reporters closed
2022-06-24 14:13:41,827 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] App info kafka.consumer for consumer-group_id-41 unregistered
2022-06-24 14:13:41,828 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] group_id: Consumer stopped
2022-06-24 14:13:41,828 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] Metrics scheduler closed
2022-06-24 14:13:41,828 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-06-24 14:13:41,828 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] App info kafka.consumer for consumer-group_id-44 unregistered
2022-06-24 14:13:41,828 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] Metrics reporters closed
2022-06-24 14:13:41,828 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] group_id: Consumer stopped
2022-06-24 14:13:41,828 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] Metrics scheduler closed
2022-06-24 14:13:41,828 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-06-24 14:13:41,829 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] Metrics reporters closed
2022-06-24 14:13:41,829 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Metrics scheduler closed
2022-06-24 14:13:41,829 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-06-24 14:13:41,830 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Metrics reporters closed
2022-06-24 14:13:41,831 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] Metrics scheduler closed
2022-06-24 14:13:41,831 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] Metrics scheduler closed
2022-06-24 14:13:41,831 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-06-24 14:13:41,831 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-06-24 14:13:41,832 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] Metrics reporters closed
2022-06-24 14:13:41,832 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] Metrics reporters closed
2022-06-24 14:13:41,832 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] App info kafka.consumer for consumer-group_id-45 unregistered
2022-06-24 14:13:41,833 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] group_id: Consumer stopped
2022-06-24 14:13:41,834 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] App info kafka.consumer for consumer-group_id-43 unregistered
2022-06-24 14:13:41,834 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] group_id: Consumer stopped
2022-06-24 14:13:41,836 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] App info kafka.consumer for consumer-group_id-46 unregistered
2022-06-24 14:13:41,836 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] group_id: Consumer stopped
2022-06-24 14:13:41,837 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] App info kafka.consumer for consumer-group_id-48 unregistered
2022-06-24 14:13:41,837 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] group_id: Consumer stopped
2022-06-24 14:13:41,837 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] App info kafka.consumer for consumer-group_id-42 unregistered
2022-06-24 14:13:41,837 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] group_id: Consumer stopped
2022-06-24 14:13:41,837 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Metrics scheduler closed
2022-06-24 14:13:41,838 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-06-24 14:13:41,838 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Metrics reporters closed
2022-06-24 14:13:41,839 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] App info kafka.consumer for consumer-group_id-47 unregistered
2022-06-24 14:13:41,839 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] group_id: Consumer stopped
2022-06-24 14:13:41,848 INFO com.netflix.discovery.DiscoveryClient [Thread-33] Shutting down DiscoveryClient ...
2022-06-24 14:13:44,861 INFO com.netflix.discovery.DiscoveryClient [Thread-33] Unregistering ...
2022-06-24 14:13:44,878 INFO com.netflix.discovery.DiscoveryClient [Thread-33] DiscoveryClient_ESTOCKMARKET-QUERY/host.docker.internal:estockmarket-query:6093 - deregister  status: 200
2022-06-24 14:13:44,881 INFO com.netflix.discovery.DiscoveryClient [Thread-33] Completed shut down of DiscoveryClient
2022-06-24 14:13:45,064 INFO org.springframework.boot.StartupInfoLogger [restartedMain] Starting EstockmarketQueryApplication using Java 11.0.12 on ctsjava91 with PID 18180 (C:\Repo\elk-stock\ESTOCKMARKET-FSE\estockmarket-query\target\classes started by cogjava182 in C:\Repo\elk-stock\ESTOCKMARKET-FSE\estockmarket-query)
2022-06-24 14:13:45,064 DEBUG org.springframework.boot.StartupInfoLogger [restartedMain] Running with Spring Boot v2.5.3, Spring v5.3.9
2022-06-24 14:13:45,065 INFO org.springframework.boot.SpringApplication [restartedMain] No active profile set, falling back to default profiles: default
2022-06-24 14:13:45,518 INFO org.springframework.data.repository.config.RepositoryConfigurationDelegate [restartedMain] Bootstrapping Spring Data MongoDB repositories in DEFAULT mode.
2022-06-24 14:13:45,549 INFO org.springframework.data.repository.config.RepositoryConfigurationDelegate [restartedMain] Finished Spring Data repository scanning in 30 ms. Found 4 MongoDB repository interfaces.
2022-06-24 14:13:45,644 INFO org.springframework.cloud.context.scope.GenericScope [restartedMain] BeanFactory id=fff56ba9-7131-3a6a-a033-f341357c57c5
2022-06-24 14:13:45,724 INFO org.springframework.boot.web.embedded.tomcat.TomcatWebServer [restartedMain] Tomcat initialized with port(s): 6093 (http)
2022-06-24 14:13:45,725 INFO org.apache.juli.logging.DirectJDKLog [restartedMain] Initializing ProtocolHandler ["http-nio-6093"]
2022-06-24 14:13:45,725 INFO org.apache.juli.logging.DirectJDKLog [restartedMain] Starting service [Tomcat]
2022-06-24 14:13:45,726 INFO org.apache.juli.logging.DirectJDKLog [restartedMain] Starting Servlet engine: [Apache Tomcat/9.0.50]
2022-06-24 14:13:45,763 INFO org.apache.juli.logging.DirectJDKLog [restartedMain] Initializing Spring embedded WebApplicationContext
2022-06-24 14:13:45,763 INFO org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext [restartedMain] Root WebApplicationContext: initialization completed in 695 ms
2022-06-24 14:13:45,792 INFO com.mongodb.diagnostics.logging.SLF4JLogger [restartedMain] Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2022-06-24 14:13:45,797 WARN org.springframework.data.convert.CustomConversions [restartedMain] Registering converter from class java.time.LocalDateTime to class org.joda.time.LocalDateTime as reading converter although it doesn't convert from a store-supported type! You might want to check your annotation setup at the converter implementation.
2022-06-24 14:13:45,801 INFO com.mongodb.diagnostics.logging.SLF4JLogger [cluster-ClusterId{value='62b5794152d23d5bec795f9a', description='null'}-localhost:27017] Opened connection [connectionId{localValue:26, serverValue:78}] to localhost:27017
2022-06-24 14:13:45,800 INFO com.mongodb.diagnostics.logging.SLF4JLogger [cluster-rtt-ClusterId{value='62b5794152d23d5bec795f9a', description='null'}-localhost:27017] Opened connection [connectionId{localValue:27, serverValue:77}] to localhost:27017
2022-06-24 14:13:45,803 INFO com.mongodb.diagnostics.logging.SLF4JLogger [cluster-ClusterId{value='62b5794152d23d5bec795f9a', description='null'}-localhost:27017] Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=6152100}
2022-06-24 14:13:45,825 INFO org.springframework.boot.devtools.autoconfigure.OptionalLiveReloadServer [restartedMain] LiveReload server is running on port 35729
2022-06-24 14:13:45,843 WARN org.springframework.data.convert.CustomConversions [restartedMain] Registering converter from class java.time.LocalDateTime to class org.joda.time.LocalDateTime as reading converter although it doesn't convert from a store-supported type! You might want to check your annotation setup at the converter implementation.
2022-06-24 14:13:45,888 INFO com.estockmarket.query.domain.service.OTPService [restartedMain] inside otp servcie {}
2022-06-24 14:13:46,156 INFO org.springframework.cloud.netflix.eureka.config.DiscoveryClientOptionalArgsConfiguration [restartedMain] Eureka HTTP Client uses RestTemplate.
2022-06-24 14:13:46,217 WARN org.springframework.cloud.loadbalancer.config.LoadBalancerCacheAutoConfiguration$LoadBalancerCaffeineWarnLogger [restartedMain] Spring Cloud LoadBalancer is currently working with the default cache. You can switch to using Caffeine cache, by adding it and org.springframework.cache.caffeine.CaffeineCacheManager to the classpath.
2022-06-24 14:13:46,265 INFO org.springframework.cloud.netflix.eureka.InstanceInfoFactory [restartedMain] Setting initial instance status as: STARTING
2022-06-24 14:13:46,268 INFO com.netflix.discovery.DiscoveryClient [restartedMain] Initializing Eureka in region us-east-1
2022-06-24 14:13:46,268 INFO com.netflix.discovery.shared.resolver.aws.ConfigClusterResolver [restartedMain] Resolving eureka endpoints via configuration
2022-06-24 14:13:46,269 INFO com.netflix.discovery.DiscoveryClient [restartedMain] Disable delta property : false
2022-06-24 14:13:46,270 INFO com.netflix.discovery.DiscoveryClient [restartedMain] Single vip registry refresh property : null
2022-06-24 14:13:46,270 INFO com.netflix.discovery.DiscoveryClient [restartedMain] Force full registry fetch : false
2022-06-24 14:13:46,270 INFO com.netflix.discovery.DiscoveryClient [restartedMain] Application is null : false
2022-06-24 14:13:46,270 INFO com.netflix.discovery.DiscoveryClient [restartedMain] Registered Applications size is zero : true
2022-06-24 14:13:46,270 INFO com.netflix.discovery.DiscoveryClient [restartedMain] Application version is -1: true
2022-06-24 14:13:46,270 INFO com.netflix.discovery.DiscoveryClient [restartedMain] Getting all instance registry info from the eureka server
2022-06-24 14:13:46,283 INFO com.netflix.discovery.DiscoveryClient [restartedMain] The response status is 200
2022-06-24 14:13:46,284 INFO com.netflix.discovery.DiscoveryClient [restartedMain] Starting heartbeat executor: renew interval is: 30
2022-06-24 14:13:46,285 INFO com.netflix.discovery.InstanceInfoReplicator [restartedMain] InstanceInfoReplicator onDemand update allowed rate per min is 4
2022-06-24 14:13:46,286 INFO com.netflix.discovery.DiscoveryClient [restartedMain] Discovery Client initialized at timestamp 1656060226286 with initial instances count: 3
2022-06-24 14:13:46,287 INFO org.springframework.cloud.netflix.eureka.serviceregistry.EurekaServiceRegistry [restartedMain] Registering application ESTOCKMARKET-QUERY with eureka with status UP
2022-06-24 14:13:46,287 INFO com.netflix.discovery.DiscoveryClient$3 [restartedMain] Saw local status change event StatusChangeEvent [timestamp=1656060226287, current=UP, previous=STARTING]
2022-06-24 14:13:46,288 INFO com.netflix.discovery.DiscoveryClient [DiscoveryClient-InstanceInfoReplicator-0] DiscoveryClient_ESTOCKMARKET-QUERY/host.docker.internal:estockmarket-query:6093: registering service...
2022-06-24 14:13:46,289 INFO org.apache.kafka.common.config.AbstractConfig [restartedMain] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-group_id-49
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = group_id
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2022-06-24 14:13:46,292 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka version: 2.7.1
2022-06-24 14:13:46,292 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka commitId: 61dbce85d0d41457
2022-06-24 14:13:46,292 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka startTimeMs: 1656060226292
2022-06-24 14:13:46,293 INFO org.apache.kafka.clients.consumer.KafkaConsumer [restartedMain] [Consumer clientId=consumer-group_id-49, groupId=group_id] Subscribed to topic(s): createStock
2022-06-24 14:13:46,295 INFO org.apache.kafka.common.config.AbstractConfig [restartedMain] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-group_id-50
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = group_id
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2022-06-24 14:13:46,299 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka version: 2.7.1
2022-06-24 14:13:46,299 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka commitId: 61dbce85d0d41457
2022-06-24 14:13:46,299 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka startTimeMs: 1656060226299
2022-06-24 14:13:46,299 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-49, groupId=group_id] Cluster ID: 5kpO7pNKTFqroM6YwsJ86A
2022-06-24 14:13:46,300 INFO org.apache.kafka.clients.consumer.KafkaConsumer [restartedMain] [Consumer clientId=consumer-group_id-50, groupId=group_id] Subscribed to topic(s): createUser
2022-06-24 14:13:46,300 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-49, groupId=group_id] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2022-06-24 14:13:46,301 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-49, groupId=group_id] (Re-)joining group
2022-06-24 14:13:46,303 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-49, groupId=group_id] (Re-)joining group
2022-06-24 14:13:46,303 INFO org.apache.kafka.common.config.AbstractConfig [restartedMain] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-group_id-51
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = group_id
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2022-06-24 14:13:46,305 INFO com.netflix.discovery.DiscoveryClient [DiscoveryClient-InstanceInfoReplicator-0] DiscoveryClient_ESTOCKMARKET-QUERY/host.docker.internal:estockmarket-query:6093 - registration status: 204
2022-06-24 14:13:46,307 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-50, groupId=group_id] Cluster ID: 5kpO7pNKTFqroM6YwsJ86A
2022-06-24 14:13:46,307 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-49, groupId=group_id] Successfully joined group with generation Generation{generationId=93, memberId='consumer-group_id-49-e8702a95-7f9f-49e1-8c7f-0f5d1e720751', protocol='range'}
2022-06-24 14:13:46,307 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka version: 2.7.1
2022-06-24 14:13:46,307 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-50, groupId=group_id] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2022-06-24 14:13:46,307 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-49, groupId=group_id] Finished assignment for group at generation 93: {consumer-group_id-49-e8702a95-7f9f-49e1-8c7f-0f5d1e720751=Assignment(partitions=[createStock-0])}
2022-06-24 14:13:46,308 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka commitId: 61dbce85d0d41457
2022-06-24 14:13:46,308 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka startTimeMs: 1656060226307
2022-06-24 14:13:46,308 INFO org.apache.kafka.clients.consumer.KafkaConsumer [restartedMain] [Consumer clientId=consumer-group_id-51, groupId=group_id] Subscribed to topic(s): removeSector
2022-06-24 14:13:46,308 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-50, groupId=group_id] (Re-)joining group
2022-06-24 14:13:46,318 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-50, groupId=group_id] (Re-)joining group
2022-06-24 14:13:46,318 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-49, groupId=group_id] Successfully synced group in generation Generation{generationId=93, memberId='consumer-group_id-49-e8702a95-7f9f-49e1-8c7f-0f5d1e720751', protocol='range'}
2022-06-24 14:13:46,319 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-49, groupId=group_id] Notifying assignor about the new Assignment(partitions=[createStock-0])
2022-06-24 14:13:46,319 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-49, groupId=group_id] Adding newly assigned partitions: createStock-0
2022-06-24 14:13:46,320 INFO org.apache.kafka.common.config.AbstractConfig [restartedMain] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-group_id-52
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = group_id
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2022-06-24 14:13:46,321 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-51, groupId=group_id] Cluster ID: 5kpO7pNKTFqroM6YwsJ86A
2022-06-24 14:13:46,322 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-51, groupId=group_id] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2022-06-24 14:13:46,323 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-51, groupId=group_id] (Re-)joining group
2022-06-24 14:13:46,323 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka version: 2.7.1
2022-06-24 14:13:46,323 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka commitId: 61dbce85d0d41457
2022-06-24 14:13:46,323 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka startTimeMs: 1656060226323
2022-06-24 14:13:46,323 INFO org.apache.kafka.clients.consumer.KafkaConsumer [restartedMain] [Consumer clientId=consumer-group_id-52, groupId=group_id] Subscribed to topic(s): createSector
2022-06-24 14:13:46,323 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-49, groupId=group_id] Setting offset for partition createStock-0 to the committed offset FetchPosition{offset=3, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:13:46,324 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] group_id: partitions assigned: [createStock-0]
2022-06-24 14:13:46,326 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-51, groupId=group_id] (Re-)joining group
2022-06-24 14:13:46,329 INFO org.apache.kafka.common.config.AbstractConfig [restartedMain] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-group_id-53
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = group_id
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2022-06-24 14:13:46,330 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-52, groupId=group_id] Cluster ID: 5kpO7pNKTFqroM6YwsJ86A
2022-06-24 14:13:46,331 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-52, groupId=group_id] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2022-06-24 14:13:46,331 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-52, groupId=group_id] (Re-)joining group
2022-06-24 14:13:46,334 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka version: 2.7.1
2022-06-24 14:13:46,334 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka commitId: 61dbce85d0d41457
2022-06-24 14:13:46,334 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka startTimeMs: 1656060226334
2022-06-24 14:13:46,334 INFO org.apache.kafka.clients.consumer.KafkaConsumer [restartedMain] [Consumer clientId=consumer-group_id-53, groupId=group_id] Subscribed to topic(s): removeStock
2022-06-24 14:13:46,335 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-52, groupId=group_id] (Re-)joining group
2022-06-24 14:13:46,336 INFO org.apache.kafka.common.config.AbstractConfig [restartedMain] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-group_id-54
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = group_id
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2022-06-24 14:13:46,339 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-53, groupId=group_id] Cluster ID: 5kpO7pNKTFqroM6YwsJ86A
2022-06-24 14:13:46,340 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-53, groupId=group_id] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2022-06-24 14:13:46,340 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka version: 2.7.1
2022-06-24 14:13:46,340 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka commitId: 61dbce85d0d41457
2022-06-24 14:13:46,340 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka startTimeMs: 1656060226340
2022-06-24 14:13:46,340 INFO org.apache.kafka.clients.consumer.KafkaConsumer [restartedMain] [Consumer clientId=consumer-group_id-54, groupId=group_id] Subscribed to topic(s): updateStock
2022-06-24 14:13:46,341 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-53, groupId=group_id] (Re-)joining group
2022-06-24 14:13:46,343 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-53, groupId=group_id] (Re-)joining group
2022-06-24 14:13:46,344 INFO org.apache.kafka.common.config.AbstractConfig [restartedMain] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-group_id-55
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = group_id
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2022-06-24 14:13:46,345 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-54, groupId=group_id] Cluster ID: 5kpO7pNKTFqroM6YwsJ86A
2022-06-24 14:13:46,346 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-54, groupId=group_id] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2022-06-24 14:13:46,346 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka version: 2.7.1
2022-06-24 14:13:46,346 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka commitId: 61dbce85d0d41457
2022-06-24 14:13:46,347 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-54, groupId=group_id] (Re-)joining group
2022-06-24 14:13:46,347 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka startTimeMs: 1656060226346
2022-06-24 14:13:46,348 INFO org.apache.kafka.clients.consumer.KafkaConsumer [restartedMain] [Consumer clientId=consumer-group_id-55, groupId=group_id] Subscribed to topic(s): removeCompany
2022-06-24 14:13:46,350 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-54, groupId=group_id] (Re-)joining group
2022-06-24 14:13:46,351 INFO org.apache.kafka.common.config.AbstractConfig [restartedMain] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-group_id-56
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = group_id
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2022-06-24 14:13:46,352 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-55, groupId=group_id] Cluster ID: 5kpO7pNKTFqroM6YwsJ86A
2022-06-24 14:13:46,353 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-55, groupId=group_id] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2022-06-24 14:13:46,353 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-55, groupId=group_id] (Re-)joining group
2022-06-24 14:13:46,355 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka version: 2.7.1
2022-06-24 14:13:46,355 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka commitId: 61dbce85d0d41457
2022-06-24 14:13:46,355 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka startTimeMs: 1656060226355
2022-06-24 14:13:46,355 INFO org.apache.kafka.clients.consumer.KafkaConsumer [restartedMain] [Consumer clientId=consumer-group_id-56, groupId=group_id] Subscribed to topic(s): createCompany
2022-06-24 14:13:46,356 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-55, groupId=group_id] (Re-)joining group
2022-06-24 14:13:46,357 INFO org.apache.juli.logging.DirectJDKLog [restartedMain] Starting ProtocolHandler ["http-nio-6093"]
2022-06-24 14:13:46,360 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-56, groupId=group_id] Cluster ID: 5kpO7pNKTFqroM6YwsJ86A
2022-06-24 14:13:46,360 INFO org.springframework.boot.web.embedded.tomcat.TomcatWebServer [restartedMain] Tomcat started on port(s): 6093 (http) with context path ''
2022-06-24 14:13:46,360 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-56, groupId=group_id] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2022-06-24 14:13:46,361 INFO org.springframework.cloud.netflix.eureka.serviceregistry.EurekaAutoServiceRegistration [restartedMain] Updating port to 6093
2022-06-24 14:13:46,361 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-56, groupId=group_id] (Re-)joining group
2022-06-24 14:13:46,363 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-56, groupId=group_id] (Re-)joining group
2022-06-24 14:13:46,400 INFO springfox.documentation.spring.web.plugins.DocumentationPluginsBootstrapper [restartedMain] Context refreshed
2022-06-24 14:13:46,401 INFO springfox.documentation.spring.web.plugins.DocumentationPluginsBootstrapper [restartedMain] Found 1 custom documentation plugin(s)
2022-06-24 14:13:46,402 INFO springfox.documentation.spring.web.scanners.ApiListingReferenceScanner [restartedMain] Scanning for api listing references
2022-06-24 14:13:46,434 INFO org.springframework.boot.StartupInfoLogger [restartedMain] Started EstockmarketQueryApplication in 1.451 seconds (JVM running for 53940.742)
2022-06-24 14:13:46,438 INFO org.springframework.boot.devtools.autoconfigure.ConditionEvaluationDeltaLoggingListener [restartedMain] Condition evaluation unchanged
2022-06-24 14:13:49,317 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-49, groupId=group_id] Attempt to heartbeat failed since group is rebalancing
2022-06-24 14:13:49,318 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-49, groupId=group_id] Revoke previously assigned partitions createStock-0
2022-06-24 14:13:49,318 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] group_id: partitions revoked: [createStock-0]
2022-06-24 14:13:49,319 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-49, groupId=group_id] (Re-)joining group
2022-06-24 14:13:49,321 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-55, groupId=group_id] Successfully joined group with generation Generation{generationId=94, memberId='consumer-group_id-55-3960580f-41ff-460c-8114-e5aa576b02e3', protocol='range'}
2022-06-24 14:13:49,322 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-51, groupId=group_id] Successfully joined group with generation Generation{generationId=94, memberId='consumer-group_id-51-24771a7d-e030-442f-b58e-4a34f5de4eaf', protocol='range'}
2022-06-24 14:13:49,322 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-50, groupId=group_id] Successfully joined group with generation Generation{generationId=94, memberId='consumer-group_id-50-f4bf8c47-9151-4050-9deb-ce04ad2a4f50', protocol='range'}
2022-06-24 14:13:49,322 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-49, groupId=group_id] Successfully joined group with generation Generation{generationId=94, memberId='consumer-group_id-49-e8702a95-7f9f-49e1-8c7f-0f5d1e720751', protocol='range'}
2022-06-24 14:13:49,322 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-53, groupId=group_id] Successfully joined group with generation Generation{generationId=94, memberId='consumer-group_id-53-8045a4fa-8605-4d78-a881-6a398c16a339', protocol='range'}
2022-06-24 14:13:49,322 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-52, groupId=group_id] Successfully joined group with generation Generation{generationId=94, memberId='consumer-group_id-52-2269fabf-3ca7-4efa-b173-a626749028fa', protocol='range'}
2022-06-24 14:13:49,322 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-56, groupId=group_id] Successfully joined group with generation Generation{generationId=94, memberId='consumer-group_id-56-95327f2e-9106-45d4-a330-3dcb920ec917', protocol='range'}
2022-06-24 14:13:49,322 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-54, groupId=group_id] Successfully joined group with generation Generation{generationId=94, memberId='consumer-group_id-54-8e49347a-cba4-477a-b630-9c256a88eeb6', protocol='range'}
2022-06-24 14:13:49,393 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-49, groupId=group_id] Finished assignment for group at generation 94: {consumer-group_id-56-95327f2e-9106-45d4-a330-3dcb920ec917=Assignment(partitions=[createCompany-0]), consumer-group_id-51-24771a7d-e030-442f-b58e-4a34f5de4eaf=Assignment(partitions=[removeSector-0]), consumer-group_id-49-e8702a95-7f9f-49e1-8c7f-0f5d1e720751=Assignment(partitions=[createStock-0]), consumer-group_id-50-f4bf8c47-9151-4050-9deb-ce04ad2a4f50=Assignment(partitions=[createUser-0]), consumer-group_id-52-2269fabf-3ca7-4efa-b173-a626749028fa=Assignment(partitions=[createSector-0]), consumer-group_id-53-8045a4fa-8605-4d78-a881-6a398c16a339=Assignment(partitions=[removeStock-0]), consumer-group_id-54-8e49347a-cba4-477a-b630-9c256a88eeb6=Assignment(partitions=[updateStock-0]), consumer-group_id-55-3960580f-41ff-460c-8114-e5aa576b02e3=Assignment(partitions=[removeCompany-0])}
2022-06-24 14:13:49,401 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-50, groupId=group_id] Successfully synced group in generation Generation{generationId=94, memberId='consumer-group_id-50-f4bf8c47-9151-4050-9deb-ce04ad2a4f50', protocol='range'}
2022-06-24 14:13:49,404 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-54, groupId=group_id] Successfully synced group in generation Generation{generationId=94, memberId='consumer-group_id-54-8e49347a-cba4-477a-b630-9c256a88eeb6', protocol='range'}
2022-06-24 14:13:49,403 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-49, groupId=group_id] Successfully synced group in generation Generation{generationId=94, memberId='consumer-group_id-49-e8702a95-7f9f-49e1-8c7f-0f5d1e720751', protocol='range'}
2022-06-24 14:13:49,404 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-56, groupId=group_id] Successfully synced group in generation Generation{generationId=94, memberId='consumer-group_id-56-95327f2e-9106-45d4-a330-3dcb920ec917', protocol='range'}
2022-06-24 14:13:49,404 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-52, groupId=group_id] Successfully synced group in generation Generation{generationId=94, memberId='consumer-group_id-52-2269fabf-3ca7-4efa-b173-a626749028fa', protocol='range'}
2022-06-24 14:13:49,405 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-50, groupId=group_id] Notifying assignor about the new Assignment(partitions=[createUser-0])
2022-06-24 14:13:49,406 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-56, groupId=group_id] Notifying assignor about the new Assignment(partitions=[createCompany-0])
2022-06-24 14:13:49,406 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-49, groupId=group_id] Notifying assignor about the new Assignment(partitions=[createStock-0])
2022-06-24 14:13:49,404 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-53, groupId=group_id] Successfully synced group in generation Generation{generationId=94, memberId='consumer-group_id-53-8045a4fa-8605-4d78-a881-6a398c16a339', protocol='range'}
2022-06-24 14:13:49,407 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-52, groupId=group_id] Notifying assignor about the new Assignment(partitions=[createSector-0])
2022-06-24 14:13:49,405 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-51, groupId=group_id] Successfully synced group in generation Generation{generationId=94, memberId='consumer-group_id-51-24771a7d-e030-442f-b58e-4a34f5de4eaf', protocol='range'}
2022-06-24 14:13:49,408 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-53, groupId=group_id] Notifying assignor about the new Assignment(partitions=[removeStock-0])
2022-06-24 14:13:49,404 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-55, groupId=group_id] Successfully synced group in generation Generation{generationId=94, memberId='consumer-group_id-55-3960580f-41ff-460c-8114-e5aa576b02e3', protocol='range'}
2022-06-24 14:13:49,409 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-49, groupId=group_id] Adding newly assigned partitions: createStock-0
2022-06-24 14:13:49,409 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-53, groupId=group_id] Adding newly assigned partitions: removeStock-0
2022-06-24 14:13:49,407 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-56, groupId=group_id] Adding newly assigned partitions: createCompany-0
2022-06-24 14:13:49,407 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-54, groupId=group_id] Notifying assignor about the new Assignment(partitions=[updateStock-0])
2022-06-24 14:13:49,408 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-50, groupId=group_id] Adding newly assigned partitions: createUser-0
2022-06-24 14:13:49,409 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-52, groupId=group_id] Adding newly assigned partitions: createSector-0
2022-06-24 14:13:49,410 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-51, groupId=group_id] Notifying assignor about the new Assignment(partitions=[removeSector-0])
2022-06-24 14:13:49,411 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-51, groupId=group_id] Adding newly assigned partitions: removeSector-0
2022-06-24 14:13:49,411 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-55, groupId=group_id] Notifying assignor about the new Assignment(partitions=[removeCompany-0])
2022-06-24 14:13:49,411 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-54, groupId=group_id] Adding newly assigned partitions: updateStock-0
2022-06-24 14:13:49,412 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-55, groupId=group_id] Adding newly assigned partitions: removeCompany-0
2022-06-24 14:13:49,412 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-56, groupId=group_id] Setting offset for partition createCompany-0 to the committed offset FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:13:49,412 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-49, groupId=group_id] Setting offset for partition createStock-0 to the committed offset FetchPosition{offset=3, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:13:49,412 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] group_id: partitions assigned: [createCompany-0]
2022-06-24 14:13:49,412 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] group_id: partitions assigned: [createStock-0]
2022-06-24 14:13:49,414 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-53, groupId=group_id] Setting offset for partition removeStock-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:13:49,414 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-52, groupId=group_id] Setting offset for partition createSector-0 to the committed offset FetchPosition{offset=1, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:13:49,414 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-51, groupId=group_id] Setting offset for partition removeSector-0 to the committed offset FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:13:49,414 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] group_id: partitions assigned: [createSector-0]
2022-06-24 14:13:49,414 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] group_id: partitions assigned: [removeSector-0]
2022-06-24 14:13:49,414 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-55, groupId=group_id] Setting offset for partition removeCompany-0 to the committed offset FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:13:49,414 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-50, groupId=group_id] Setting offset for partition createUser-0 to the committed offset FetchPosition{offset=1, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:13:49,414 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] group_id: partitions assigned: [removeCompany-0]
2022-06-24 14:13:49,414 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] group_id: partitions assigned: [removeStock-0]
2022-06-24 14:13:49,414 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-54, groupId=group_id] Setting offset for partition updateStock-0 to the committed offset FetchPosition{offset=7, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:13:49,415 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] group_id: partitions assigned: [updateStock-0]
2022-06-24 14:13:49,414 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] group_id: partitions assigned: [createUser-0]
2022-06-24 14:13:57,813 INFO org.springframework.cloud.netflix.eureka.serviceregistry.EurekaServiceRegistry [Thread-39] Unregistering application ESTOCKMARKET-QUERY with eureka with status DOWN
2022-06-24 14:13:57,813 INFO com.netflix.discovery.DiscoveryClient$3 [Thread-39] Saw local status change event StatusChangeEvent [timestamp=1656060237813, current=DOWN, previous=UP]
2022-06-24 14:13:57,814 INFO com.netflix.discovery.DiscoveryClient [DiscoveryClient-InstanceInfoReplicator-0] DiscoveryClient_ESTOCKMARKET-QUERY/host.docker.internal:estockmarket-query:6093: registering service...
2022-06-24 14:13:57,825 INFO com.netflix.discovery.DiscoveryClient [DiscoveryClient-InstanceInfoReplicator-0] DiscoveryClient_ESTOCKMARKET-QUERY/host.docker.internal:estockmarket-query:6093 - registration status: 204
2022-06-24 14:13:57,872 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-49, groupId=group_id] Revoke previously assigned partitions createStock-0
2022-06-24 14:13:57,872 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] group_id: partitions revoked: [createStock-0]
2022-06-24 14:13:57,872 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-50, groupId=group_id] Revoke previously assigned partitions createUser-0
2022-06-24 14:13:57,872 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-51, groupId=group_id] Revoke previously assigned partitions removeSector-0
2022-06-24 14:13:57,873 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] group_id: partitions revoked: [createUser-0]
2022-06-24 14:13:57,873 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-49, groupId=group_id] Member consumer-group_id-49-e8702a95-7f9f-49e1-8c7f-0f5d1e720751 sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2022-06-24 14:13:57,873 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] group_id: partitions revoked: [removeSector-0]
2022-06-24 14:13:57,873 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-52, groupId=group_id] Revoke previously assigned partitions createSector-0
2022-06-24 14:13:57,873 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-49, groupId=group_id] Unsubscribed all topics or patterns and assigned partitions
2022-06-24 14:13:57,873 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-54, groupId=group_id] Revoke previously assigned partitions updateStock-0
2022-06-24 14:13:57,873 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-51, groupId=group_id] Member consumer-group_id-51-24771a7d-e030-442f-b58e-4a34f5de4eaf sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2022-06-24 14:13:57,873 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] group_id: partitions revoked: [updateStock-0]
2022-06-24 14:13:57,873 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-50, groupId=group_id] Member consumer-group_id-50-f4bf8c47-9151-4050-9deb-ce04ad2a4f50 sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2022-06-24 14:13:57,873 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-56, groupId=group_id] Revoke previously assigned partitions createCompany-0
2022-06-24 14:13:57,873 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-54, groupId=group_id] Member consumer-group_id-54-8e49347a-cba4-477a-b630-9c256a88eeb6 sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2022-06-24 14:13:57,873 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] group_id: partitions revoked: [createCompany-0]
2022-06-24 14:13:57,873 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-51, groupId=group_id] Unsubscribed all topics or patterns and assigned partitions
2022-06-24 14:13:57,873 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-56, groupId=group_id] Member consumer-group_id-56-95327f2e-9106-45d4-a330-3dcb920ec917 sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2022-06-24 14:13:57,873 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-50, groupId=group_id] Unsubscribed all topics or patterns and assigned partitions
2022-06-24 14:13:57,873 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-54, groupId=group_id] Unsubscribed all topics or patterns and assigned partitions
2022-06-24 14:13:57,873 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-55, groupId=group_id] Revoke previously assigned partitions removeCompany-0
2022-06-24 14:13:57,873 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-56, groupId=group_id] Unsubscribed all topics or patterns and assigned partitions
2022-06-24 14:13:57,873 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] group_id: partitions revoked: [removeCompany-0]
2022-06-24 14:13:57,873 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-55, groupId=group_id] Member consumer-group_id-55-3960580f-41ff-460c-8114-e5aa576b02e3 sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2022-06-24 14:13:57,873 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-53, groupId=group_id] Revoke previously assigned partitions removeStock-0
2022-06-24 14:13:57,874 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] group_id: partitions revoked: [removeStock-0]
2022-06-24 14:13:57,873 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] group_id: partitions revoked: [createSector-0]
2022-06-24 14:13:57,874 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-55, groupId=group_id] Unsubscribed all topics or patterns and assigned partitions
2022-06-24 14:13:57,874 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-53, groupId=group_id] Member consumer-group_id-53-8045a4fa-8605-4d78-a881-6a398c16a339 sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2022-06-24 14:13:57,874 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-52, groupId=group_id] Member consumer-group_id-52-2269fabf-3ca7-4efa-b173-a626749028fa sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2022-06-24 14:13:57,874 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-53, groupId=group_id] Unsubscribed all topics or patterns and assigned partitions
2022-06-24 14:13:57,875 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-52, groupId=group_id] Unsubscribed all topics or patterns and assigned partitions
2022-06-24 14:13:57,878 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] Metrics scheduler closed
2022-06-24 14:13:57,878 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-06-24 14:13:57,878 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] Metrics reporters closed
2022-06-24 14:13:57,879 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] Metrics scheduler closed
2022-06-24 14:13:57,879 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-06-24 14:13:57,879 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] Metrics reporters closed
2022-06-24 14:13:57,880 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] Metrics scheduler closed
2022-06-24 14:13:57,880 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-06-24 14:13:57,880 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] App info kafka.consumer for consumer-group_id-49 unregistered
2022-06-24 14:13:57,880 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] Metrics reporters closed
2022-06-24 14:13:57,880 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] group_id: Consumer stopped
2022-06-24 14:13:57,880 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] Metrics scheduler closed
2022-06-24 14:13:57,880 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-06-24 14:13:57,880 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] Metrics reporters closed
2022-06-24 14:13:57,882 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Metrics scheduler closed
2022-06-24 14:13:57,882 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-06-24 14:13:57,882 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Metrics scheduler closed
2022-06-24 14:13:57,882 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-06-24 14:13:57,882 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Metrics reporters closed
2022-06-24 14:13:57,882 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Metrics reporters closed
2022-06-24 14:13:57,883 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] Metrics scheduler closed
2022-06-24 14:13:57,883 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-06-24 14:13:57,883 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] Metrics reporters closed
2022-06-24 14:13:57,884 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] App info kafka.consumer for consumer-group_id-51 unregistered
2022-06-24 14:13:57,884 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] group_id: Consumer stopped
2022-06-24 14:13:57,886 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] Metrics scheduler closed
2022-06-24 14:13:57,886 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-06-24 14:13:57,886 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] Metrics reporters closed
2022-06-24 14:13:57,887 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] App info kafka.consumer for consumer-group_id-50 unregistered
2022-06-24 14:13:57,888 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] group_id: Consumer stopped
2022-06-24 14:13:57,889 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] App info kafka.consumer for consumer-group_id-54 unregistered
2022-06-24 14:13:57,889 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] group_id: Consumer stopped
2022-06-24 14:13:57,890 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] App info kafka.consumer for consumer-group_id-53 unregistered
2022-06-24 14:13:57,890 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] group_id: Consumer stopped
2022-06-24 14:13:57,891 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] App info kafka.consumer for consumer-group_id-55 unregistered
2022-06-24 14:13:57,891 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] group_id: Consumer stopped
2022-06-24 14:13:57,891 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] App info kafka.consumer for consumer-group_id-56 unregistered
2022-06-24 14:13:57,891 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] group_id: Consumer stopped
2022-06-24 14:13:57,892 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] App info kafka.consumer for consumer-group_id-52 unregistered
2022-06-24 14:13:57,892 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] group_id: Consumer stopped
2022-06-24 14:13:57,898 INFO com.netflix.discovery.DiscoveryClient [Thread-39] Shutting down DiscoveryClient ...
2022-06-24 14:14:00,907 INFO com.netflix.discovery.DiscoveryClient [Thread-39] Unregistering ...
2022-06-24 14:14:00,924 INFO com.netflix.discovery.DiscoveryClient [Thread-39] DiscoveryClient_ESTOCKMARKET-QUERY/host.docker.internal:estockmarket-query:6093 - deregister  status: 200
2022-06-24 14:14:00,927 INFO com.netflix.discovery.DiscoveryClient [Thread-39] Completed shut down of DiscoveryClient
2022-06-24 14:14:01,202 INFO org.springframework.boot.StartupInfoLogger [restartedMain] Starting EstockmarketQueryApplication using Java 11.0.12 on ctsjava91 with PID 18180 (C:\Repo\elk-stock\ESTOCKMARKET-FSE\estockmarket-query\target\classes started by cogjava182 in C:\Repo\elk-stock\ESTOCKMARKET-FSE\estockmarket-query)
2022-06-24 14:14:01,202 DEBUG org.springframework.boot.StartupInfoLogger [restartedMain] Running with Spring Boot v2.5.3, Spring v5.3.9
2022-06-24 14:14:01,202 INFO org.springframework.boot.SpringApplication [restartedMain] No active profile set, falling back to default profiles: default
2022-06-24 14:14:01,674 INFO org.springframework.data.repository.config.RepositoryConfigurationDelegate [restartedMain] Bootstrapping Spring Data MongoDB repositories in DEFAULT mode.
2022-06-24 14:14:01,704 INFO org.springframework.data.repository.config.RepositoryConfigurationDelegate [restartedMain] Finished Spring Data repository scanning in 30 ms. Found 4 MongoDB repository interfaces.
2022-06-24 14:14:01,803 INFO org.springframework.cloud.context.scope.GenericScope [restartedMain] BeanFactory id=fff56ba9-7131-3a6a-a033-f341357c57c5
2022-06-24 14:14:01,883 INFO org.springframework.boot.web.embedded.tomcat.TomcatWebServer [restartedMain] Tomcat initialized with port(s): 6093 (http)
2022-06-24 14:14:01,884 INFO org.apache.juli.logging.DirectJDKLog [restartedMain] Initializing ProtocolHandler ["http-nio-6093"]
2022-06-24 14:14:01,884 INFO org.apache.juli.logging.DirectJDKLog [restartedMain] Starting service [Tomcat]
2022-06-24 14:14:01,885 INFO org.apache.juli.logging.DirectJDKLog [restartedMain] Starting Servlet engine: [Apache Tomcat/9.0.50]
2022-06-24 14:14:01,923 INFO org.apache.juli.logging.DirectJDKLog [restartedMain] Initializing Spring embedded WebApplicationContext
2022-06-24 14:14:01,923 INFO org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext [restartedMain] Root WebApplicationContext: initialization completed in 718 ms
2022-06-24 14:14:01,952 INFO com.mongodb.diagnostics.logging.SLF4JLogger [restartedMain] Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2022-06-24 14:14:01,956 INFO com.mongodb.diagnostics.logging.SLF4JLogger [cluster-rtt-ClusterId{value='62b5795152d23d5bec795f9b', description='null'}-localhost:27017] Opened connection [connectionId{localValue:29, serverValue:79}] to localhost:27017
2022-06-24 14:14:01,957 WARN org.springframework.data.convert.CustomConversions [restartedMain] Registering converter from class java.time.LocalDateTime to class org.joda.time.LocalDateTime as reading converter although it doesn't convert from a store-supported type! You might want to check your annotation setup at the converter implementation.
2022-06-24 14:14:01,956 INFO com.mongodb.diagnostics.logging.SLF4JLogger [cluster-ClusterId{value='62b5795152d23d5bec795f9b', description='null'}-localhost:27017] Opened connection [connectionId{localValue:28, serverValue:80}] to localhost:27017
2022-06-24 14:14:01,960 INFO com.mongodb.diagnostics.logging.SLF4JLogger [cluster-ClusterId{value='62b5795152d23d5bec795f9b', description='null'}-localhost:27017] Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1533500}
2022-06-24 14:14:01,984 INFO org.springframework.boot.devtools.autoconfigure.OptionalLiveReloadServer [restartedMain] LiveReload server is running on port 35729
2022-06-24 14:14:02,006 WARN org.springframework.data.convert.CustomConversions [restartedMain] Registering converter from class java.time.LocalDateTime to class org.joda.time.LocalDateTime as reading converter although it doesn't convert from a store-supported type! You might want to check your annotation setup at the converter implementation.
2022-06-24 14:14:02,058 INFO com.estockmarket.query.domain.service.OTPService [restartedMain] inside otp servcie {}
2022-06-24 14:14:02,313 INFO org.springframework.cloud.netflix.eureka.config.DiscoveryClientOptionalArgsConfiguration [restartedMain] Eureka HTTP Client uses RestTemplate.
2022-06-24 14:14:02,369 WARN org.springframework.cloud.loadbalancer.config.LoadBalancerCacheAutoConfiguration$LoadBalancerCaffeineWarnLogger [restartedMain] Spring Cloud LoadBalancer is currently working with the default cache. You can switch to using Caffeine cache, by adding it and org.springframework.cache.caffeine.CaffeineCacheManager to the classpath.
2022-06-24 14:14:02,406 INFO org.springframework.cloud.netflix.eureka.InstanceInfoFactory [restartedMain] Setting initial instance status as: STARTING
2022-06-24 14:14:02,408 INFO com.netflix.discovery.DiscoveryClient [restartedMain] Initializing Eureka in region us-east-1
2022-06-24 14:14:02,408 INFO com.netflix.discovery.shared.resolver.aws.ConfigClusterResolver [restartedMain] Resolving eureka endpoints via configuration
2022-06-24 14:14:02,409 INFO com.netflix.discovery.DiscoveryClient [restartedMain] Disable delta property : false
2022-06-24 14:14:02,409 INFO com.netflix.discovery.DiscoveryClient [restartedMain] Single vip registry refresh property : null
2022-06-24 14:14:02,410 INFO com.netflix.discovery.DiscoveryClient [restartedMain] Force full registry fetch : false
2022-06-24 14:14:02,410 INFO com.netflix.discovery.DiscoveryClient [restartedMain] Application is null : false
2022-06-24 14:14:02,410 INFO com.netflix.discovery.DiscoveryClient [restartedMain] Registered Applications size is zero : true
2022-06-24 14:14:02,410 INFO com.netflix.discovery.DiscoveryClient [restartedMain] Application version is -1: true
2022-06-24 14:14:02,410 INFO com.netflix.discovery.DiscoveryClient [restartedMain] Getting all instance registry info from the eureka server
2022-06-24 14:14:02,423 INFO com.netflix.discovery.DiscoveryClient [restartedMain] The response status is 200
2022-06-24 14:14:02,424 INFO com.netflix.discovery.DiscoveryClient [restartedMain] Starting heartbeat executor: renew interval is: 30
2022-06-24 14:14:02,425 INFO com.netflix.discovery.InstanceInfoReplicator [restartedMain] InstanceInfoReplicator onDemand update allowed rate per min is 4
2022-06-24 14:14:02,426 INFO com.netflix.discovery.DiscoveryClient [restartedMain] Discovery Client initialized at timestamp 1656060242426 with initial instances count: 3
2022-06-24 14:14:02,427 INFO org.springframework.cloud.netflix.eureka.serviceregistry.EurekaServiceRegistry [restartedMain] Registering application ESTOCKMARKET-QUERY with eureka with status UP
2022-06-24 14:14:02,427 INFO com.netflix.discovery.DiscoveryClient$3 [restartedMain] Saw local status change event StatusChangeEvent [timestamp=1656060242427, current=UP, previous=STARTING]
2022-06-24 14:14:02,428 INFO com.netflix.discovery.DiscoveryClient [DiscoveryClient-InstanceInfoReplicator-0] DiscoveryClient_ESTOCKMARKET-QUERY/host.docker.internal:estockmarket-query:6093: registering service...
2022-06-24 14:14:02,429 INFO org.apache.kafka.common.config.AbstractConfig [restartedMain] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-group_id-57
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = group_id
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2022-06-24 14:14:02,432 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka version: 2.7.1
2022-06-24 14:14:02,432 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka commitId: 61dbce85d0d41457
2022-06-24 14:14:02,433 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka startTimeMs: 1656060242432
2022-06-24 14:14:02,433 INFO org.apache.kafka.clients.consumer.KafkaConsumer [restartedMain] [Consumer clientId=consumer-group_id-57, groupId=group_id] Subscribed to topic(s): createStock
2022-06-24 14:14:02,436 INFO org.apache.kafka.common.config.AbstractConfig [restartedMain] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-group_id-58
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = group_id
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2022-06-24 14:14:02,439 INFO com.netflix.discovery.DiscoveryClient [DiscoveryClient-InstanceInfoReplicator-0] DiscoveryClient_ESTOCKMARKET-QUERY/host.docker.internal:estockmarket-query:6093 - registration status: 204
2022-06-24 14:14:02,439 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka version: 2.7.1
2022-06-24 14:14:02,439 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka commitId: 61dbce85d0d41457
2022-06-24 14:14:02,439 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-57, groupId=group_id] Cluster ID: 5kpO7pNKTFqroM6YwsJ86A
2022-06-24 14:14:02,440 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka startTimeMs: 1656060242439
2022-06-24 14:14:02,440 INFO org.apache.kafka.clients.consumer.KafkaConsumer [restartedMain] [Consumer clientId=consumer-group_id-58, groupId=group_id] Subscribed to topic(s): createUser
2022-06-24 14:14:02,440 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-57, groupId=group_id] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2022-06-24 14:14:02,441 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-57, groupId=group_id] (Re-)joining group
2022-06-24 14:14:02,443 INFO org.apache.kafka.common.config.AbstractConfig [restartedMain] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-group_id-59
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = group_id
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2022-06-24 14:14:02,445 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-58, groupId=group_id] Cluster ID: 5kpO7pNKTFqroM6YwsJ86A
2022-06-24 14:14:02,445 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-57, groupId=group_id] (Re-)joining group
2022-06-24 14:14:02,445 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-58, groupId=group_id] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2022-06-24 14:14:02,446 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka version: 2.7.1
2022-06-24 14:14:02,446 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka commitId: 61dbce85d0d41457
2022-06-24 14:14:02,446 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka startTimeMs: 1656060242446
2022-06-24 14:14:02,446 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-58, groupId=group_id] (Re-)joining group
2022-06-24 14:14:02,446 INFO org.apache.kafka.clients.consumer.KafkaConsumer [restartedMain] [Consumer clientId=consumer-group_id-59, groupId=group_id] Subscribed to topic(s): removeSector
2022-06-24 14:14:02,450 INFO org.apache.kafka.common.config.AbstractConfig [restartedMain] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-group_id-60
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = group_id
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2022-06-24 14:14:02,451 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-59, groupId=group_id] Cluster ID: 5kpO7pNKTFqroM6YwsJ86A
2022-06-24 14:14:02,451 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-59, groupId=group_id] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2022-06-24 14:14:02,452 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-59, groupId=group_id] (Re-)joining group
2022-06-24 14:14:02,453 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka version: 2.7.1
2022-06-24 14:14:02,454 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka commitId: 61dbce85d0d41457
2022-06-24 14:14:02,454 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka startTimeMs: 1656060242453
2022-06-24 14:14:02,454 INFO org.apache.kafka.clients.consumer.KafkaConsumer [restartedMain] [Consumer clientId=consumer-group_id-60, groupId=group_id] Subscribed to topic(s): createSector
2022-06-24 14:14:02,457 INFO org.apache.kafka.common.config.AbstractConfig [restartedMain] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-group_id-61
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = group_id
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2022-06-24 14:14:02,458 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-60, groupId=group_id] Cluster ID: 5kpO7pNKTFqroM6YwsJ86A
2022-06-24 14:14:02,458 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-60, groupId=group_id] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2022-06-24 14:14:02,459 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-60, groupId=group_id] (Re-)joining group
2022-06-24 14:14:02,459 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka version: 2.7.1
2022-06-24 14:14:02,459 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka commitId: 61dbce85d0d41457
2022-06-24 14:14:02,459 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka startTimeMs: 1656060242459
2022-06-24 14:14:02,459 INFO org.apache.kafka.clients.consumer.KafkaConsumer [restartedMain] [Consumer clientId=consumer-group_id-61, groupId=group_id] Subscribed to topic(s): removeStock
2022-06-24 14:14:02,462 INFO org.apache.kafka.common.config.AbstractConfig [restartedMain] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-group_id-62
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = group_id
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2022-06-24 14:14:02,462 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-57, groupId=group_id] Successfully joined group with generation Generation{generationId=96, memberId='consumer-group_id-57-0af0198e-3647-4729-a6ed-66613172ded9', protocol='range'}
2022-06-24 14:14:02,463 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-57, groupId=group_id] Finished assignment for group at generation 96: {consumer-group_id-57-0af0198e-3647-4729-a6ed-66613172ded9=Assignment(partitions=[createStock-0])}
2022-06-24 14:14:02,464 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-60, groupId=group_id] (Re-)joining group
2022-06-24 14:14:02,466 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-61, groupId=group_id] Cluster ID: 5kpO7pNKTFqroM6YwsJ86A
2022-06-24 14:14:02,466 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-58, groupId=group_id] (Re-)joining group
2022-06-24 14:14:02,466 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-61, groupId=group_id] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2022-06-24 14:14:02,466 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-59, groupId=group_id] (Re-)joining group
2022-06-24 14:14:02,466 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka version: 2.7.1
2022-06-24 14:14:02,466 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka commitId: 61dbce85d0d41457
2022-06-24 14:14:02,466 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka startTimeMs: 1656060242466
2022-06-24 14:14:02,466 INFO org.apache.kafka.clients.consumer.KafkaConsumer [restartedMain] [Consumer clientId=consumer-group_id-62, groupId=group_id] Subscribed to topic(s): updateStock
2022-06-24 14:14:02,468 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-61, groupId=group_id] (Re-)joining group
2022-06-24 14:14:02,468 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-57, groupId=group_id] Successfully synced group in generation Generation{generationId=96, memberId='consumer-group_id-57-0af0198e-3647-4729-a6ed-66613172ded9', protocol='range'}
2022-06-24 14:14:02,468 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-57, groupId=group_id] Notifying assignor about the new Assignment(partitions=[createStock-0])
2022-06-24 14:14:02,468 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-57, groupId=group_id] Adding newly assigned partitions: createStock-0
2022-06-24 14:14:02,470 INFO org.apache.kafka.common.config.AbstractConfig [restartedMain] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-group_id-63
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = group_id
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2022-06-24 14:14:02,471 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-61, groupId=group_id] (Re-)joining group
2022-06-24 14:14:02,471 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-62, groupId=group_id] Cluster ID: 5kpO7pNKTFqroM6YwsJ86A
2022-06-24 14:14:02,472 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-62, groupId=group_id] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2022-06-24 14:14:02,472 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-57, groupId=group_id] Setting offset for partition createStock-0 to the committed offset FetchPosition{offset=3, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:14:02,473 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-62, groupId=group_id] (Re-)joining group
2022-06-24 14:14:02,473 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] group_id: partitions assigned: [createStock-0]
2022-06-24 14:14:02,474 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka version: 2.7.1
2022-06-24 14:14:02,474 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka commitId: 61dbce85d0d41457
2022-06-24 14:14:02,475 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka startTimeMs: 1656060242474
2022-06-24 14:14:02,475 INFO org.apache.kafka.clients.consumer.KafkaConsumer [restartedMain] [Consumer clientId=consumer-group_id-63, groupId=group_id] Subscribed to topic(s): removeCompany
2022-06-24 14:14:02,475 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-62, groupId=group_id] (Re-)joining group
2022-06-24 14:14:02,477 INFO org.apache.kafka.common.config.AbstractConfig [restartedMain] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-group_id-64
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = group_id
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2022-06-24 14:14:02,479 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-63, groupId=group_id] Cluster ID: 5kpO7pNKTFqroM6YwsJ86A
2022-06-24 14:14:02,486 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-63, groupId=group_id] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2022-06-24 14:14:02,487 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-63, groupId=group_id] (Re-)joining group
2022-06-24 14:14:02,487 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka version: 2.7.1
2022-06-24 14:14:02,487 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka commitId: 61dbce85d0d41457
2022-06-24 14:14:02,487 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka startTimeMs: 1656060242487
2022-06-24 14:14:02,488 INFO org.apache.kafka.clients.consumer.KafkaConsumer [restartedMain] [Consumer clientId=consumer-group_id-64, groupId=group_id] Subscribed to topic(s): createCompany
2022-06-24 14:14:02,489 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-63, groupId=group_id] (Re-)joining group
2022-06-24 14:14:02,489 INFO org.apache.juli.logging.DirectJDKLog [restartedMain] Starting ProtocolHandler ["http-nio-6093"]
2022-06-24 14:14:02,492 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-64, groupId=group_id] Cluster ID: 5kpO7pNKTFqroM6YwsJ86A
2022-06-24 14:14:02,492 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-64, groupId=group_id] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2022-06-24 14:14:02,492 INFO org.springframework.boot.web.embedded.tomcat.TomcatWebServer [restartedMain] Tomcat started on port(s): 6093 (http) with context path ''
2022-06-24 14:14:02,493 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-64, groupId=group_id] (Re-)joining group
2022-06-24 14:14:02,493 INFO org.springframework.cloud.netflix.eureka.serviceregistry.EurekaAutoServiceRegistration [restartedMain] Updating port to 6093
2022-06-24 14:14:02,495 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-64, groupId=group_id] (Re-)joining group
2022-06-24 14:14:02,528 INFO springfox.documentation.spring.web.plugins.DocumentationPluginsBootstrapper [restartedMain] Context refreshed
2022-06-24 14:14:02,529 INFO springfox.documentation.spring.web.plugins.DocumentationPluginsBootstrapper [restartedMain] Found 1 custom documentation plugin(s)
2022-06-24 14:14:02,530 INFO springfox.documentation.spring.web.scanners.ApiListingReferenceScanner [restartedMain] Scanning for api listing references
2022-06-24 14:14:02,559 INFO org.springframework.boot.StartupInfoLogger [restartedMain] Started EstockmarketQueryApplication in 1.443 seconds (JVM running for 53956.868)
2022-06-24 14:14:02,562 INFO org.springframework.boot.devtools.autoconfigure.ConditionEvaluationDeltaLoggingListener [restartedMain] Condition evaluation unchanged
2022-06-24 14:14:05,464 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-57, groupId=group_id] Attempt to heartbeat failed since group is rebalancing
2022-06-24 14:14:05,465 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-57, groupId=group_id] Revoke previously assigned partitions createStock-0
2022-06-24 14:14:05,465 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] group_id: partitions revoked: [createStock-0]
2022-06-24 14:14:05,465 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-57, groupId=group_id] (Re-)joining group
2022-06-24 14:14:05,468 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-60, groupId=group_id] Successfully joined group with generation Generation{generationId=97, memberId='consumer-group_id-60-52ebc9a2-cedc-4927-a8d5-400e686235fb', protocol='range'}
2022-06-24 14:14:05,468 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-58, groupId=group_id] Successfully joined group with generation Generation{generationId=97, memberId='consumer-group_id-58-51bcd1cd-2c41-4398-8248-29bb9f56d5e6', protocol='range'}
2022-06-24 14:14:05,468 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-62, groupId=group_id] Successfully joined group with generation Generation{generationId=97, memberId='consumer-group_id-62-10bd205e-d03b-4131-8470-2b9fe7104efc', protocol='range'}
2022-06-24 14:14:05,468 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-59, groupId=group_id] Successfully joined group with generation Generation{generationId=97, memberId='consumer-group_id-59-0fedfaf4-de5b-446f-9c58-6b84e167afb0', protocol='range'}
2022-06-24 14:14:05,468 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-57, groupId=group_id] Successfully joined group with generation Generation{generationId=97, memberId='consumer-group_id-57-0af0198e-3647-4729-a6ed-66613172ded9', protocol='range'}
2022-06-24 14:14:05,468 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-64, groupId=group_id] Successfully joined group with generation Generation{generationId=97, memberId='consumer-group_id-64-36fc9031-9035-4dff-a6ef-a408054c91d0', protocol='range'}
2022-06-24 14:14:05,468 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-61, groupId=group_id] Successfully joined group with generation Generation{generationId=97, memberId='consumer-group_id-61-ced568da-fb8e-4b16-b92e-a72f27f72d94', protocol='range'}
2022-06-24 14:14:05,468 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-63, groupId=group_id] Successfully joined group with generation Generation{generationId=97, memberId='consumer-group_id-63-06438b57-31e0-4b8b-9464-26069e2a6a21', protocol='range'}
2022-06-24 14:14:05,526 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-57, groupId=group_id] Finished assignment for group at generation 97: {consumer-group_id-59-0fedfaf4-de5b-446f-9c58-6b84e167afb0=Assignment(partitions=[removeSector-0]), consumer-group_id-64-36fc9031-9035-4dff-a6ef-a408054c91d0=Assignment(partitions=[createCompany-0]), consumer-group_id-61-ced568da-fb8e-4b16-b92e-a72f27f72d94=Assignment(partitions=[removeStock-0]), consumer-group_id-62-10bd205e-d03b-4131-8470-2b9fe7104efc=Assignment(partitions=[updateStock-0]), consumer-group_id-58-51bcd1cd-2c41-4398-8248-29bb9f56d5e6=Assignment(partitions=[createUser-0]), consumer-group_id-57-0af0198e-3647-4729-a6ed-66613172ded9=Assignment(partitions=[createStock-0]), consumer-group_id-60-52ebc9a2-cedc-4927-a8d5-400e686235fb=Assignment(partitions=[createSector-0]), consumer-group_id-63-06438b57-31e0-4b8b-9464-26069e2a6a21=Assignment(partitions=[removeCompany-0])}
2022-06-24 14:14:05,530 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-57, groupId=group_id] Successfully synced group in generation Generation{generationId=97, memberId='consumer-group_id-57-0af0198e-3647-4729-a6ed-66613172ded9', protocol='range'}
2022-06-24 14:14:05,532 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-60, groupId=group_id] Successfully synced group in generation Generation{generationId=97, memberId='consumer-group_id-60-52ebc9a2-cedc-4927-a8d5-400e686235fb', protocol='range'}
2022-06-24 14:14:05,532 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-57, groupId=group_id] Notifying assignor about the new Assignment(partitions=[createStock-0])
2022-06-24 14:14:05,533 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-60, groupId=group_id] Notifying assignor about the new Assignment(partitions=[createSector-0])
2022-06-24 14:14:05,534 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-62, groupId=group_id] Successfully synced group in generation Generation{generationId=97, memberId='consumer-group_id-62-10bd205e-d03b-4131-8470-2b9fe7104efc', protocol='range'}
2022-06-24 14:14:05,534 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-61, groupId=group_id] Successfully synced group in generation Generation{generationId=97, memberId='consumer-group_id-61-ced568da-fb8e-4b16-b92e-a72f27f72d94', protocol='range'}
2022-06-24 14:14:05,536 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-58, groupId=group_id] Successfully synced group in generation Generation{generationId=97, memberId='consumer-group_id-58-51bcd1cd-2c41-4398-8248-29bb9f56d5e6', protocol='range'}
2022-06-24 14:14:05,536 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-62, groupId=group_id] Notifying assignor about the new Assignment(partitions=[updateStock-0])
2022-06-24 14:14:05,534 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-64, groupId=group_id] Successfully synced group in generation Generation{generationId=97, memberId='consumer-group_id-64-36fc9031-9035-4dff-a6ef-a408054c91d0', protocol='range'}
2022-06-24 14:14:05,536 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-61, groupId=group_id] Notifying assignor about the new Assignment(partitions=[removeStock-0])
2022-06-24 14:14:05,534 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-57, groupId=group_id] Adding newly assigned partitions: createStock-0
2022-06-24 14:14:05,536 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-58, groupId=group_id] Notifying assignor about the new Assignment(partitions=[createUser-0])
2022-06-24 14:14:05,536 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-60, groupId=group_id] Adding newly assigned partitions: createSector-0
2022-06-24 14:14:05,536 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-64, groupId=group_id] Notifying assignor about the new Assignment(partitions=[createCompany-0])
2022-06-24 14:14:05,536 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-63, groupId=group_id] Successfully synced group in generation Generation{generationId=97, memberId='consumer-group_id-63-06438b57-31e0-4b8b-9464-26069e2a6a21', protocol='range'}
2022-06-24 14:14:05,536 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-64, groupId=group_id] Adding newly assigned partitions: createCompany-0
2022-06-24 14:14:05,536 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-59, groupId=group_id] Successfully synced group in generation Generation{generationId=97, memberId='consumer-group_id-59-0fedfaf4-de5b-446f-9c58-6b84e167afb0', protocol='range'}
2022-06-24 14:14:05,537 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-59, groupId=group_id] Notifying assignor about the new Assignment(partitions=[removeSector-0])
2022-06-24 14:14:05,536 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-62, groupId=group_id] Adding newly assigned partitions: updateStock-0
2022-06-24 14:14:05,536 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-63, groupId=group_id] Notifying assignor about the new Assignment(partitions=[removeCompany-0])
2022-06-24 14:14:05,536 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-61, groupId=group_id] Adding newly assigned partitions: removeStock-0
2022-06-24 14:14:05,537 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-59, groupId=group_id] Adding newly assigned partitions: removeSector-0
2022-06-24 14:14:05,537 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-63, groupId=group_id] Adding newly assigned partitions: removeCompany-0
2022-06-24 14:14:05,536 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-58, groupId=group_id] Adding newly assigned partitions: createUser-0
2022-06-24 14:14:05,539 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-57, groupId=group_id] Setting offset for partition createStock-0 to the committed offset FetchPosition{offset=3, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:14:05,539 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] group_id: partitions assigned: [createStock-0]
2022-06-24 14:14:05,539 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-60, groupId=group_id] Setting offset for partition createSector-0 to the committed offset FetchPosition{offset=1, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:14:05,539 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-63, groupId=group_id] Setting offset for partition removeCompany-0 to the committed offset FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:14:05,539 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] group_id: partitions assigned: [createSector-0]
2022-06-24 14:14:05,539 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-64, groupId=group_id] Setting offset for partition createCompany-0 to the committed offset FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:14:05,539 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] group_id: partitions assigned: [removeCompany-0]
2022-06-24 14:14:05,539 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] group_id: partitions assigned: [createCompany-0]
2022-06-24 14:14:05,539 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-62, groupId=group_id] Setting offset for partition updateStock-0 to the committed offset FetchPosition{offset=7, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:14:05,540 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] group_id: partitions assigned: [updateStock-0]
2022-06-24 14:14:05,540 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-58, groupId=group_id] Setting offset for partition createUser-0 to the committed offset FetchPosition{offset=1, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:14:05,541 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] group_id: partitions assigned: [createUser-0]
2022-06-24 14:14:05,542 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-59, groupId=group_id] Setting offset for partition removeSector-0 to the committed offset FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:14:05,543 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] group_id: partitions assigned: [removeSector-0]
2022-06-24 14:14:05,543 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-61, groupId=group_id] Setting offset for partition removeStock-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:14:05,543 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] group_id: partitions assigned: [removeStock-0]
2022-06-24 14:16:02,722 INFO org.springframework.cloud.netflix.eureka.serviceregistry.EurekaServiceRegistry [Thread-45] Unregistering application ESTOCKMARKET-QUERY with eureka with status DOWN
2022-06-24 14:16:02,723 INFO com.netflix.discovery.DiscoveryClient$3 [Thread-45] Saw local status change event StatusChangeEvent [timestamp=1656060362723, current=DOWN, previous=UP]
2022-06-24 14:16:02,723 INFO com.netflix.discovery.DiscoveryClient [DiscoveryClient-InstanceInfoReplicator-0] DiscoveryClient_ESTOCKMARKET-QUERY/host.docker.internal:estockmarket-query:6093: registering service...
2022-06-24 14:16:02,731 INFO com.netflix.discovery.DiscoveryClient [DiscoveryClient-InstanceInfoReplicator-0] DiscoveryClient_ESTOCKMARKET-QUERY/host.docker.internal:estockmarket-query:6093 - registration status: 204
2022-06-24 14:16:02,779 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-57, groupId=group_id] Revoke previously assigned partitions createStock-0
2022-06-24 14:16:02,779 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] group_id: partitions revoked: [createStock-0]
2022-06-24 14:16:02,779 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-57, groupId=group_id] Member consumer-group_id-57-0af0198e-3647-4729-a6ed-66613172ded9 sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2022-06-24 14:16:02,779 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-59, groupId=group_id] Revoke previously assigned partitions removeSector-0
2022-06-24 14:16:02,779 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-58, groupId=group_id] Revoke previously assigned partitions createUser-0
2022-06-24 14:16:02,779 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] group_id: partitions revoked: [removeSector-0]
2022-06-24 14:16:02,779 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] group_id: partitions revoked: [createUser-0]
2022-06-24 14:16:02,779 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-57, groupId=group_id] Unsubscribed all topics or patterns and assigned partitions
2022-06-24 14:16:02,780 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-63, groupId=group_id] Revoke previously assigned partitions removeCompany-0
2022-06-24 14:16:02,780 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-61, groupId=group_id] Revoke previously assigned partitions removeStock-0
2022-06-24 14:16:02,780 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] group_id: partitions revoked: [removeCompany-0]
2022-06-24 14:16:02,780 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-62, groupId=group_id] Revoke previously assigned partitions updateStock-0
2022-06-24 14:16:02,780 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-58, groupId=group_id] Member consumer-group_id-58-51bcd1cd-2c41-4398-8248-29bb9f56d5e6 sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2022-06-24 14:16:02,780 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-63, groupId=group_id] Member consumer-group_id-63-06438b57-31e0-4b8b-9464-26069e2a6a21 sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2022-06-24 14:16:02,780 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-60, groupId=group_id] Revoke previously assigned partitions createSector-0
2022-06-24 14:16:02,779 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-59, groupId=group_id] Member consumer-group_id-59-0fedfaf4-de5b-446f-9c58-6b84e167afb0 sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2022-06-24 14:16:02,780 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] group_id: partitions revoked: [removeStock-0]
2022-06-24 14:16:02,780 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-58, groupId=group_id] Unsubscribed all topics or patterns and assigned partitions
2022-06-24 14:16:02,780 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-63, groupId=group_id] Unsubscribed all topics or patterns and assigned partitions
2022-06-24 14:16:02,780 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-59, groupId=group_id] Unsubscribed all topics or patterns and assigned partitions
2022-06-24 14:16:02,780 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] group_id: partitions revoked: [updateStock-0]
2022-06-24 14:16:02,780 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-61, groupId=group_id] Member consumer-group_id-61-ced568da-fb8e-4b16-b92e-a72f27f72d94 sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2022-06-24 14:16:02,780 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] group_id: partitions revoked: [createSector-0]
2022-06-24 14:16:02,780 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-62, groupId=group_id] Member consumer-group_id-62-10bd205e-d03b-4131-8470-2b9fe7104efc sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2022-06-24 14:16:02,780 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-60, groupId=group_id] Member consumer-group_id-60-52ebc9a2-cedc-4927-a8d5-400e686235fb sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2022-06-24 14:16:02,781 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-61, groupId=group_id] Unsubscribed all topics or patterns and assigned partitions
2022-06-24 14:16:02,781 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-62, groupId=group_id] Unsubscribed all topics or patterns and assigned partitions
2022-06-24 14:16:02,781 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-60, groupId=group_id] Unsubscribed all topics or patterns and assigned partitions
2022-06-24 14:16:02,781 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-64, groupId=group_id] Revoke previously assigned partitions createCompany-0
2022-06-24 14:16:02,781 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] group_id: partitions revoked: [createCompany-0]
2022-06-24 14:16:02,781 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-64, groupId=group_id] Member consumer-group_id-64-36fc9031-9035-4dff-a6ef-a408054c91d0 sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2022-06-24 14:16:02,781 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-64, groupId=group_id] Unsubscribed all topics or patterns and assigned partitions
2022-06-24 14:16:02,784 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] Metrics scheduler closed
2022-06-24 14:16:02,785 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-06-24 14:16:02,785 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] Metrics reporters closed
2022-06-24 14:16:02,786 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] App info kafka.consumer for consumer-group_id-57 unregistered
2022-06-24 14:16:02,786 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] group_id: Consumer stopped
2022-06-24 14:16:02,791 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] Metrics scheduler closed
2022-06-24 14:16:02,791 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-06-24 14:16:02,791 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] Metrics reporters closed
2022-06-24 14:16:02,792 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Metrics scheduler closed
2022-06-24 14:16:02,792 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-06-24 14:16:02,793 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Metrics reporters closed
2022-06-24 14:16:02,793 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] App info kafka.consumer for consumer-group_id-58 unregistered
2022-06-24 14:16:02,793 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] group_id: Consumer stopped
2022-06-24 14:16:02,794 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] App info kafka.consumer for consumer-group_id-63 unregistered
2022-06-24 14:16:02,794 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] group_id: Consumer stopped
2022-06-24 14:16:02,795 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] Metrics scheduler closed
2022-06-24 14:16:02,795 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-06-24 14:16:02,795 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] Metrics reporters closed
2022-06-24 14:16:02,796 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] App info kafka.consumer for consumer-group_id-61 unregistered
2022-06-24 14:16:02,796 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] group_id: Consumer stopped
2022-06-24 14:16:02,797 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] Metrics scheduler closed
2022-06-24 14:16:02,797 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-06-24 14:16:02,797 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] Metrics reporters closed
2022-06-24 14:16:02,799 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] App info kafka.consumer for consumer-group_id-59 unregistered
2022-06-24 14:16:02,799 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] group_id: Consumer stopped
2022-06-24 14:16:02,800 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Metrics scheduler closed
2022-06-24 14:16:02,800 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-06-24 14:16:02,800 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Metrics reporters closed
2022-06-24 14:16:02,801 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] App info kafka.consumer for consumer-group_id-62 unregistered
2022-06-24 14:16:02,801 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] group_id: Consumer stopped
2022-06-24 14:16:02,802 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] Metrics scheduler closed
2022-06-24 14:16:02,802 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-06-24 14:16:02,803 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] Metrics reporters closed
2022-06-24 14:16:02,804 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] App info kafka.consumer for consumer-group_id-60 unregistered
2022-06-24 14:16:02,804 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] group_id: Consumer stopped
2022-06-24 14:16:02,809 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] Metrics scheduler closed
2022-06-24 14:16:02,809 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-06-24 14:16:02,809 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] Metrics reporters closed
2022-06-24 14:16:02,810 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] App info kafka.consumer for consumer-group_id-64 unregistered
2022-06-24 14:16:02,811 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] group_id: Consumer stopped
2022-06-24 14:16:02,828 INFO com.netflix.discovery.DiscoveryClient [Thread-45] Shutting down DiscoveryClient ...
2022-06-24 14:16:05,831 INFO com.netflix.discovery.DiscoveryClient [Thread-45] Unregistering ...
2022-06-24 14:16:05,846 INFO com.netflix.discovery.DiscoveryClient [Thread-45] DiscoveryClient_ESTOCKMARKET-QUERY/host.docker.internal:estockmarket-query:6093 - deregister  status: 200
2022-06-24 14:16:05,849 INFO com.netflix.discovery.DiscoveryClient [Thread-45] Completed shut down of DiscoveryClient
2022-06-24 14:16:06,044 INFO org.springframework.boot.StartupInfoLogger [restartedMain] Starting EstockmarketQueryApplication using Java 11.0.12 on ctsjava91 with PID 18180 (C:\Repo\elk-stock\ESTOCKMARKET-FSE\estockmarket-query\target\classes started by cogjava182 in C:\Repo\elk-stock\ESTOCKMARKET-FSE\estockmarket-query)
2022-06-24 14:16:06,045 DEBUG org.springframework.boot.StartupInfoLogger [restartedMain] Running with Spring Boot v2.5.3, Spring v5.3.9
2022-06-24 14:16:06,045 INFO org.springframework.boot.SpringApplication [restartedMain] No active profile set, falling back to default profiles: default
2022-06-24 14:16:06,499 INFO org.springframework.data.repository.config.RepositoryConfigurationDelegate [restartedMain] Bootstrapping Spring Data MongoDB repositories in DEFAULT mode.
2022-06-24 14:16:06,530 INFO org.springframework.data.repository.config.RepositoryConfigurationDelegate [restartedMain] Finished Spring Data repository scanning in 30 ms. Found 4 MongoDB repository interfaces.
2022-06-24 14:16:06,617 INFO org.springframework.cloud.context.scope.GenericScope [restartedMain] BeanFactory id=fff56ba9-7131-3a6a-a033-f341357c57c5
2022-06-24 14:16:06,693 INFO org.springframework.boot.web.embedded.tomcat.TomcatWebServer [restartedMain] Tomcat initialized with port(s): 6093 (http)
2022-06-24 14:16:06,694 INFO org.apache.juli.logging.DirectJDKLog [restartedMain] Initializing ProtocolHandler ["http-nio-6093"]
2022-06-24 14:16:06,694 INFO org.apache.juli.logging.DirectJDKLog [restartedMain] Starting service [Tomcat]
2022-06-24 14:16:06,694 INFO org.apache.juli.logging.DirectJDKLog [restartedMain] Starting Servlet engine: [Apache Tomcat/9.0.50]
2022-06-24 14:16:06,731 INFO org.apache.juli.logging.DirectJDKLog [restartedMain] Initializing Spring embedded WebApplicationContext
2022-06-24 14:16:06,731 INFO org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext [restartedMain] Root WebApplicationContext: initialization completed in 684 ms
2022-06-24 14:16:06,761 INFO com.mongodb.diagnostics.logging.SLF4JLogger [restartedMain] Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}
2022-06-24 14:16:06,764 INFO com.mongodb.diagnostics.logging.SLF4JLogger [cluster-ClusterId{value='62b579ce52d23d5bec795f9c', description='null'}-localhost:27017] Opened connection [connectionId{localValue:30, serverValue:81}] to localhost:27017
2022-06-24 14:16:06,766 INFO com.mongodb.diagnostics.logging.SLF4JLogger [cluster-rtt-ClusterId{value='62b579ce52d23d5bec795f9c', description='null'}-localhost:27017] Opened connection [connectionId{localValue:31, serverValue:82}] to localhost:27017
2022-06-24 14:16:06,768 INFO com.mongodb.diagnostics.logging.SLF4JLogger [cluster-ClusterId{value='62b579ce52d23d5bec795f9c', description='null'}-localhost:27017] Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1218300}
2022-06-24 14:16:06,768 WARN org.springframework.data.convert.CustomConversions [restartedMain] Registering converter from class java.time.LocalDateTime to class org.joda.time.LocalDateTime as reading converter although it doesn't convert from a store-supported type! You might want to check your annotation setup at the converter implementation.
2022-06-24 14:16:06,795 INFO org.springframework.boot.devtools.autoconfigure.OptionalLiveReloadServer [restartedMain] LiveReload server is running on port 35729
2022-06-24 14:16:06,813 WARN org.springframework.data.convert.CustomConversions [restartedMain] Registering converter from class java.time.LocalDateTime to class org.joda.time.LocalDateTime as reading converter although it doesn't convert from a store-supported type! You might want to check your annotation setup at the converter implementation.
2022-06-24 14:16:06,854 INFO com.estockmarket.query.domain.service.OTPService [restartedMain] inside otp servcie {}
2022-06-24 14:16:07,096 INFO org.springframework.cloud.netflix.eureka.config.DiscoveryClientOptionalArgsConfiguration [restartedMain] Eureka HTTP Client uses RestTemplate.
2022-06-24 14:16:07,152 WARN org.springframework.cloud.loadbalancer.config.LoadBalancerCacheAutoConfiguration$LoadBalancerCaffeineWarnLogger [restartedMain] Spring Cloud LoadBalancer is currently working with the default cache. You can switch to using Caffeine cache, by adding it and org.springframework.cache.caffeine.CaffeineCacheManager to the classpath.
2022-06-24 14:16:07,195 INFO org.springframework.cloud.netflix.eureka.InstanceInfoFactory [restartedMain] Setting initial instance status as: STARTING
2022-06-24 14:16:07,197 INFO com.netflix.discovery.DiscoveryClient [restartedMain] Initializing Eureka in region us-east-1
2022-06-24 14:16:07,197 INFO com.netflix.discovery.shared.resolver.aws.ConfigClusterResolver [restartedMain] Resolving eureka endpoints via configuration
2022-06-24 14:16:07,198 INFO com.netflix.discovery.DiscoveryClient [restartedMain] Disable delta property : false
2022-06-24 14:16:07,198 INFO com.netflix.discovery.DiscoveryClient [restartedMain] Single vip registry refresh property : null
2022-06-24 14:16:07,199 INFO com.netflix.discovery.DiscoveryClient [restartedMain] Force full registry fetch : false
2022-06-24 14:16:07,199 INFO com.netflix.discovery.DiscoveryClient [restartedMain] Application is null : false
2022-06-24 14:16:07,199 INFO com.netflix.discovery.DiscoveryClient [restartedMain] Registered Applications size is zero : true
2022-06-24 14:16:07,199 INFO com.netflix.discovery.DiscoveryClient [restartedMain] Application version is -1: true
2022-06-24 14:16:07,199 INFO com.netflix.discovery.DiscoveryClient [restartedMain] Getting all instance registry info from the eureka server
2022-06-24 14:16:07,211 INFO com.netflix.discovery.DiscoveryClient [restartedMain] The response status is 200
2022-06-24 14:16:07,212 INFO com.netflix.discovery.DiscoveryClient [restartedMain] Starting heartbeat executor: renew interval is: 30
2022-06-24 14:16:07,213 INFO com.netflix.discovery.InstanceInfoReplicator [restartedMain] InstanceInfoReplicator onDemand update allowed rate per min is 4
2022-06-24 14:16:07,213 INFO com.netflix.discovery.DiscoveryClient [restartedMain] Discovery Client initialized at timestamp 1656060367213 with initial instances count: 3
2022-06-24 14:16:07,215 INFO org.springframework.cloud.netflix.eureka.serviceregistry.EurekaServiceRegistry [restartedMain] Registering application ESTOCKMARKET-QUERY with eureka with status UP
2022-06-24 14:16:07,215 INFO com.netflix.discovery.DiscoveryClient$3 [restartedMain] Saw local status change event StatusChangeEvent [timestamp=1656060367215, current=UP, previous=STARTING]
2022-06-24 14:16:07,216 INFO com.netflix.discovery.DiscoveryClient [DiscoveryClient-InstanceInfoReplicator-0] DiscoveryClient_ESTOCKMARKET-QUERY/host.docker.internal:estockmarket-query:6093: registering service...
2022-06-24 14:16:07,217 INFO org.apache.kafka.common.config.AbstractConfig [restartedMain] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-group_id-65
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = group_id
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2022-06-24 14:16:07,220 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka version: 2.7.1
2022-06-24 14:16:07,220 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka commitId: 61dbce85d0d41457
2022-06-24 14:16:07,220 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka startTimeMs: 1656060367220
2022-06-24 14:16:07,220 INFO org.apache.kafka.clients.consumer.KafkaConsumer [restartedMain] [Consumer clientId=consumer-group_id-65, groupId=group_id] Subscribed to topic(s): createStock
2022-06-24 14:16:07,222 INFO org.apache.kafka.common.config.AbstractConfig [restartedMain] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-group_id-66
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = group_id
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2022-06-24 14:16:07,225 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-65, groupId=group_id] Cluster ID: 5kpO7pNKTFqroM6YwsJ86A
2022-06-24 14:16:07,225 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka version: 2.7.1
2022-06-24 14:16:07,225 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka commitId: 61dbce85d0d41457
2022-06-24 14:16:07,227 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-65, groupId=group_id] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2022-06-24 14:16:07,227 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka startTimeMs: 1656060367225
2022-06-24 14:16:07,227 INFO org.apache.kafka.clients.consumer.KafkaConsumer [restartedMain] [Consumer clientId=consumer-group_id-66, groupId=group_id] Subscribed to topic(s): createUser
2022-06-24 14:16:07,227 INFO com.netflix.discovery.DiscoveryClient [DiscoveryClient-InstanceInfoReplicator-0] DiscoveryClient_ESTOCKMARKET-QUERY/host.docker.internal:estockmarket-query:6093 - registration status: 204
2022-06-24 14:16:07,227 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-65, groupId=group_id] (Re-)joining group
2022-06-24 14:16:07,229 INFO org.apache.kafka.common.config.AbstractConfig [restartedMain] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-group_id-67
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = group_id
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2022-06-24 14:16:07,230 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-65, groupId=group_id] (Re-)joining group
2022-06-24 14:16:07,231 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-66, groupId=group_id] Cluster ID: 5kpO7pNKTFqroM6YwsJ86A
2022-06-24 14:16:07,231 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-66, groupId=group_id] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2022-06-24 14:16:07,232 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-66, groupId=group_id] (Re-)joining group
2022-06-24 14:16:07,232 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka version: 2.7.1
2022-06-24 14:16:07,232 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka commitId: 61dbce85d0d41457
2022-06-24 14:16:07,232 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka startTimeMs: 1656060367232
2022-06-24 14:16:07,232 INFO org.apache.kafka.clients.consumer.KafkaConsumer [restartedMain] [Consumer clientId=consumer-group_id-67, groupId=group_id] Subscribed to topic(s): removeSector
2022-06-24 14:16:07,235 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-65, groupId=group_id] Successfully joined group with generation Generation{generationId=99, memberId='consumer-group_id-65-23c6940a-4d9e-4937-bbe2-83468c87df1b', protocol='range'}
2022-06-24 14:16:07,235 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-66, groupId=group_id] (Re-)joining group
2022-06-24 14:16:07,235 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-65, groupId=group_id] Finished assignment for group at generation 99: {consumer-group_id-65-23c6940a-4d9e-4937-bbe2-83468c87df1b=Assignment(partitions=[createStock-0])}
2022-06-24 14:16:07,236 INFO org.apache.kafka.common.config.AbstractConfig [restartedMain] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-group_id-68
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = group_id
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2022-06-24 14:16:07,238 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-67, groupId=group_id] Cluster ID: 5kpO7pNKTFqroM6YwsJ86A
2022-06-24 14:16:07,240 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-65, groupId=group_id] SyncGroup failed: The group began another rebalance. Need to re-join the group. Sent generation was Generation{generationId=99, memberId='consumer-group_id-65-23c6940a-4d9e-4937-bbe2-83468c87df1b', protocol='range'}
2022-06-24 14:16:07,240 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-67, groupId=group_id] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2022-06-24 14:16:07,240 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-65, groupId=group_id] Rebalance failed.
org.apache.kafka.common.errors.RebalanceInProgressException: The group is rebalancing, so a rejoin is needed.
2022-06-24 14:16:07,240 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-65, groupId=group_id] (Re-)joining group
2022-06-24 14:16:07,240 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-67, groupId=group_id] (Re-)joining group
2022-06-24 14:16:07,241 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka version: 2.7.1
2022-06-24 14:16:07,241 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka commitId: 61dbce85d0d41457
2022-06-24 14:16:07,241 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka startTimeMs: 1656060367241
2022-06-24 14:16:07,241 INFO org.apache.kafka.clients.consumer.KafkaConsumer [restartedMain] [Consumer clientId=consumer-group_id-68, groupId=group_id] Subscribed to topic(s): createSector
2022-06-24 14:16:07,245 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-65, groupId=group_id] Successfully joined group with generation Generation{generationId=100, memberId='consumer-group_id-65-23c6940a-4d9e-4937-bbe2-83468c87df1b', protocol='range'}
2022-06-24 14:16:07,245 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-66, groupId=group_id] Successfully joined group with generation Generation{generationId=100, memberId='consumer-group_id-66-c7c5983e-4c5f-472e-b8c4-a7139546d7d1', protocol='range'}
2022-06-24 14:16:07,247 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-67, groupId=group_id] (Re-)joining group
2022-06-24 14:16:07,247 INFO org.apache.kafka.common.config.AbstractConfig [restartedMain] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-group_id-69
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = group_id
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2022-06-24 14:16:07,249 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-66, groupId=group_id] SyncGroup failed: The group began another rebalance. Need to re-join the group. Sent generation was Generation{generationId=100, memberId='consumer-group_id-66-c7c5983e-4c5f-472e-b8c4-a7139546d7d1', protocol='range'}
2022-06-24 14:16:07,250 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-65, groupId=group_id] Finished assignment for group at generation 100: {consumer-group_id-66-c7c5983e-4c5f-472e-b8c4-a7139546d7d1=Assignment(partitions=[createUser-0]), consumer-group_id-65-23c6940a-4d9e-4937-bbe2-83468c87df1b=Assignment(partitions=[createStock-0])}
2022-06-24 14:16:07,250 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-66, groupId=group_id] Rebalance failed.
org.apache.kafka.common.errors.RebalanceInProgressException: The group is rebalancing, so a rejoin is needed.
2022-06-24 14:16:07,250 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-66, groupId=group_id] (Re-)joining group
2022-06-24 14:16:07,251 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-65, groupId=group_id] SyncGroup failed: The group began another rebalance. Need to re-join the group. Sent generation was Generation{generationId=100, memberId='consumer-group_id-65-23c6940a-4d9e-4937-bbe2-83468c87df1b', protocol='range'}
2022-06-24 14:16:07,251 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-65, groupId=group_id] Rebalance failed.
org.apache.kafka.common.errors.RebalanceInProgressException: The group is rebalancing, so a rejoin is needed.
2022-06-24 14:16:07,251 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-65, groupId=group_id] (Re-)joining group
2022-06-24 14:16:07,251 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-68, groupId=group_id] Cluster ID: 5kpO7pNKTFqroM6YwsJ86A
2022-06-24 14:16:07,251 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka version: 2.7.1
2022-06-24 14:16:07,251 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka commitId: 61dbce85d0d41457
2022-06-24 14:16:07,251 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka startTimeMs: 1656060367251
2022-06-24 14:16:07,251 INFO org.apache.kafka.clients.consumer.KafkaConsumer [restartedMain] [Consumer clientId=consumer-group_id-69, groupId=group_id] Subscribed to topic(s): removeStock
2022-06-24 14:16:07,251 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-68, groupId=group_id] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2022-06-24 14:16:07,252 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-68, groupId=group_id] (Re-)joining group
2022-06-24 14:16:07,253 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-66, groupId=group_id] Successfully joined group with generation Generation{generationId=101, memberId='consumer-group_id-66-c7c5983e-4c5f-472e-b8c4-a7139546d7d1', protocol='range'}
2022-06-24 14:16:07,254 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-67, groupId=group_id] Successfully joined group with generation Generation{generationId=101, memberId='consumer-group_id-67-bc9b1ac8-4e3b-4c45-922a-faa15f9f5d76', protocol='range'}
2022-06-24 14:16:07,254 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-65, groupId=group_id] Successfully joined group with generation Generation{generationId=101, memberId='consumer-group_id-65-23c6940a-4d9e-4937-bbe2-83468c87df1b', protocol='range'}
2022-06-24 14:16:07,255 INFO org.apache.kafka.common.config.AbstractConfig [restartedMain] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-group_id-70
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = group_id
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2022-06-24 14:16:07,255 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-65, groupId=group_id] Finished assignment for group at generation 101: {consumer-group_id-66-c7c5983e-4c5f-472e-b8c4-a7139546d7d1=Assignment(partitions=[createUser-0]), consumer-group_id-65-23c6940a-4d9e-4937-bbe2-83468c87df1b=Assignment(partitions=[createStock-0]), consumer-group_id-67-bc9b1ac8-4e3b-4c45-922a-faa15f9f5d76=Assignment(partitions=[removeSector-0])}
2022-06-24 14:16:07,257 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-68, groupId=group_id] (Re-)joining group
2022-06-24 14:16:07,257 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-69, groupId=group_id] Cluster ID: 5kpO7pNKTFqroM6YwsJ86A
2022-06-24 14:16:07,258 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-69, groupId=group_id] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2022-06-24 14:16:07,258 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka version: 2.7.1
2022-06-24 14:16:07,258 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka commitId: 61dbce85d0d41457
2022-06-24 14:16:07,258 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-69, groupId=group_id] (Re-)joining group
2022-06-24 14:16:07,258 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka startTimeMs: 1656060367258
2022-06-24 14:16:07,259 INFO org.apache.kafka.clients.consumer.KafkaConsumer [restartedMain] [Consumer clientId=consumer-group_id-70, groupId=group_id] Subscribed to topic(s): updateStock
2022-06-24 14:16:07,260 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-65, groupId=group_id] Successfully synced group in generation Generation{generationId=101, memberId='consumer-group_id-65-23c6940a-4d9e-4937-bbe2-83468c87df1b', protocol='range'}
2022-06-24 14:16:07,262 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-66, groupId=group_id] Successfully synced group in generation Generation{generationId=101, memberId='consumer-group_id-66-c7c5983e-4c5f-472e-b8c4-a7139546d7d1', protocol='range'}
2022-06-24 14:16:07,262 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-65, groupId=group_id] Notifying assignor about the new Assignment(partitions=[createStock-0])
2022-06-24 14:16:07,262 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-65, groupId=group_id] Adding newly assigned partitions: createStock-0
2022-06-24 14:16:07,262 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-66, groupId=group_id] Notifying assignor about the new Assignment(partitions=[createUser-0])
2022-06-24 14:16:07,262 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-67, groupId=group_id] Successfully synced group in generation Generation{generationId=101, memberId='consumer-group_id-67-bc9b1ac8-4e3b-4c45-922a-faa15f9f5d76', protocol='range'}
2022-06-24 14:16:07,262 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-66, groupId=group_id] Adding newly assigned partitions: createUser-0
2022-06-24 14:16:07,263 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-67, groupId=group_id] Notifying assignor about the new Assignment(partitions=[removeSector-0])
2022-06-24 14:16:07,263 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-69, groupId=group_id] (Re-)joining group
2022-06-24 14:16:07,263 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-67, groupId=group_id] Adding newly assigned partitions: removeSector-0
2022-06-24 14:16:07,263 INFO org.apache.kafka.common.config.AbstractConfig [restartedMain] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-group_id-71
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = group_id
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2022-06-24 14:16:07,264 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-65, groupId=group_id] Setting offset for partition createStock-0 to the committed offset FetchPosition{offset=3, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:16:07,264 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-66, groupId=group_id] Setting offset for partition createUser-0 to the committed offset FetchPosition{offset=1, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:16:07,264 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] group_id: partitions assigned: [createStock-0]
2022-06-24 14:16:07,264 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] group_id: partitions assigned: [createUser-0]
2022-06-24 14:16:07,265 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-67, groupId=group_id] Setting offset for partition removeSector-0 to the committed offset FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:16:07,265 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-70, groupId=group_id] Cluster ID: 5kpO7pNKTFqroM6YwsJ86A
2022-06-24 14:16:07,265 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] group_id: partitions assigned: [removeSector-0]
2022-06-24 14:16:07,266 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka version: 2.7.1
2022-06-24 14:16:07,266 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-70, groupId=group_id] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2022-06-24 14:16:07,266 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka commitId: 61dbce85d0d41457
2022-06-24 14:16:07,266 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka startTimeMs: 1656060367266
2022-06-24 14:16:07,266 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-70, groupId=group_id] (Re-)joining group
2022-06-24 14:16:07,266 INFO org.apache.kafka.clients.consumer.KafkaConsumer [restartedMain] [Consumer clientId=consumer-group_id-71, groupId=group_id] Subscribed to topic(s): removeCompany
2022-06-24 14:16:07,269 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-70, groupId=group_id] (Re-)joining group
2022-06-24 14:16:07,269 INFO org.apache.kafka.common.config.AbstractConfig [restartedMain] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-group_id-72
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = group_id
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2022-06-24 14:16:07,270 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-71, groupId=group_id] Cluster ID: 5kpO7pNKTFqroM6YwsJ86A
2022-06-24 14:16:07,271 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-71, groupId=group_id] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2022-06-24 14:16:07,272 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-71, groupId=group_id] (Re-)joining group
2022-06-24 14:16:07,273 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka version: 2.7.1
2022-06-24 14:16:07,273 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka commitId: 61dbce85d0d41457
2022-06-24 14:16:07,273 INFO org.apache.kafka.common.utils.AppInfoParser$AppInfo [restartedMain] Kafka startTimeMs: 1656060367273
2022-06-24 14:16:07,274 INFO org.apache.kafka.clients.consumer.KafkaConsumer [restartedMain] [Consumer clientId=consumer-group_id-72, groupId=group_id] Subscribed to topic(s): createCompany
2022-06-24 14:16:07,274 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-71, groupId=group_id] (Re-)joining group
2022-06-24 14:16:07,275 INFO org.apache.juli.logging.DirectJDKLog [restartedMain] Starting ProtocolHandler ["http-nio-6093"]
2022-06-24 14:16:07,278 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-72, groupId=group_id] Cluster ID: 5kpO7pNKTFqroM6YwsJ86A
2022-06-24 14:16:07,279 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-72, groupId=group_id] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2022-06-24 14:16:07,279 INFO org.springframework.boot.web.embedded.tomcat.TomcatWebServer [restartedMain] Tomcat started on port(s): 6093 (http) with context path ''
2022-06-24 14:16:07,279 INFO org.springframework.cloud.netflix.eureka.serviceregistry.EurekaAutoServiceRegistration [restartedMain] Updating port to 6093
2022-06-24 14:16:07,279 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-72, groupId=group_id] (Re-)joining group
2022-06-24 14:16:07,281 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-72, groupId=group_id] (Re-)joining group
2022-06-24 14:16:07,312 INFO springfox.documentation.spring.web.plugins.DocumentationPluginsBootstrapper [restartedMain] Context refreshed
2022-06-24 14:16:07,313 INFO springfox.documentation.spring.web.plugins.DocumentationPluginsBootstrapper [restartedMain] Found 1 custom documentation plugin(s)
2022-06-24 14:16:07,315 INFO springfox.documentation.spring.web.scanners.ApiListingReferenceScanner [restartedMain] Scanning for api listing references
2022-06-24 14:16:07,342 INFO org.springframework.boot.StartupInfoLogger [restartedMain] Started EstockmarketQueryApplication in 1.37 seconds (JVM running for 54081.65)
2022-06-24 14:16:07,345 INFO org.springframework.boot.devtools.autoconfigure.ConditionEvaluationDeltaLoggingListener [restartedMain] Condition evaluation unchanged
2022-06-24 14:16:10,269 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-66, groupId=group_id] Attempt to heartbeat failed since group is rebalancing
2022-06-24 14:16:10,269 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-67, groupId=group_id] Attempt to heartbeat failed since group is rebalancing
2022-06-24 14:16:10,269 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-65, groupId=group_id] Attempt to heartbeat failed since group is rebalancing
2022-06-24 14:16:10,270 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-66, groupId=group_id] Revoke previously assigned partitions createUser-0
2022-06-24 14:16:10,270 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-67, groupId=group_id] Revoke previously assigned partitions removeSector-0
2022-06-24 14:16:10,270 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-65, groupId=group_id] Revoke previously assigned partitions createStock-0
2022-06-24 14:16:10,270 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] group_id: partitions revoked: [removeSector-0]
2022-06-24 14:16:10,270 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] group_id: partitions revoked: [createStock-0]
2022-06-24 14:16:10,270 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] group_id: partitions revoked: [createUser-0]
2022-06-24 14:16:10,270 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-67, groupId=group_id] (Re-)joining group
2022-06-24 14:16:10,270 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-65, groupId=group_id] (Re-)joining group
2022-06-24 14:16:10,270 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-66, groupId=group_id] (Re-)joining group
2022-06-24 14:16:10,280 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-70, groupId=group_id] Successfully joined group with generation Generation{generationId=102, memberId='consumer-group_id-70-cfc0ec36-b165-4685-8198-58b7a95c76fc', protocol='range'}
2022-06-24 14:16:10,280 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-67, groupId=group_id] Successfully joined group with generation Generation{generationId=102, memberId='consumer-group_id-67-bc9b1ac8-4e3b-4c45-922a-faa15f9f5d76', protocol='range'}
2022-06-24 14:16:10,280 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-65, groupId=group_id] Successfully joined group with generation Generation{generationId=102, memberId='consumer-group_id-65-23c6940a-4d9e-4937-bbe2-83468c87df1b', protocol='range'}
2022-06-24 14:16:10,280 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-69, groupId=group_id] Successfully joined group with generation Generation{generationId=102, memberId='consumer-group_id-69-f62d9195-cdb0-4619-9115-7394a5d45547', protocol='range'}
2022-06-24 14:16:10,280 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-68, groupId=group_id] Successfully joined group with generation Generation{generationId=102, memberId='consumer-group_id-68-93321fb9-e363-4658-8c33-23a6354c46fe', protocol='range'}
2022-06-24 14:16:10,280 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-66, groupId=group_id] Successfully joined group with generation Generation{generationId=102, memberId='consumer-group_id-66-c7c5983e-4c5f-472e-b8c4-a7139546d7d1', protocol='range'}
2022-06-24 14:16:10,280 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-72, groupId=group_id] Successfully joined group with generation Generation{generationId=102, memberId='consumer-group_id-72-fda1755f-06ee-41ae-b037-238da7c34291', protocol='range'}
2022-06-24 14:16:10,280 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-71, groupId=group_id] Successfully joined group with generation Generation{generationId=102, memberId='consumer-group_id-71-0c9e0a1d-a357-4a20-9a97-ccb40703f8ee', protocol='range'}
2022-06-24 14:16:10,330 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-65, groupId=group_id] Finished assignment for group at generation 102: {consumer-group_id-71-0c9e0a1d-a357-4a20-9a97-ccb40703f8ee=Assignment(partitions=[removeCompany-0]), consumer-group_id-70-cfc0ec36-b165-4685-8198-58b7a95c76fc=Assignment(partitions=[updateStock-0]), consumer-group_id-66-c7c5983e-4c5f-472e-b8c4-a7139546d7d1=Assignment(partitions=[createUser-0]), consumer-group_id-68-93321fb9-e363-4658-8c33-23a6354c46fe=Assignment(partitions=[createSector-0]), consumer-group_id-72-fda1755f-06ee-41ae-b037-238da7c34291=Assignment(partitions=[createCompany-0]), consumer-group_id-69-f62d9195-cdb0-4619-9115-7394a5d45547=Assignment(partitions=[removeStock-0]), consumer-group_id-65-23c6940a-4d9e-4937-bbe2-83468c87df1b=Assignment(partitions=[createStock-0]), consumer-group_id-67-bc9b1ac8-4e3b-4c45-922a-faa15f9f5d76=Assignment(partitions=[removeSector-0])}
2022-06-24 14:16:10,334 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-65, groupId=group_id] Successfully synced group in generation Generation{generationId=102, memberId='consumer-group_id-65-23c6940a-4d9e-4937-bbe2-83468c87df1b', protocol='range'}
2022-06-24 14:16:10,334 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-67, groupId=group_id] Successfully synced group in generation Generation{generationId=102, memberId='consumer-group_id-67-bc9b1ac8-4e3b-4c45-922a-faa15f9f5d76', protocol='range'}
2022-06-24 14:16:10,335 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-66, groupId=group_id] Successfully synced group in generation Generation{generationId=102, memberId='consumer-group_id-66-c7c5983e-4c5f-472e-b8c4-a7139546d7d1', protocol='range'}
2022-06-24 14:16:10,336 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-65, groupId=group_id] Notifying assignor about the new Assignment(partitions=[createStock-0])
2022-06-24 14:16:10,336 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-66, groupId=group_id] Notifying assignor about the new Assignment(partitions=[createUser-0])
2022-06-24 14:16:10,336 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-67, groupId=group_id] Notifying assignor about the new Assignment(partitions=[removeSector-0])
2022-06-24 14:16:10,336 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-65, groupId=group_id] Adding newly assigned partitions: createStock-0
2022-06-24 14:16:10,336 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-66, groupId=group_id] Adding newly assigned partitions: createUser-0
2022-06-24 14:16:10,338 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-67, groupId=group_id] Adding newly assigned partitions: removeSector-0
2022-06-24 14:16:10,339 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-70, groupId=group_id] Successfully synced group in generation Generation{generationId=102, memberId='consumer-group_id-70-cfc0ec36-b165-4685-8198-58b7a95c76fc', protocol='range'}
2022-06-24 14:16:10,339 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-69, groupId=group_id] Successfully synced group in generation Generation{generationId=102, memberId='consumer-group_id-69-f62d9195-cdb0-4619-9115-7394a5d45547', protocol='range'}
2022-06-24 14:16:10,339 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-70, groupId=group_id] Notifying assignor about the new Assignment(partitions=[updateStock-0])
2022-06-24 14:16:10,339 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-69, groupId=group_id] Notifying assignor about the new Assignment(partitions=[removeStock-0])
2022-06-24 14:16:10,340 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-70, groupId=group_id] Adding newly assigned partitions: updateStock-0
2022-06-24 14:16:10,340 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-69, groupId=group_id] Adding newly assigned partitions: removeStock-0
2022-06-24 14:16:10,340 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-71, groupId=group_id] Successfully synced group in generation Generation{generationId=102, memberId='consumer-group_id-71-0c9e0a1d-a357-4a20-9a97-ccb40703f8ee', protocol='range'}
2022-06-24 14:16:10,340 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-65, groupId=group_id] Setting offset for partition createStock-0 to the committed offset FetchPosition{offset=3, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:16:10,340 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-67, groupId=group_id] Setting offset for partition removeSector-0 to the committed offset FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:16:10,341 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-68, groupId=group_id] Successfully synced group in generation Generation{generationId=102, memberId='consumer-group_id-68-93321fb9-e363-4658-8c33-23a6354c46fe', protocol='range'}
2022-06-24 14:16:10,341 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-66, groupId=group_id] Setting offset for partition createUser-0 to the committed offset FetchPosition{offset=1, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:16:10,341 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-71, groupId=group_id] Notifying assignor about the new Assignment(partitions=[removeCompany-0])
2022-06-24 14:16:10,341 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] group_id: partitions assigned: [removeSector-0]
2022-06-24 14:16:10,341 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] group_id: partitions assigned: [createUser-0]
2022-06-24 14:16:10,341 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-68, groupId=group_id] Notifying assignor about the new Assignment(partitions=[createSector-0])
2022-06-24 14:16:10,341 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-71, groupId=group_id] Adding newly assigned partitions: removeCompany-0
2022-06-24 14:16:10,340 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] group_id: partitions assigned: [createStock-0]
2022-06-24 14:16:10,341 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-72, groupId=group_id] Successfully synced group in generation Generation{generationId=102, memberId='consumer-group_id-72-fda1755f-06ee-41ae-b037-238da7c34291', protocol='range'}
2022-06-24 14:16:10,341 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-68, groupId=group_id] Adding newly assigned partitions: createSector-0
2022-06-24 14:16:10,341 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-72, groupId=group_id] Notifying assignor about the new Assignment(partitions=[createCompany-0])
2022-06-24 14:16:10,341 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-72, groupId=group_id] Adding newly assigned partitions: createCompany-0
2022-06-24 14:16:10,342 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-69, groupId=group_id] Setting offset for partition removeStock-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:16:10,342 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-70, groupId=group_id] Setting offset for partition updateStock-0 to the committed offset FetchPosition{offset=7, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:16:10,342 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] group_id: partitions assigned: [removeStock-0]
2022-06-24 14:16:10,342 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] group_id: partitions assigned: [updateStock-0]
2022-06-24 14:16:10,343 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-71, groupId=group_id] Setting offset for partition removeCompany-0 to the committed offset FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:16:10,343 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-72, groupId=group_id] Setting offset for partition createCompany-0 to the committed offset FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:16:10,343 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] group_id: partitions assigned: [removeCompany-0]
2022-06-24 14:16:10,343 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-68, groupId=group_id] Setting offset for partition createSector-0 to the committed offset FetchPosition{offset=1, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2022-06-24 14:16:10,343 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] group_id: partitions assigned: [createCompany-0]
2022-06-24 14:16:10,343 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] group_id: partitions assigned: [createSector-0]
2022-06-24 14:16:25,758 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks C-008 48.0
2022-06-24 14:16:25,826 INFO com.mongodb.diagnostics.logging.SLF4JLogger [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Opened connection [connectionId{localValue:32, serverValue:83}] to localhost:27017
2022-06-24 14:16:48,071 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks C-008 12.0
2022-06-24 14:18:13,997 INFO org.apache.juli.logging.DirectJDKLog [http-nio-6093-exec-1] Initializing Spring DispatcherServlet 'dispatcherServlet'
2022-06-24 14:18:13,997 INFO org.springframework.web.servlet.FrameworkServlet [http-nio-6093-exec-1] Initializing Servlet 'dispatcherServlet'
2022-06-24 14:18:14,000 INFO org.springframework.web.servlet.FrameworkServlet [http-nio-6093-exec-1] Completed initialization in 3 ms
2022-06-24 14:18:14,002 INFO com.estockmarket.query.application.controller.CompanyController [http-nio-6093-exec-1] get all company details 
2022-06-24 14:18:14,002 INFO com.estockmarket.query.domain.service.CompanyService [http-nio-6093-exec-1] fetch all company {}
2022-06-24 14:18:14,011 INFO com.estockmarket.query.domain.service.CompanyService [http-nio-6093-exec-1] convert to company dto com.estockmarket.query.domain.model.Company@4180a043
2022-06-24 14:18:42,171 INFO com.estockmarket.query.application.controller.CompanyController [http-nio-6093-exec-3] get all company details 
2022-06-24 14:18:42,171 INFO com.estockmarket.query.domain.service.CompanyService [http-nio-6093-exec-3] fetch all company {}
2022-06-24 14:18:42,174 INFO com.estockmarket.query.domain.service.CompanyService [http-nio-6093-exec-3] convert to company dto com.estockmarket.query.domain.model.Company@5ff19a35
2022-06-24 14:18:47,281 INFO com.estockmarket.query.application.controller.CompanyController [http-nio-6093-exec-4] get all company details 
2022-06-24 14:18:47,282 INFO com.estockmarket.query.domain.service.CompanyService [http-nio-6093-exec-4] fetch all company {}
2022-06-24 14:18:47,284 INFO com.estockmarket.query.domain.service.CompanyService [http-nio-6093-exec-4] convert to company dto com.estockmarket.query.domain.model.Company@67213823
2022-06-24 14:21:07,218 INFO com.netflix.discovery.shared.resolver.aws.ConfigClusterResolver [AsyncResolver-bootstrap-executor-0] Resolving eureka endpoints via configuration
2022-06-24 14:25:56,154 INFO com.estockmarket.query.application.controller.CompanyController [http-nio-6093-exec-2] get all company details 
2022-06-24 14:25:56,159 INFO com.estockmarket.query.domain.service.CompanyService [http-nio-6093-exec-2] fetch all company {}
2022-06-24 14:25:56,177 INFO com.estockmarket.query.domain.service.CompanyService [http-nio-6093-exec-2] convert to company dto com.estockmarket.query.domain.model.Company@6498b7a1
2022-06-24 14:26:07,227 INFO com.netflix.discovery.shared.resolver.aws.ConfigClusterResolver [AsyncResolver-bootstrap-executor-0] Resolving eureka endpoints via configuration
2022-06-24 14:26:39,686 INFO com.estockmarket.query.application.controller.CompanyController [http-nio-6093-exec-6] get all company details 
2022-06-24 14:26:39,687 INFO com.estockmarket.query.domain.service.CompanyService [http-nio-6093-exec-6] fetch all company {}
2022-06-24 14:26:39,689 INFO com.estockmarket.query.domain.service.CompanyService [http-nio-6093-exec-6] convert to company dto com.estockmarket.query.domain.model.Company@2c1546f1
2022-06-24 14:27:20,657 INFO com.estockmarket.query.application.controller.CompanyController [http-nio-6093-exec-7] get all company details 
2022-06-24 14:27:20,658 INFO com.estockmarket.query.domain.service.CompanyService [http-nio-6093-exec-7] fetch all company {}
2022-06-24 14:27:20,660 INFO com.estockmarket.query.domain.service.CompanyService [http-nio-6093-exec-7] convert to company dto com.estockmarket.query.domain.model.Company@37777ab1
2022-06-24 14:27:49,923 INFO com.estockmarket.query.application.controller.StockController [http-nio-6093-exec-8] fetch min,max and average stock price based on company code,date C-008
2022-06-24 14:27:49,923 INFO com.estockmarket.query.application.controller.StockController [http-nio-6093-exec-9] fetch stocks based on code,date C-008
2022-06-24 14:27:49,924 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-8] fetch stockaggregate based on code,date C-008
2022-06-24 14:27:49,924 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-9] fetch company stocks based on code,date C-008
2022-06-24 14:27:49,926 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-9] fetch stock query based on code C-008
2022-06-24 14:27:49,926 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-8] fetch stock query based on code C-008
2022-06-24 14:27:50,014 INFO com.mongodb.diagnostics.logging.SLF4JLogger [http-nio-6093-exec-8] Opened connection [connectionId{localValue:33, serverValue:84}] to localhost:27017
2022-06-24 14:31:07,235 INFO com.netflix.discovery.shared.resolver.aws.ConfigClusterResolver [AsyncResolver-bootstrap-executor-0] Resolving eureka endpoints via configuration
2022-06-24 14:31:35,835 INFO com.estockmarket.query.application.controller.CompanyController [http-nio-6093-exec-3] get all company details 
2022-06-24 14:31:35,835 INFO com.estockmarket.query.domain.service.CompanyService [http-nio-6093-exec-3] fetch all company {}
2022-06-24 14:31:35,838 INFO com.estockmarket.query.domain.service.CompanyService [http-nio-6093-exec-3] convert to company dto com.estockmarket.query.domain.model.Company@2b231151
2022-06-24 14:32:37,108 INFO com.estockmarket.query.application.controller.CompanyController [http-nio-6093-exec-5] get all company details 
2022-06-24 14:32:37,109 INFO com.estockmarket.query.domain.service.CompanyService [http-nio-6093-exec-5] fetch all company {}
2022-06-24 14:32:37,110 INFO com.estockmarket.query.domain.service.CompanyService [http-nio-6093-exec-5] convert to company dto com.estockmarket.query.domain.model.Company@4a567612
2022-06-24 14:36:07,251 INFO com.netflix.discovery.shared.resolver.aws.ConfigClusterResolver [AsyncResolver-bootstrap-executor-0] Resolving eureka endpoints via configuration
2022-06-24 14:40:36,312 INFO com.estockmarket.query.application.controller.CompanyController [http-nio-6093-exec-6] get all company details 
2022-06-24 14:40:36,313 INFO com.estockmarket.query.domain.service.CompanyService [http-nio-6093-exec-6] fetch all company {}
2022-06-24 14:40:36,316 INFO com.estockmarket.query.domain.service.CompanyService [http-nio-6093-exec-6] convert to company dto com.estockmarket.query.domain.model.Company@58b454d1
2022-06-24 14:40:41,436 INFO com.estockmarket.query.application.controller.StockController [http-nio-6093-exec-7] fetch min,max and average stock price based on company code,date C-008
2022-06-24 14:40:41,436 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-7] fetch stockaggregate based on code,date C-008
2022-06-24 14:40:41,437 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-7] fetch stock query based on code C-008
2022-06-24 14:40:41,439 INFO com.estockmarket.query.application.controller.StockController [http-nio-6093-exec-9] fetch stocks based on code,date C-008
2022-06-24 14:40:41,439 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-9] fetch company stocks based on code,date C-008
2022-06-24 14:40:41,439 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-9] fetch stock query based on code C-008
2022-06-24 14:41:07,259 INFO com.netflix.discovery.shared.resolver.aws.ConfigClusterResolver [AsyncResolver-bootstrap-executor-0] Resolving eureka endpoints via configuration
2022-06-24 14:42:01,667 INFO com.estockmarket.query.domain.service.StockService [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] update stocks C-008 11.0
2022-06-24 14:42:01,694 INFO com.estockmarket.query.application.controller.StockController [http-nio-6093-exec-1] fetch stocks based on code,date C-008
2022-06-24 14:42:01,694 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-1] fetch company stocks based on code,date C-008
2022-06-24 14:42:01,694 INFO com.estockmarket.query.application.controller.StockController [http-nio-6093-exec-3] fetch min,max and average stock price based on company code,date C-008
2022-06-24 14:42:01,694 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-3] fetch stockaggregate based on code,date C-008
2022-06-24 14:42:01,694 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-1] fetch stock query based on code C-008
2022-06-24 14:42:01,694 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-3] fetch stock query based on code C-008
2022-06-24 14:42:22,089 INFO com.estockmarket.query.application.controller.StockController [http-nio-6093-exec-4] fetch stocks based on code,date C-008
2022-06-24 14:42:22,089 INFO com.estockmarket.query.application.controller.StockController [http-nio-6093-exec-5] fetch min,max and average stock price based on company code,date C-008
2022-06-24 14:42:22,090 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-4] fetch company stocks based on code,date C-008
2022-06-24 14:42:22,090 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-5] fetch stockaggregate based on code,date C-008
2022-06-24 14:42:22,090 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-4] fetch stock query based on code C-008
2022-06-24 14:42:22,090 INFO com.estockmarket.query.domain.service.StockService [http-nio-6093-exec-5] fetch stock query based on code C-008
2022-06-24 14:46:07,275 INFO com.netflix.discovery.shared.resolver.aws.ConfigClusterResolver [AsyncResolver-bootstrap-executor-0] Resolving eureka endpoints via configuration
2022-06-24 14:51:07,287 INFO com.netflix.discovery.shared.resolver.aws.ConfigClusterResolver [AsyncResolver-bootstrap-executor-0] Resolving eureka endpoints via configuration
2022-06-24 14:56:07,297 INFO com.netflix.discovery.shared.resolver.aws.ConfigClusterResolver [AsyncResolver-bootstrap-executor-0] Resolving eureka endpoints via configuration
2022-06-24 15:01:07,307 INFO com.netflix.discovery.shared.resolver.aws.ConfigClusterResolver [AsyncResolver-bootstrap-executor-0] Resolving eureka endpoints via configuration
2022-06-24 15:06:07,316 INFO com.netflix.discovery.shared.resolver.aws.ConfigClusterResolver [AsyncResolver-bootstrap-executor-0] Resolving eureka endpoints via configuration
2022-06-24 15:11:07,325 INFO com.netflix.discovery.shared.resolver.aws.ConfigClusterResolver [AsyncResolver-bootstrap-executor-0] Resolving eureka endpoints via configuration
2022-06-24 15:11:29,990 INFO org.springframework.boot.admin.SpringApplicationAdminMXBeanRegistrar$SpringApplicationAdmin [RMI TCP Connection(154)-127.0.0.1] Application shutdown requested.
2022-06-24 15:11:29,991 INFO org.springframework.cloud.netflix.eureka.serviceregistry.EurekaServiceRegistry [RMI TCP Connection(154)-127.0.0.1] Unregistering application ESTOCKMARKET-QUERY with eureka with status DOWN
2022-06-24 15:11:29,992 INFO com.netflix.discovery.DiscoveryClient$3 [RMI TCP Connection(154)-127.0.0.1] Saw local status change event StatusChangeEvent [timestamp=1656063689992, current=DOWN, previous=UP]
2022-06-24 15:11:29,993 INFO com.netflix.discovery.DiscoveryClient [DiscoveryClient-InstanceInfoReplicator-0] DiscoveryClient_ESTOCKMARKET-QUERY/host.docker.internal:estockmarket-query:6093: registering service...
2022-06-24 15:11:30,012 INFO com.netflix.discovery.DiscoveryClient [DiscoveryClient-InstanceInfoReplicator-0] DiscoveryClient_ESTOCKMARKET-QUERY/host.docker.internal:estockmarket-query:6093 - registration status: 204
2022-06-24 15:11:30,034 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-65, groupId=group_id] Revoke previously assigned partitions createStock-0
2022-06-24 15:11:30,034 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-66, groupId=group_id] Revoke previously assigned partitions createUser-0
2022-06-24 15:11:30,034 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-67, groupId=group_id] Revoke previously assigned partitions removeSector-0
2022-06-24 15:11:30,034 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] group_id: partitions revoked: [createUser-0]
2022-06-24 15:11:30,034 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-69, groupId=group_id] Revoke previously assigned partitions removeStock-0
2022-06-24 15:11:30,034 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] group_id: partitions revoked: [createStock-0]
2022-06-24 15:11:30,035 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-68, groupId=group_id] Revoke previously assigned partitions createSector-0
2022-06-24 15:11:30,035 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-65, groupId=group_id] Member consumer-group_id-65-23c6940a-4d9e-4937-bbe2-83468c87df1b sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2022-06-24 15:11:30,035 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] group_id: partitions revoked: [removeStock-0]
2022-06-24 15:11:30,035 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-70, groupId=group_id] Revoke previously assigned partitions updateStock-0
2022-06-24 15:11:30,035 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] group_id: partitions revoked: [createSector-0]
2022-06-24 15:11:30,035 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-69, groupId=group_id] Member consumer-group_id-69-f62d9195-cdb0-4619-9115-7394a5d45547 sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2022-06-24 15:11:30,035 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] group_id: partitions revoked: [updateStock-0]
2022-06-24 15:11:30,035 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] [Consumer clientId=consumer-group_id-65, groupId=group_id] Unsubscribed all topics or patterns and assigned partitions
2022-06-24 15:11:30,035 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-68, groupId=group_id] Member consumer-group_id-68-93321fb9-e363-4658-8c33-23a6354c46fe sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2022-06-24 15:11:30,035 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-70, groupId=group_id] Member consumer-group_id-70-cfc0ec36-b165-4685-8198-58b7a95c76fc sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2022-06-24 15:11:30,035 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] [Consumer clientId=consumer-group_id-69, groupId=group_id] Unsubscribed all topics or patterns and assigned partitions
2022-06-24 15:11:30,035 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-66, groupId=group_id] Member consumer-group_id-66-c7c5983e-4c5f-472e-b8c4-a7139546d7d1 sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2022-06-24 15:11:30,035 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [Consumer clientId=consumer-group_id-68, groupId=group_id] Unsubscribed all topics or patterns and assigned partitions
2022-06-24 15:11:30,035 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-72, groupId=group_id] Revoke previously assigned partitions createCompany-0
2022-06-24 15:11:30,035 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-71, groupId=group_id] Revoke previously assigned partitions removeCompany-0
2022-06-24 15:11:30,035 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] [Consumer clientId=consumer-group_id-66, groupId=group_id] Unsubscribed all topics or patterns and assigned partitions
2022-06-24 15:11:30,035 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] group_id: partitions revoked: [removeCompany-0]
2022-06-24 15:11:30,035 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] group_id: partitions revoked: [removeSector-0]
2022-06-24 15:11:30,035 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] group_id: partitions revoked: [createCompany-0]
2022-06-24 15:11:30,036 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-71, groupId=group_id] Member consumer-group_id-71-0c9e0a1d-a357-4a20-9a97-ccb40703f8ee sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2022-06-24 15:11:30,036 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-67, groupId=group_id] Member consumer-group_id-67-bc9b1ac8-4e3b-4c45-922a-faa15f9f5d76 sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2022-06-24 15:11:30,036 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-72, groupId=group_id] Member consumer-group_id-72-fda1755f-06ee-41ae-b037-238da7c34291 sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2022-06-24 15:11:30,036 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-group_id-71, groupId=group_id] Unsubscribed all topics or patterns and assigned partitions
2022-06-24 15:11:30,036 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [Consumer clientId=consumer-group_id-67, groupId=group_id] Unsubscribed all topics or patterns and assigned partitions
2022-06-24 15:11:30,036 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [Consumer clientId=consumer-group_id-72, groupId=group_id] Unsubscribed all topics or patterns and assigned partitions
2022-06-24 15:11:30,036 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] [Consumer clientId=consumer-group_id-70, groupId=group_id] Unsubscribed all topics or patterns and assigned partitions
2022-06-24 15:11:30,041 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] Metrics scheduler closed
2022-06-24 15:11:30,041 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-06-24 15:11:30,041 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] Metrics reporters closed
2022-06-24 15:11:30,041 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] Metrics scheduler closed
2022-06-24 15:11:30,042 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-06-24 15:11:30,042 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] Metrics reporters closed
2022-06-24 15:11:30,043 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] Metrics scheduler closed
2022-06-24 15:11:30,043 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-06-24 15:11:30,044 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] Metrics reporters closed
2022-06-24 15:11:30,044 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] App info kafka.consumer for consumer-group_id-65 unregistered
2022-06-24 15:11:30,044 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#6-0-C-1] group_id: Consumer stopped
2022-06-24 15:11:30,045 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] Metrics scheduler closed
2022-06-24 15:11:30,045 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-06-24 15:11:30,045 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] Metrics reporters closed
2022-06-24 15:11:30,045 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] App info kafka.consumer for consumer-group_id-69 unregistered
2022-06-24 15:11:30,045 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#4-0-C-1] group_id: Consumer stopped
2022-06-24 15:11:30,046 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Metrics scheduler closed
2022-06-24 15:11:30,046 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-06-24 15:11:30,046 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Metrics reporters closed
2022-06-24 15:11:30,048 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] Metrics scheduler closed
2022-06-24 15:11:30,048 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-06-24 15:11:30,048 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] Metrics reporters closed
2022-06-24 15:11:30,048 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Metrics scheduler closed
2022-06-24 15:11:30,049 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-06-24 15:11:30,049 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] Metrics reporters closed
2022-06-24 15:11:30,049 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] App info kafka.consumer for consumer-group_id-68 unregistered
2022-06-24 15:11:30,049 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] group_id: Consumer stopped
2022-06-24 15:11:30,052 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] Metrics scheduler closed
2022-06-24 15:11:30,052 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-06-24 15:11:30,052 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] Metrics reporters closed
2022-06-24 15:11:30,052 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] App info kafka.consumer for consumer-group_id-66 unregistered
2022-06-24 15:11:30,052 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#7-0-C-1] group_id: Consumer stopped
2022-06-24 15:11:30,053 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] App info kafka.consumer for consumer-group_id-71 unregistered
2022-06-24 15:11:30,054 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] group_id: Consumer stopped
2022-06-24 15:11:30,054 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] App info kafka.consumer for consumer-group_id-67 unregistered
2022-06-24 15:11:30,054 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] group_id: Consumer stopped
2022-06-24 15:11:30,055 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] App info kafka.consumer for consumer-group_id-70 unregistered
2022-06-24 15:11:30,055 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#5-0-C-1] group_id: Consumer stopped
2022-06-24 15:11:30,055 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] App info kafka.consumer for consumer-group_id-72 unregistered
2022-06-24 15:11:30,056 INFO org.springframework.core.log.LogAccessor [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] group_id: Consumer stopped
2022-06-24 15:11:30,062 INFO com.mongodb.diagnostics.logging.SLF4JLogger [RMI TCP Connection(154)-127.0.0.1] Closed connection [connectionId{localValue:33, serverValue:84}] to localhost:27017 because the pool has been closed.
2022-06-24 15:11:30,063 INFO com.mongodb.diagnostics.logging.SLF4JLogger [RMI TCP Connection(154)-127.0.0.1] Closed connection [connectionId{localValue:32, serverValue:83}] to localhost:27017 because the pool has been closed.
2022-06-24 15:11:30,065 INFO com.netflix.discovery.DiscoveryClient [RMI TCP Connection(154)-127.0.0.1] Shutting down DiscoveryClient ...
2022-06-24 15:11:33,076 INFO com.netflix.discovery.DiscoveryClient [RMI TCP Connection(154)-127.0.0.1] Unregistering ...
2022-06-24 15:11:33,092 INFO com.netflix.discovery.DiscoveryClient [RMI TCP Connection(154)-127.0.0.1] DiscoveryClient_ESTOCKMARKET-QUERY/host.docker.internal:estockmarket-query:6093 - deregister  status: 200
2022-06-24 15:11:33,095 INFO com.netflix.discovery.DiscoveryClient [RMI TCP Connection(154)-127.0.0.1] Completed shut down of DiscoveryClient
